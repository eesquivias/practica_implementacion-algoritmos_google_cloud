{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Práctica Final-Detección_Trolls_Twitch_Eva.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj8bcxFX85UO"
      },
      "source": [
        "# Práctica Final: detección de mensajes troll en chat de Twitch en tiempo real\n",
        "\n",
        "Durante este último año la plataforma de vídeo en streaming Twitch ha cogido mucha popularidad debido a la situación que hemos vivido debido al COVID-19. Por esto, mucha gente de todas las edades ha empezado a consumir esta plataforma de manera diaria.\n",
        "\n",
        "Como consecuencia, no sólo han aumentado las personas que ven contenido en Twitch, sino también el número de los denominados *trolls*, gente que pone comentarios ofensivos en los chat de los streamers.\n",
        "\n",
        "En esta práctica se desarrollará un sistema autónomo basado en IA y desplegado en GCP que detectará en tiempo real si los mensajes que se envían a un canal de Twitch son de un *troll* o no. La práctica constará de tres partes principales que serán evaluadas en la corrección:\n",
        "1. Entrenamiento e inferencia en Batch de un modelo usando Dataflow y AI Platform. **(3.5 puntos)**.\n",
        "2. Despliegue e inferencia online en microservicio con el modelo. **(3.5 puntos)**.\n",
        "3. Inferencia en streaming de un canal de Twitch con el microservicio anterior. **(3 puntos)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgE9XB8ACVmQ"
      },
      "source": [
        "# Configuración de nuestro proyecto en GCP\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfGMurfni-CV",
        "outputId": "19c64a65-704c-4c65-c945-5eaa650f6528"
      },
      "source": [
        "! pip install apache-beam"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting apache-beam\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/c9/395a9759dfbf9e87203a69c33b2e94f10d566d9391bddb6f99facafe64c3/apache_beam-2.30.0-cp37-cp37m-manylinux2010_x86_64.whl (9.6MB)\n",
            "\u001b[K     |████████████████████████████████| 9.6MB 14.8MB/s \n",
            "\u001b[?25hCollecting avro-python3!=1.9.2,<1.10.0,>=1.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/80/acd1455bea0a9fcdc60a748a97dcbb3ff624726fb90987a0fc1c19e7a5a5/avro-python3-1.9.2.1.tar.gz\n",
            "Requirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam) (1.34.1)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/11/345f3173809cea7f1a193bfbf02403fff250a3360e0e118a1630985e547d/dill-0.3.1.1.tar.gz (151kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 45.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam) (3.7.4.3)\n",
            "Collecting future<1.0.0,>=0.18.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 28.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam) (3.12.4)\n",
            "Collecting fastavro<2,>=0.21.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/92/10ee74edb0a39f4a7af1cf271b3ac725c54f5c243c26fa5059cd794d15d7/fastavro-1.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 21.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: httplib2<0.20.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache-beam) (0.17.4)\n",
            "Requirement already satisfied: oauth2client<5,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam) (4.1.3)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam) (3.11.4)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam) (1.7)\n",
            "Requirement already satisfied: pyarrow<4.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam) (3.0.0)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam) (1.3.0)\n",
            "Collecting requests<3.0.0,>=2.24.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam) (2018.9)\n",
            "Requirement already satisfied: numpy<1.21.0,>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam) (2.8.1)\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/08/f7/4c3fad73123a24d7394b6f40d1ec9c1cbf2e921cfea1797216ffd0a51fb1/hdfs-2.6.0-py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<2,>=1.29.0->apache-beam) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf<4,>=3.12.2->apache-beam) (57.0.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache-beam) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache-beam) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache-beam) (0.2.8)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pydot<2,>=1.2.0->apache-beam) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam) (2021.5.30)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam) (2.10)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam) (0.6.2)\n",
            "Building wheels for collected packages: avro-python3, dill, future\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-cp37-none-any.whl size=43516 sha256=715a52737b4d27dc476e31f00d63a5260e09e9eeb792155172f6ca88a30bce2e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/d3/be/86620c9dd3fca68986c33b9c616510289fc0abb81ec9aa70bd\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-cp37-none-any.whl size=78545 sha256=4f4bdaeadaac387f06a507aedc7539c866ad65d8aa5540ef48f584b1f9436b7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/b1/91/f02e76c732915c4015ab4010f3015469866c1eb9b14058d8e7\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491070 sha256=628510b1c81e9e1e14b4d94f66d05d3a983f6e056cd138f0e6797f6ca6467331\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "Successfully built avro-python3 dill future\n",
            "\u001b[31mERROR: multiprocess 0.70.12.2 has requirement dill>=0.3.4, but you'll have dill 0.3.1.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: avro-python3, dill, future, fastavro, requests, hdfs, apache-beam\n",
            "  Found existing installation: dill 0.3.4\n",
            "    Uninstalling dill-0.3.4:\n",
            "      Successfully uninstalled dill-0.3.4\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "Successfully installed apache-beam-2.30.0 avro-python3-1.9.2.1 dill-0.3.1.1 fastavro-1.4.2 future-0.18.2 hdfs-2.6.0 requests-2.25.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAorVw7RCT9C",
        "outputId": "45cb1d73-2f5f-4d05-d71c-5d71843a379a"
      },
      "source": [
        "PROJECT_ID = \"twitch-practiceeva\" #@param {type:\"string\"}\n",
        "! gcloud config set project $PROJECT_ID"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbAl8pSkCXCm"
      },
      "source": [
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import auth as google_auth\n",
        "  google_auth.authenticate_user()\n",
        "\n",
        "# If you are running this notebook locally, replace the string below with the\n",
        "# path to your service account key and run this cell to authenticate your GCP\n",
        "# account.\n",
        "else:\n",
        "  %env GOOGLE_APPLICATION_CREDENTIALS ''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6Zpx14FCfXy"
      },
      "source": [
        "BUCKET_NAME = \"twitch-practiceeva\" #@param {type:\"string\"}\n",
        "REGION = \"europe-west1\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrDTf8qhMgGK",
        "outputId": "1eec657e-bce7-46ac-bc9c-91e2b4648d41"
      },
      "source": [
        "! gsutil mb -l $REGION gs://$BUCKET_NAME #sólo la primera vez"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating gs://twitch-practiceeva/...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE4V-Xt0ChSb",
        "outputId": "086b3c71-5fe2-45dd-b692-16995c0ca768"
      },
      "source": [
        "! gsutil ls -al gs://$BUCKET_NAME"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   2756023  2021-07-02T15:52:43Z  gs://twitch-practiceeva/data.json#1625241163220787  metageneration=1\n",
            "                                 gs://twitch-practiceeva/beam-temp/\n",
            "                                 gs://twitch-practiceeva/model/\n",
            "                                 gs://twitch-practiceeva/predictions/\n",
            "                                 gs://twitch-practiceeva/trainer/\n",
            "                                 gs://twitch-practiceeva/transformed_data/\n",
            "TOTAL: 1 objects, 2756023 bytes (2.63 MiB)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D8Qw14y_nRa"
      },
      "source": [
        "# Entrenamiento e inferencia en Batch\n",
        "\n",
        "Para esta primera parte se va a utilizar [Tweets Dataset for Detection of Cyber-Trolls](https://www.kaggle.com/dataturks/dataset-for-detection-of-cybertrolls). El objetivo es desarrollar un clasificador binario para detectar si el mensaje recibido es troll (1) o no (0). **Las métricas obtenidas del entrenamiento y la inferencia no se tendrán en cuenta para la evaluación de la práctica, la importancia está en la arquitectura de la solución**.\n",
        "\n",
        "A continuación os dejo un diagrama con la arquitectura que se va a desarrollar:\n",
        "\n",
        "![batch_diagram](https://drive.google.com/uc?export=view&id=1h1BkIunyKSkJYFRbXKNWpHOZ_rDUyGAT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM_dh-471gIq"
      },
      "source": [
        "A continuación, se van a subir los datos de entrenamiento al bucket del proyecto que se haya creado. **Importante:** crea el bucket en una única región para evitar problemas más adelante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sqp4L_nmUjAc",
        "outputId": "7c95c04a-be5b-4f2e-d497-44620c1f2b7c"
      },
      "source": [
        "# Upload data to your bucket\n",
        "! wget https://storage.googleapis.com/twitch-practice-keepcoding/data.json -O - | gsutil cp - gs://$BUCKET_NAME/data.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-02 15:52:40--  https://storage.googleapis.com/twitch-practice-keepcoding/data.json\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.13.80, 172.217.13.240, 172.217.15.80, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.13.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2756023 (2.6M) [application/json]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                     0%[                    ]       0  --.-KB/s               Copying from <STDIN>...\n",
            "-                   100%[===================>]   2.63M  1.41MB/s    in 1.9s    \n",
            "\n",
            "2021-07-02 15:52:42 (1.41 MB/s) - written to stdout [2756023/2756023]\n",
            "\n",
            "/ [1 files][    0.0 B/    0.0 B]                                                \n",
            "Operation completed over 1 objects.                                              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCQQ9j2I11tg"
      },
      "source": [
        "Ahora se crea el directorio dónde vas a desarrollar esta primera parte de la práctica."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsblBlJ6RrGm"
      },
      "source": [
        "%mkdir /content/batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyK51quU1_oi"
      },
      "source": [
        "Se establece el directorio de trabajo que hemos creado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7ybSaotRwkP",
        "outputId": "9ee81651-fbb4-4135-acee-62682f4be54f"
      },
      "source": [
        "import os\n",
        "\n",
        "# Set the working directory to the sample code directory\n",
        "%cd /content/batch\n",
        "\n",
        "WORK_DIR = os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/batch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NSd4bAo2aj_"
      },
      "source": [
        "Ahora se descargarán los datos en el workspace de Colab para trabajar en local."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOmUJSg1JCgg",
        "outputId": "57be212a-e35d-4a08-ed72-19d837361be6"
      },
      "source": [
        "! wget https://storage.googleapis.com/twitch-practice-keepcoding/data.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-03 14:54:46--  https://storage.googleapis.com/twitch-practice-keepcoding/data.json\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.65.80, 142.250.188.208, 142.251.33.208, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.65.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2756023 (2.6M) [application/json]\n",
            "Saving to: ‘data.json’\n",
            "\n",
            "\rdata.json             0%[                    ]       0  --.-KB/s               \rdata.json           100%[===================>]   2.63M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-07-03 14:54:46 (129 MB/s) - ‘data.json’ saved [2756023/2756023]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRoW6zyg2kEz"
      },
      "source": [
        "Se establecen las dependencias que se usarán en la práctica. Se pueden añadir y quitar las dependencias que no se usen o viceversa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s0-8_JK_z2D",
        "outputId": "a1f7d0e2-4d9a-46ec-8dc4-54bd3a9f3e77"
      },
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "apache-beam[gcp]==2.24.0\n",
        "tensorflow-transform==0.24.1\n",
        "tensorflow==2.3.0\n",
        "tfx==0.24.1\n",
        "gensim==3.6.0\n",
        "fsspec==0.8.4\n",
        "gcsfs==0.7.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing requirements.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIl8lrPp2x2j"
      },
      "source": [
        "Instalamos las dependencias. **No olvidarse de reiniciar el entorno al instalar y establecer las variables y credenciales de GCP al arrancar.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZS6P8A4c_8le",
        "outputId": "7ce3e8d9-7d30-427f-81b2-95c45400522d"
      },
      "source": [
        "! pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting apache-beam[gcp]==2.24.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/1b/10ab8380b6d56700b3eba1a7bbf980878ea9d0457303e0934760fc7882d7/apache_beam-2.24.0-cp37-cp37m-manylinux2010_x86_64.whl (8.5MB)\n",
            "\u001b[K     |████████████████████████████████| 8.6MB 25.4MB/s \n",
            "\u001b[?25hCollecting tensorflow-transform==0.24.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/9f/acad7dab38ba19f4c574de2b50ec13343fce6ac51c291b5fc81e59bc4466/tensorflow_transform-0.24.1-py3-none-any.whl (373kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 38.7MB/s \n",
            "\u001b[?25hCollecting tensorflow==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/89/f2d29c2eafc2eeafb17d5634340e06366af904d332341200a49d954bce85/tensorflow-2.3.0-cp37-cp37m-manylinux2010_x86_64.whl (320.4MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4MB 47kB/s \n",
            "\u001b[?25hCollecting tfx==0.24.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/28/d283681f9e7bc489b59712654f65229c75b84d4450137092a18d143a2943/tfx-0.24.1-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 28.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim==3.6.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (3.6.0)\n",
            "Collecting fsspec==0.8.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/8b/1df260f860f17cb08698170153ef7db672c497c1840dcc8613ce26a8a005/fsspec-0.8.4-py3-none-any.whl (91kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 8.6MB/s \n",
            "\u001b[?25hCollecting gcsfs==0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/85/75/3d669945d41e5aedd5c4333b9dc6192b7839d2bafd04b75b8222d4e92ae0/gcsfs-0.7.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.7)\n",
            "Requirement already satisfied: numpy<2,>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.19.5)\n",
            "Requirement already satisfied: future<1.0.0,>=0.18.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.18.2)\n",
            "Requirement already satisfied: httplib2<0.18.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.17.4)\n",
            "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.3.1.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2.25.1)\n",
            "Requirement already satisfied: avro-python3!=1.9.2,<1.10.0,>=1.8.1; python_version >= \"3.0\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.9.2.1)\n",
            "Requirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.34.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.12.4)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2.6.0)\n",
            "Collecting pyarrow<0.18.0,>=0.15.1; python_version >= \"3.0\" or platform_system != \"Windows\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/78/dcd7f290cd018581b5c73f6c87e2b004f1161cdf6f55c7b2c87d78174592/pyarrow-0.17.1-cp37-cp37m-manylinux2014_x86_64.whl (63.8MB)\n",
            "\u001b[K     |████████████████████████████████| 63.8MB 90kB/s \n",
            "\u001b[?25hRequirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.11.4)\n",
            "Collecting mock<3.0.0,>=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.7.4.3)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2018.9)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2.8.1)\n",
            "Collecting fastavro<0.24,>=0.21.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/72/6e5bdda35e3e2b216b8ec9711fef894fb08394bd842e08481982fad8c424/fastavro-0.23.6-cp37-cp37m-manylinux2010_x86_64.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 28.5MB/s \n",
            "\u001b[?25hCollecting oauth2client<4,>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/7b/bc893e35d6ca46a72faa4b9eaac25c687ce60e1fbe978993fe2de1b0ff0d/oauth2client-3.0.0.tar.gz (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.3MB/s \n",
            "\u001b[?25hCollecting google-cloud-language<2,>=1.3.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/b8/965a97ba60287910d342623da1da615254bded3e0965728cf7fc6339b7c8/google_cloud_language-1.3.0-py2.py3-none-any.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.8MB/s \n",
            "\u001b[?25hCollecting google-apitools<0.5.32,>=0.5.31; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/da/aefc4cf4c168b5d875344cd9dddc77e3a2d11986b630251af5ce47dd2843/google-apitools-0.5.31.tar.gz (173kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 47.5MB/s \n",
            "\u001b[?25hCollecting cachetools<4,>=3.1.0; extra == \"gcp\"\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/a6/30b0a0bef12283e83e58c1d6e7b5aabc7acfc4110df81a4471655d33e704/cachetools-3.1.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: google-auth<2,>=1.18.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.31.0)\n",
            "Collecting google-cloud-pubsub<2,>=0.39.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/b3/dd83eca4cd1019d592e82595ea45d53f11e39db4ee99daa66ceb8a1b2d89/google_cloud_pubsub-1.7.0-py2.py3-none-any.whl (144kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 46.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-bigquery<2,>=1.6.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.21.0)\n",
            "Collecting google-cloud-spanner<2,>=1.13.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/16/c9262ca40f3a278f38df9d21dece1ae01ee24f8ed29937bd1f066f908f9f/google_cloud_spanner-1.19.1-py2.py3-none-any.whl (255kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 46.3MB/s \n",
            "\u001b[?25hCollecting google-cloud-vision<2,>=0.38.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/7f/e10d602c2dc3f749f1b78377a3357790f1da71b28e7da9e5bc20b3a9bd40/google_cloud_vision-1.0.0-py2.py3-none-any.whl (435kB)\n",
            "\u001b[K     |████████████████████████████████| 440kB 44.9MB/s \n",
            "\u001b[?25hCollecting google-cloud-dlp<2,>=0.12.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/c3/5b73c15f59207b20df288573c2ea203c7b126df8330add380d8b50bc0d5c/google_cloud_dlp-1.0.0-py2.py3-none-any.whl (169kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 47.2MB/s \n",
            "\u001b[?25hCollecting grpcio-gcp<1,>=0.2.2; extra == \"gcp\"\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/83/1f1095815be0de19102df41e250ebbd7dae97d7d14e22c18da07ed5ed9d4/grpcio_gcp-0.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: google-cloud-datastore<2,>=1.7.1; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.8.0)\n",
            "Collecting google-cloud-videointelligence<2,>=1.8.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/29/8d06211102c87768dc34943d9c92abd8b67491ffedd6d09b56305b1ab255/google_cloud_videointelligence-1.16.1-py2.py3-none-any.whl (183kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 41.7MB/s \n",
            "\u001b[?25hCollecting google-cloud-bigtable<2,>=0.31.1; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/17/536768bada8f93f124826b36dbdcdf08edd0e5ef0ca76b4c911f9f28596a/google_cloud_bigtable-1.7.0-py2.py3-none-any.whl (267kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 40.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-core<2,>=0.28.1; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.0.3)\n",
            "Requirement already satisfied: six<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow-transform==0.24.1->-r requirements.txt (line 3)) (1.15.0)\n",
            "Collecting tfx-bsl<0.25,>=0.24.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/52/e2281ce61b802e7e5d50cf37968f5fb26cd31945d44a1412579ff54bc22a/tfx_bsl-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 29.7MB/s \n",
            "\u001b[?25hCollecting tensorflow-metadata<0.25,>=0.24\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/c4/1ff6a8afaac19250780a82bc05907586f6c23f45a5983df8921040e2b04c/tensorflow_metadata-0.24.0-py3-none-any.whl (44kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.3MB/s \n",
            "\u001b[?25hCollecting absl-py<0.11,>=0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/07/f69dd3367368ad69f174bfe426a973651412ec11d48ec05c000f19fe0561/absl_py-0.10.0-py3-none-any.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 33.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0->-r requirements.txt (line 4)) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0->-r requirements.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0->-r requirements.txt (line 4)) (2.5.0)\n",
            "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/ed/5853ec0ae380cba4588eab1524e18ece1583b65f7ae0e97321f5ff9dfd60/tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 42.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0->-r requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0->-r requirements.txt (line 4)) (3.3.0)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 46.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0->-r requirements.txt (line 4)) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0->-r requirements.txt (line 4)) (1.1.2)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0->-r requirements.txt (line 4)) (1.4.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0->-r requirements.txt (line 4)) (0.36.2)\n",
            "Collecting attrs<20,>=19.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/db/4313ab3be961f7a763066401fb77f7748373b6094076ae2bda2806988af6/attrs-19.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: jinja2<3,>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from tfx==0.24.1->-r requirements.txt (line 5)) (2.11.3)\n",
            "Collecting tensorflow-model-analysis<0.25,>=0.24.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/2e/d3ae21fdeeff836ab2383c2d4c59c921f12722fa56cd350386e8866b7196/tensorflow_model_analysis-0.24.3-py3-none-any.whl (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 20.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client<2,>=1.7.8 in /usr/local/lib/python3.7/dist-packages (from tfx==0.24.1->-r requirements.txt (line 5)) (1.12.8)\n",
            "Requirement already satisfied: pyyaml<6,>=3.12 in /usr/local/lib/python3.7/dist-packages (from tfx==0.24.1->-r requirements.txt (line 5)) (3.13)\n",
            "Collecting keras-tuner<2,>=1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/ba/c7a7cda64b67846bde66ffdaaf3199a79d4e35ab6b6f170cc1b7d235646e/keras_tuner-1.0.3-py3-none-any.whl (96kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 12.2MB/s \n",
            "\u001b[?25hCollecting kubernetes<12,>=10.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/46/d4b9364fcab59b5f9248e014c7592df8de84ad6548a6fe3de2d805bb75fc/kubernetes-11.0.0-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 28.6MB/s \n",
            "\u001b[?25hCollecting docker<5,>=4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/22/410313ad554477e87ec406d38d85f810e61ddb0d2fc44e64994857476de9/docker-4.4.4-py2.py3-none-any.whl (147kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 45.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: click<8,>=7 in /usr/local/lib/python3.7/dist-packages (from tfx==0.24.1->-r requirements.txt (line 5)) (7.1.2)\n",
            "Collecting ml-metadata<0.25,>=0.24\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/f3/7586f62963fc678514f1132e5d3e53ee4f9efff9f0f6f24c4a60877acbda/ml_metadata-0.24.0-cp37-cp37m-manylinux2010_x86_64.whl (2.8MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8MB 16.4MB/s \n",
            "\u001b[?25hCollecting tensorflow-data-validation<0.25,>=0.24.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/04/06b6be6a8770c72aba0ffa63f27712263fd3ba15c93f1ca71133b9bd888c/tensorflow_data_validation-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 36.9MB/s \n",
            "\u001b[?25hCollecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15\n",
            "  Downloading https://files.pythonhosted.org/packages/27/a6/a534deae4086c0fef9a77537a4779bb5a9dd8841f8d347c509c96b342b2e/tensorflow_serving_api-2.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.6.0->-r requirements.txt (line 6)) (5.1.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 8)) (4.4.2)\n",
            "Collecting aiohttp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 23.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 8)) (0.4.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2021.5.30)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf<4,>=3.12.2->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (57.0.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.6.2)\n",
            "Collecting pbr>=0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/e0/1d4702dd81121d04a477c272d47ee5b6bc970d1a0990b11befa275c55cf2/pbr-5.6.0-py2.py3-none-any.whl (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 49.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pydot<2,>=1.2.0->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2.4.7)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (4.7.2)\n",
            "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-language<2,>=1.3.0; extra == \"gcp\"->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.26.3)\n",
            "Collecting fasteners>=0.14\n",
            "  Downloading https://files.pythonhosted.org/packages/31/91/6630ebd169ca170634ca8a10dfcc5f5c11b0621672d4c2c9e40381c6d81a/fasteners-0.16.3-py2.py3-none-any.whl\n",
            "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
            "  Downloading https://files.pythonhosted.org/packages/65/19/2060c8faa325fddc09aa67af98ffcb6813f39a0ad805679fa64815362b3a/grpc-google-iam-v1-0.12.3.tar.gz\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<2,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: pandas<2,>=1.0 in /usr/local/lib/python3.7/dist-packages (from tfx-bsl<0.25,>=0.24.1->tensorflow-transform==0.24.1->-r requirements.txt (line 3)) (1.1.5)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata<0.25,>=0.24->tensorflow-transform==0.24.1->-r requirements.txt (line 3)) (1.53.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0->-r requirements.txt (line 4)) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0->-r requirements.txt (line 4)) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0->-r requirements.txt (line 4)) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0->-r requirements.txt (line 4)) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<3,>=2.7.3->tfx==0.24.1->-r requirements.txt (line 5)) (2.0.1)\n",
            "Requirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (7.6.3)\n",
            "Requirement already satisfied: jupyter<2,>=1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (1.0.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->tfx==0.24.1->-r requirements.txt (line 5)) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->tfx==0.24.1->-r requirements.txt (line 5)) (3.0.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner<2,>=1->tfx==0.24.1->-r requirements.txt (line 5)) (5.5.0)\n",
            "Collecting kt-legacy\n",
            "  Downloading https://files.pythonhosted.org/packages/76/c7/7ebe02ef2495b84a47dc92a4e943260b264b6546783ca23e451bcd8c09c1/kt-legacy-1.0.3.tar.gz\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner<2,>=1->tfx==0.24.1->-r requirements.txt (line 5)) (20.9)\n",
            "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/5f/3c211d168b2e9f9342cfb53bcfc26aab0eac63b998015e7af7bcae66119d/websocket_client-1.1.0-py2.py3-none-any.whl (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib in /usr/local/lib/python3.7/dist-packages (from kubernetes<12,>=10.0.1->tfx==0.24.1->-r requirements.txt (line 5)) (1.3.0)\n",
            "Collecting joblib<0.15,>=0.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 50.5MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 48.0MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Collecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 46.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0->-r requirements.txt (line 4)) (4.5.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (5.1.3)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (5.0.5)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (4.10.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (1.0.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (3.5.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (5.6.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (5.2.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (5.1.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner<2,>=1->tfx==0.24.1->-r requirements.txt (line 5)) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner<2,>=1->tfx==0.24.1->-r requirements.txt (line 5)) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner<2,>=1->tfx==0.24.1->-r requirements.txt (line 5)) (1.0.18)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner<2,>=1->tfx==0.24.1->-r requirements.txt (line 5)) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner<2,>=1->tfx==0.24.1->-r requirements.txt (line 5)) (0.8.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib->kubernetes<12,>=10.0.1->tfx==0.24.1->-r requirements.txt (line 5)) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0->-r requirements.txt (line 4)) (3.4.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (2.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (4.7.1)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (5.3.5)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (1.5.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (0.10.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (0.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (3.3.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (0.5.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (1.4.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (0.8.4)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter<2,>=1->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (1.9.0)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter<2,>=1->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (22.1.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner<2,>=1->tfx==0.24.1->-r requirements.txt (line 5)) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->keras-tuner<2,>=1->tfx==0.24.1->-r requirements.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->-r requirements.txt (line 5)) (0.5.1)\n",
            "Building wheels for collected packages: oauth2client, google-apitools, grpc-google-iam-v1, kt-legacy\n",
            "  Building wheel for oauth2client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for oauth2client: filename=oauth2client-3.0.0-cp37-none-any.whl size=106376 sha256=f1299ec332120182c7e5c9b2adcc6386772a27e488cf5ce7d2b686b99d31dd1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/f7/87/b932f09c6335dbcf45d916937105a372ab14f353a9ca431d7d\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.31-cp37-none-any.whl size=131043 sha256=162647db6f6e81b8ba66300500a0032b99a9d8693b26527bb263ad05fc788bb1\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/43/31/09a9dad88d3aec6fed2d63bd35dfc532fca372e2edec5af5bf\n",
            "  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grpc-google-iam-v1: filename=grpc_google_iam_v1-0.12.3-cp37-none-any.whl size=18516 sha256=ffb001f9ddd73acb527a1725a540b31b71b7b34540f405efdb817d15dced06ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/3a/83/77a1e18e1a8757186df834b86ce6800120ac9c79cd8ca4091b\n",
            "  Building wheel for kt-legacy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kt-legacy: filename=kt_legacy-1.0.3-cp37-none-any.whl size=9569 sha256=4d19a87cadfbb5bc60afe1c25eaec5393b89fae05e34e43de3dbb12380f7effd\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/99/a8/6b116b0f69be60cc475d1cd36680f3a09f284d86655bb99d93\n",
            "Successfully built oauth2client google-apitools grpc-google-iam-v1 kt-legacy\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pymc3 3.11.2 has requirement cachetools>=4.2.1, but you'll have cachetools 3.1.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pydrive 1.3.1 has requirement oauth2client>=4.0.0, but you'll have oauth2client 3.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-cloud-spanner 1.19.1 has requirement google-cloud-core<2.0dev,>=1.4.1, but you'll have google-cloud-core 1.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-cloud-bigtable 1.7.0 has requirement google-cloud-core<2.0dev,>=1.4.1, but you'll have google-cloud-core 1.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-serving-api 2.5.1 has requirement tensorflow<3,>=2.5.0, but you'll have tensorflow 2.3.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pyarrow, pbr, mock, fastavro, oauth2client, google-cloud-language, fasteners, google-apitools, cachetools, grpc-google-iam-v1, google-cloud-pubsub, google-cloud-spanner, google-cloud-vision, google-cloud-dlp, grpcio-gcp, google-cloud-videointelligence, google-cloud-bigtable, apache-beam, tensorflow-estimator, gast, h5py, absl-py, tensorflow, tensorflow-metadata, tensorflow-serving-api, tfx-bsl, tensorflow-transform, attrs, tensorflow-model-analysis, kt-legacy, keras-tuner, websocket-client, kubernetes, docker, ml-metadata, joblib, tensorflow-data-validation, tfx, fsspec, multidict, yarl, async-timeout, aiohttp, gcsfs\n",
            "  Found existing installation: pyarrow 3.0.0\n",
            "    Uninstalling pyarrow-3.0.0:\n",
            "      Successfully uninstalled pyarrow-3.0.0\n",
            "  Found existing installation: fastavro 1.4.2\n",
            "    Uninstalling fastavro-1.4.2:\n",
            "      Successfully uninstalled fastavro-1.4.2\n",
            "  Found existing installation: oauth2client 4.1.3\n",
            "    Uninstalling oauth2client-4.1.3:\n",
            "      Successfully uninstalled oauth2client-4.1.3\n",
            "  Found existing installation: google-cloud-language 1.2.0\n",
            "    Uninstalling google-cloud-language-1.2.0:\n",
            "      Successfully uninstalled google-cloud-language-1.2.0\n",
            "  Found existing installation: cachetools 4.2.2\n",
            "    Uninstalling cachetools-4.2.2:\n",
            "      Successfully uninstalled cachetools-4.2.2\n",
            "  Found existing installation: apache-beam 2.30.0\n",
            "    Uninstalling apache-beam-2.30.0:\n",
            "      Successfully uninstalled apache-beam-2.30.0\n",
            "  Found existing installation: tensorflow-estimator 2.5.0\n",
            "    Uninstalling tensorflow-estimator-2.5.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
            "  Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Found existing installation: absl-py 0.12.0\n",
            "    Uninstalling absl-py-0.12.0:\n",
            "      Successfully uninstalled absl-py-0.12.0\n",
            "  Found existing installation: tensorflow 2.5.0\n",
            "    Uninstalling tensorflow-2.5.0:\n",
            "      Successfully uninstalled tensorflow-2.5.0\n",
            "  Found existing installation: tensorflow-metadata 1.0.0\n",
            "    Uninstalling tensorflow-metadata-1.0.0:\n",
            "      Successfully uninstalled tensorflow-metadata-1.0.0\n",
            "  Found existing installation: attrs 21.2.0\n",
            "    Uninstalling attrs-21.2.0:\n",
            "      Successfully uninstalled attrs-21.2.0\n",
            "  Found existing installation: joblib 1.0.1\n",
            "    Uninstalling joblib-1.0.1:\n",
            "      Successfully uninstalled joblib-1.0.1\n",
            "Successfully installed absl-py-0.10.0 aiohttp-3.7.4.post0 apache-beam-2.24.0 async-timeout-3.0.1 attrs-19.3.0 cachetools-3.1.1 docker-4.4.4 fastavro-0.23.6 fasteners-0.16.3 fsspec-0.8.4 gast-0.3.3 gcsfs-0.7.1 google-apitools-0.5.31 google-cloud-bigtable-1.7.0 google-cloud-dlp-1.0.0 google-cloud-language-1.3.0 google-cloud-pubsub-1.7.0 google-cloud-spanner-1.19.1 google-cloud-videointelligence-1.16.1 google-cloud-vision-1.0.0 grpc-google-iam-v1-0.12.3 grpcio-gcp-0.2.2 h5py-2.10.0 joblib-0.14.1 keras-tuner-1.0.3 kt-legacy-1.0.3 kubernetes-11.0.0 ml-metadata-0.24.0 mock-2.0.0 multidict-5.1.0 oauth2client-3.0.0 pbr-5.6.0 pyarrow-0.17.1 tensorflow-2.3.0 tensorflow-data-validation-0.24.1 tensorflow-estimator-2.3.0 tensorflow-metadata-0.24.0 tensorflow-model-analysis-0.24.3 tensorflow-serving-api-2.5.1 tensorflow-transform-0.24.1 tfx-0.24.1 tfx-bsl-0.24.1 websocket-client-1.1.0 yarl-1.6.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "apache_beam",
                  "cachetools",
                  "fastavro",
                  "google",
                  "oauth2client",
                  "pyarrow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0DDu27S3CJv"
      },
      "source": [
        "##**Entregable (0.5 puntos)**\n",
        "\n",
        "Desarrollar un pipeline de preprocesamiento utilizando Apache Beam para generar datos de train, eval y test para los datos proporcionados anteriormente. Requisitos:\n",
        "\n",
        "- Proporcionar dos modos de ejecución: `train` y `test`\n",
        "- Soportar ejecuciones en local con `DirectRunner` y ejecuciones en Dataflow usando `DataFlowRunner`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoL1ZS5Tq2R2",
        "outputId": "5767ae05-3ffb-418b-8027-9de4b0e2d3df"
      },
      "source": [
        "# Nos aseguramos que nuestras variables de entorno no hayan desaparecido al reiniciar el kernel\n",
        "\n",
        "print(f\"Project: {PROJECT_ID}\")\n",
        "print(f\"Region: {REGION}\")\n",
        "print(f\"Bucket: {BUCKET_NAME}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Project: twitch-practiceeva\n",
            "Region: europe-west1\n",
            "Bucket: twitch-practiceeva\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1blctlxs_dPO",
        "outputId": "ffc70743-e902-4481-8b87-fd99088f723e"
      },
      "source": [
        "%%writefile preprocess.py\n",
        "\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import argparse # con esta librería se define el work-dir, el runner, el input, output\n",
        "import logging #para establecer trazas\n",
        "import re #para expresiones regulares\n",
        "import os #para interactuar con sistema de ficheros local \n",
        "import json #para leer datos json\n",
        "import random #para generar semillas...para que siempre nos de lo mismo...\n",
        "\n",
        "from past.builtins import unicode\n",
        "\n",
        "import apache_beam as beam\n",
        "from apache_beam.io import ReadFromText #transformaciones entrada-salida. leer\n",
        "from apache_beam.io import WriteToText #escribir\n",
        "from apache_beam.coders.coders import Coder # para transformar formato de entrada de datos en estandar\n",
        "from apache_beam.options.pipeline_options import PipelineOptions #establecemos opciones \n",
        "from apache_beam.options.pipeline_options import SetupOptions, DirectOptions #de configuración y de forma directa\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# CLEANING\n",
        "STOP_WORDS = stopwords.words(\"english\")\n",
        "STEMMER = SnowballStemmer(\"english\")\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "\n",
        "#raw_data = [json.loads(line) for line in open(\"data.json\", 'r')]\n",
        "\n",
        "class ExtractColumnsDoFn(beam.DoFn):\n",
        "    def process(self, element):\n",
        "        # space removal\n",
        "        element_split = json.loads(element)     \n",
        "        # text, sentiment\n",
        "        yield element_split.get('content'), element_split.get('annotation').get('label')[0]\n",
        "\n",
        "\n",
        "class PreprocessColumnsTrainFn(beam.DoFn):\n",
        "    def process_sentiment(self, sentiment):\n",
        "        sentiment = int(sentiment)\n",
        "        if sentiment == 1:\n",
        "            return \"TROLL\"\n",
        "        else:\n",
        "            return \"NOTROLL\"\n",
        "\n",
        "    def process_text(self, text):\n",
        "        # Remove link,user and special characters\n",
        "        stem = False\n",
        "        text = re.sub(TEXT_CLEANING_RE, \" \", str(text).lower()).strip()\n",
        "        tokens = []\n",
        "        for token in text.split():\n",
        "            if token not in STOP_WORDS:\n",
        "                if stem:\n",
        "                    tokens.append(STEMMER.stem(token))\n",
        "                else:\n",
        "                    tokens.append(token)\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def process(self, element):\n",
        "        processed_text = self.process_text(element[0])\n",
        "        processed_sentiment = self.process_sentiment(element[1])\n",
        "        yield f\"{processed_text}, {processed_sentiment}\"\n",
        "\n",
        "\n",
        "class CustomCoder(Coder):\n",
        "    \"\"\"A custom coder used for reading and writing strings\"\"\"\n",
        "\n",
        "    def __init__(self, encoding: str):\n",
        "        # latin-1\n",
        "        # iso-8859-1\n",
        "        self.encoding = encoding\n",
        "\n",
        "    def encode(self, value):\n",
        "        return value.encode(self.encoding)\n",
        "\n",
        "    def decode(self, value):\n",
        "        return value.decode(self.encoding)\n",
        "\n",
        "    def is_deterministic(self):\n",
        "        return True\n",
        "\n",
        "\n",
        "def run(argv=None, save_main_session=True):\n",
        "\n",
        "    \"\"\"Main entry point; defines and runs the wordcount pipeline.\"\"\"\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--work-dir\", dest=\"work_dir\", required=True, help=\"Working directory\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--input\", dest=\"input\", required=True, help=\"Input dataset in work dir\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output\",\n",
        "        dest=\"output\",\n",
        "        required=True,\n",
        "        help=\"Output path to store transformed data in work dir\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--mode\",\n",
        "        dest=\"mode\",\n",
        "        required=True,\n",
        "        choices=[\"train\", \"test\"],\n",
        "        help=\"Type of output to store transformed data\",\n",
        "    )\n",
        "\n",
        "    known_args, pipeline_args = parser.parse_known_args(argv)\n",
        "\n",
        "    # We use the save_main_session option because one or more DoFn's in this\n",
        "    # workflow rely on global context (e.g., a module imported at module level).\n",
        "    pipeline_options = PipelineOptions(pipeline_args)\n",
        "    pipeline_options.view_as(SetupOptions).save_main_session = save_main_session\n",
        "    pipeline_options.view_as(DirectOptions).direct_num_workers = 0\n",
        "\n",
        "    # The pipeline will be run on exiting the with block.\n",
        "    with beam.Pipeline(options=pipeline_options) as p:\n",
        "\n",
        "        # Read the text file[pattern] into a PCollection.\n",
        "        raw_data = p | \"ReadTwitchData\" >> ReadFromText(\n",
        "            known_args.input, coder=CustomCoder(\"latin-1\")\n",
        "            )\n",
        "        eval_percent = 20\n",
        "        assert 0 < eval_percent < 100, \"eval_percent must in the range (0-100)\"\n",
        "        train_data, test_data = (\n",
        "            raw_data\n",
        "            | \"Split train-test dataset\"\n",
        "            >> beam.Partition(\n",
        "                    lambda elem, _: int(random.uniform(0, 100) < eval_percent), 2\n",
        "            )\n",
        "        )\n",
        "        if known_args.mode == \"train\":\n",
        "\n",
        "            transformed_data = (\n",
        "                train_data\n",
        "                | \"ExtractColumns\" >> beam.ParDo(ExtractColumnsDoFn())\n",
        "                | \"Preprocess\" >> beam.ParDo(PreprocessColumnsTrainFn())\n",
        "            )\n",
        "\n",
        "            new_eval_percent = 20\n",
        "            assert 0 < new_eval_percent < 100, \"eval_percent must in the range (0-100)\"\n",
        "            train_dataset, eval_dataset = (\n",
        "                transformed_data\n",
        "                | \"Split dataset\"\n",
        "                >> beam.Partition(\n",
        "                    lambda elem, _: int(random.uniform(0, 100) < new_eval_percent), 2\n",
        "                )\n",
        "            )\n",
        "\n",
        "            train_dataset | \"TrainWriteToCSV\" >> WriteToText(\n",
        "                os.path.join(known_args.output, \"train\", \"part\")\n",
        "            )\n",
        "            eval_dataset | \"EvalWriteToCSV\" >> WriteToText(\n",
        "                os.path.join(known_args.output, \"eval\", \"part\")\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            transformed_data = (\n",
        "                test_data\n",
        "                | \"ExtractColumns\" >> beam.ParDo(ExtractColumnsDoFn())\n",
        "                | \"Preprocess\" >> beam.Map(lambda x: f'\"{x[0]}\"')\n",
        "            )\n",
        "\n",
        "            transformed_data | \"TestWriteToCSV\" >> WriteToText(\n",
        "                os.path.join(known_args.output, \"test\", \"part\")\n",
        "            )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "    run()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing preprocess.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qawuKC0k4B0e"
      },
      "source": [
        "Se proporciona un fichero `setup.py` necesario para ejecutar en DataFlow. Modificar la variable `REQUIRED_PACKAGES` con las dependencias que se hayan usado en el `requirements.txt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1MQvWsk_mVX",
        "outputId": "8e549abd-9418-4cd8-8cf7-76594733c29d"
      },
      "source": [
        "%%writefile setup.py\n",
        "\n",
        "import setuptools\n",
        "\n",
        "REQUIRED_PACKAGES = [\n",
        "    \"apache-beam[gcp]==2.24.0\",\n",
        "    \"tensorflow-transform==0.24.1\",\n",
        "    \"tensorflow==2.3.0\",\n",
        "    \"tfx==0.24.1\",\n",
        "    \"gensim==3.6.0\",\n",
        "    \"fsspec==0.8.4\",\n",
        "    \"gcsfs==0.7.1\",\n",
        "]\n",
        "\n",
        "setuptools.setup(\n",
        "    name=\"twitchstreaming\",\n",
        "    version=\"0.0.1\",\n",
        "    install_requires=REQUIRED_PACKAGES,\n",
        "    packages=setuptools.find_packages(),\n",
        "    include_package_data=True,\n",
        "    description=\"Troll detection\",\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing setup.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLDlz_fL4UJA"
      },
      "source": [
        "### Validación preprocess train en local (0.25 puntos)\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de entrenamiento y validación en local."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFpirg37C3bN",
        "outputId": "cd36aa1b-f476-4d1a-ab67-dab641cde5c5"
      },
      "source": [
        "! python3 preprocess.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --runner DirectRunner \\\n",
        "  --input $WORK_DIR/data.json \\\n",
        "  --output $WORK_DIR/transformed_data \\\n",
        "  --mode train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f4214899b00> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f4214899c20> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f4214899cb0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f4214899d40> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f4214899dd0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f4214899ef0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f4214899f80> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f421489b050> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f421489b0e0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f421489b320> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f421489b290> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f421489b3b0> ====================\n",
            "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 100\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f421480fcd0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
            "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 100\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f421480f950> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/DoOnce/Impulse_19)+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_20))+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/DoOnce/Map(decode)_22))+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/InitializeWrite_23))+(ref_PCollection_PCollection_13/Write))+(ref_PCollection_PCollection_14/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/DoOnce/Impulse_35)+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_36))+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/DoOnce/Map(decode)_38))+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/InitializeWrite_39))+(ref_PCollection_PCollection_24/Write))+(ref_PCollection_PCollection_25/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_ReadTwitchData/Read/_SDFBoundedSourceWrapper/Impulse_5)+(ReadTwitchData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(ReadTwitchData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_1_split/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((((((((((ref_PCollection_PCollection_1_split/Read)+(ReadTwitchData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Split train-test dataset/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)_9))+(ref_AppliedPTransform_ExtractColumns_10))+(ref_AppliedPTransform_Preprocess_11))+(ref_AppliedPTransform_Split dataset/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)_14))+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)_24))+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)_40))+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/WriteBundles_25))+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/Pair_26))+(TrainWriteToCSV/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/WriteBundles_41))+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/Pair_42))+(EvalWriteToCSV/Write/WriteImpl/GroupByKey/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((TrainWriteToCSV/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/Extract_28))+(ref_PCollection_PCollection_19/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((ref_PCollection_PCollection_13/Read)+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/PreFinalize_29))+(ref_PCollection_PCollection_20/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((EvalWriteToCSV/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/Extract_44))+(ref_PCollection_PCollection_30/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((ref_PCollection_PCollection_24/Read)+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/PreFinalize_45))+(ref_PCollection_PCollection_31/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (ref_PCollection_PCollection_13/Read)+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/FinalizeWrite_30)\n",
            "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 2 (skipped: 0), batches: 2, num_threads: 2\n",
            "INFO:apache_beam.io.filebasedsink:Renamed 2 shards in 0.10 seconds.\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (ref_PCollection_PCollection_24/Read)+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/FinalizeWrite_46)\n",
            "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 2 (skipped: 0), batches: 2, num_threads: 2\n",
            "INFO:apache_beam.io.filebasedsink:Renamed 2 shards in 0.11 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkkG271a4qnA"
      },
      "source": [
        "### Validación preprocess test en local (0.25 puntos)\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de test en local."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4lkLgecLS3i",
        "outputId": "2637e0c4-e985-4330-bf98-98de5040f2b6"
      },
      "source": [
        "! python3 preprocess.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --runner DirectRunner \\\n",
        "  --input $WORK_DIR/data.json \\\n",
        "  --output $WORK_DIR/transformed_data \\\n",
        "  --mode test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f51686a7710> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f51686a7830> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f51686a78c0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f51686a7950> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f51686a79e0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f51686a7b00> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f51686a7b90> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f51686a7c20> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f51686a7cb0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f51686a7ef0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f51686a7e60> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f51686a7f80> ====================\n",
            "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 100\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f51683eefd0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
            "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 100\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f51683eeb90> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/DoOnce/Impulse_16)+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_17))+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/DoOnce/Map(decode)_19))+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/InitializeWrite_20))+(ref_PCollection_PCollection_10/Write))+(ref_PCollection_PCollection_11/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_ReadTwitchData/Read/_SDFBoundedSourceWrapper/Impulse_5)+(ReadTwitchData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(ReadTwitchData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_1_split/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((((ref_PCollection_PCollection_1_split/Read)+(ReadTwitchData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Split train-test dataset/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)_9))+(ref_AppliedPTransform_ExtractColumns_10))+(ref_AppliedPTransform_Preprocess_11))+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)_21))+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/WriteBundles_22))+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/Pair_23))+(TestWriteToCSV/Write/WriteImpl/GroupByKey/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((TestWriteToCSV/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/Extract_25))+(ref_PCollection_PCollection_16/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((ref_PCollection_PCollection_10/Read)+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/PreFinalize_26))+(ref_PCollection_PCollection_17/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (ref_PCollection_PCollection_10/Read)+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/FinalizeWrite_27)\n",
            "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 2 (skipped: 0), batches: 2, num_threads: 2\n",
            "INFO:apache_beam.io.filebasedsink:Renamed 2 shards in 0.10 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geZM9Sbj45LK"
      },
      "source": [
        "## Entregable 2 (1.25 puntos)\n",
        "\n",
        "Desarrollar una tarea de entrenamiento para los datos preprocesados. Requisitos:\n",
        "\n",
        "- Soportar ejecuciones en local usando el SDK de AI-Platform y ejecuciones en GCP con el mismo código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMUwXgm_5el-"
      },
      "source": [
        "Se crea el directorio donde se dejará este entregable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMi8dI1gLoIc"
      },
      "source": [
        "%mkdir /content/batch/trainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dJyMXTuNPwo",
        "outputId": "4bb5d4a6-a7fc-4998-d363-3cb5dd0c670b"
      },
      "source": [
        "%%writefile trainer/__init__.py\n",
        "\n",
        "version = \"0.1.0\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing trainer/__init__.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUu6gKaTL_S2",
        "outputId": "185c5879-3322-403e-8f20-9c3a91fc41b1"
      },
      "source": [
        "%%writefile trainer/task.py\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import argparse\n",
        "import multiprocessing as mp\n",
        "import logging\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "import pickle\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Embedding,\n",
        "    LSTM,\n",
        ")\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "# WORD2VEC\n",
        "W2V_SIZE = 300\n",
        "W2V_WINDOW = 7\n",
        "# 32\n",
        "W2V_EPOCH = 5\n",
        "W2V_MIN_COUNT = 10\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 300\n",
        "\n",
        "# SENTIMENT\n",
        "TROLL = \"TROLL\"\n",
        "NOTROLL = \"NOTROLL\"\n",
        "SENTIMENT_THRESHOLDS = (0.5)\n",
        "\n",
        "# EXPORT\n",
        "KERAS_MODEL = \"model.h5\"\n",
        "WORD2VEC_MODEL = \"model.w2v\"\n",
        "TOKENIZER_MODEL = \"tokenizer.pkl\"\n",
        "ENCODER_MODEL = \"encoder.pkl\"\n",
        "\n",
        "\n",
        "def generate_word2vec(train_df):\n",
        "    documents = [_text.split() for _text in train_df.text.values]\n",
        "    w2v_model = gensim.models.word2vec.Word2Vec(\n",
        "        size=W2V_SIZE,\n",
        "        window=W2V_WINDOW,\n",
        "        min_count=W2V_MIN_COUNT,\n",
        "        workers=mp.cpu_count(),\n",
        "    )\n",
        "    w2v_model.build_vocab(documents)\n",
        "\n",
        "    words = w2v_model.wv.vocab.keys()\n",
        "    vocab_size = len(words)\n",
        "    logging.info(f\"Vocab size: {vocab_size}\")\n",
        "    w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)\n",
        "\n",
        "    return w2v_model\n",
        "\n",
        "\n",
        "def generate_tokenizer(train_df):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(train_df.text)\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    logging.info(f\"Total words: {vocab_size}\")\n",
        "    return tokenizer, vocab_size\n",
        "\n",
        "\n",
        "def generate_label_encoder(train_df):\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(train_df.sentiment.tolist())\n",
        "    return encoder\n",
        "\n",
        "\n",
        "def generate_embedding(word2vec_model, vocab_size, tokenizer):\n",
        "    embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if word in word2vec_model.wv:\n",
        "            embedding_matrix[i] = word2vec_model.wv[word]\n",
        "    return Embedding(\n",
        "        vocab_size,\n",
        "        W2V_SIZE,\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=SEQUENCE_LENGTH,\n",
        "        trainable=False,\n",
        "    )\n",
        "\n",
        "\n",
        "def train_and_evaluate(\n",
        "    work_dir, train_df, eval_df, batch_size=1024, epochs=8, steps=100\n",
        "):\n",
        "\n",
        "    \"\"\"\n",
        "    Trains and evaluates the estimator given.\n",
        "    The input functions are generated by the preprocessing function.\n",
        "    \"\"\"\n",
        "\n",
        "    model_dir = os.path.join(work_dir, \"model\")\n",
        "    if tf.io.gfile.exists(model_dir):\n",
        "        tf.io.gfile.rmtree(model_dir)\n",
        "    tf.io.gfile.mkdir(model_dir)\n",
        "\n",
        "    # Specify where to store our model\n",
        "    run_config = tf.estimator.RunConfig()\n",
        "    run_config = run_config.replace(model_dir=model_dir)\n",
        "\n",
        "    # This will give us a more granular visualization of the training\n",
        "    run_config = run_config.replace(save_summary_steps=10)\n",
        "\n",
        "    # Create Word2vec of training data\n",
        "    logging.info(\"---- Generating word2vec model ----\")\n",
        "    word2vec_model = generate_word2vec(train_df)\n",
        "\n",
        "    # Tokenize training data\n",
        "    logging.info(\"---- Generating tokenizer ----\")\n",
        "    tokenizer, vocab_size = generate_tokenizer(train_df)\n",
        "\n",
        "    logging.info(\"---- Tokenizing train data ----\")\n",
        "    x_train = pad_sequences(\n",
        "        tokenizer.texts_to_sequences(train_df.text), maxlen=SEQUENCE_LENGTH\n",
        "    )\n",
        "    logging.info(\"---- Tokenizing eval data ----\")\n",
        "    x_eval = pad_sequences(\n",
        "        tokenizer.texts_to_sequences(eval_df.text), maxlen=SEQUENCE_LENGTH\n",
        "    )\n",
        "\n",
        "    # Label Encoder\n",
        "    logging.info(\"---- Generating label encoder ----\")\n",
        "    label_encoder = generate_label_encoder(train_df)\n",
        "\n",
        "    logging.info(\"---- Encoding train target ----\")\n",
        "    y_train = label_encoder.transform(train_df.sentiment.tolist())\n",
        "    logging.info(\"---- Encoding eval target ----\")\n",
        "    y_eval = label_encoder.transform(eval_df.sentiment.tolist())\n",
        "\n",
        "    y_train = y_train.reshape(-1, 1)\n",
        "    y_eval = y_eval.reshape(-1, 1)\n",
        "\n",
        "    # Create Embedding Layer\n",
        "    logging.info(\"---- Generating embedding layer ----\")\n",
        "    embedding_layer = generate_embedding(word2vec_model, vocab_size, tokenizer)\n",
        "\n",
        "    logging.info(\"---- Generating Sequential model ----\")\n",
        "    model = Sequential()\n",
        "    model.add(embedding_layer)\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    logging.info(\"---- Adding loss function to model ----\")\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "    logging.info(\"---- Adding callbacks to model ----\")\n",
        "    callbacks = [\n",
        "        ReduceLROnPlateau(monitor=\"val_loss\", patience=5, cooldown=0),\n",
        "        EarlyStopping(monitor=\"val_accuracy\", min_delta=1e-4, patience=5),\n",
        "    ]\n",
        "\n",
        "    logging.info(\"---- Training model ----\")\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        steps_per_epoch=steps,\n",
        "        epochs=epochs,\n",
        "        validation_split=0.1,\n",
        "        verbose=1,\n",
        "        callbacks=callbacks,\n",
        "    )\n",
        "\n",
        "    logging.info(\"---- Evaluating model ----\")\n",
        "    score = model.evaluate(x_eval, y_eval, batch_size=batch_size)\n",
        "    logging.info(f\"ACCURACY: {score[1]}\")\n",
        "    logging.info(f\"LOSS: {score[0]}\")\n",
        "\n",
        "    logging.info(\"---- Saving models ----\")\n",
        "    pickle.dump(\n",
        "        tokenizer,\n",
        "        tf.io.gfile.GFile(os.path.join(model_dir, TOKENIZER_MODEL), mode=\"wb\"),\n",
        "        protocol=0,\n",
        "    )\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".h5\") as local_file:\n",
        "        with tf.io.gfile.GFile(\n",
        "            os.path.join(model_dir, KERAS_MODEL), mode=\"wb\"\n",
        "        ) as gcs_file:\n",
        "            model.save(local_file.name)\n",
        "            gcs_file.write(local_file.read())\n",
        "\n",
        "    # word2vec_model.save(os.path.join(model_dir, WORD2VEC_MODEL))\n",
        "\n",
        "    # pickle.dump(\n",
        "    #     label_encoder, open(os.path.join(model_dir, ENCODER_MODEL), \"wb\"), protocol=0\n",
        "    # )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    \"\"\"Main function called by AI Platform.\"\"\"\n",
        "\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "    parser = argparse.ArgumentParser(\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--job-dir\",\n",
        "        help=\"Directory for staging trainer files. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--work-dir\",\n",
        "        required=True,\n",
        "        help=\"Directory for staging and working files. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--batch-size\",\n",
        "        type=int,\n",
        "        default=1024,\n",
        "        help=\"Batch size for training and evaluation.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--epochs\", type=int, default=8, help=\"Number of epochs to train the model\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--steps\",\n",
        "        type=int,\n",
        "        default=100,\n",
        "        help=\"Number of steps per epoch to train the model\",\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    train_data_files = tf.io.gfile.glob(\n",
        "        os.path.join(args.work_dir, \"transformed_data/train/part-*\")\n",
        "    )\n",
        "    eval_data_files = tf.io.gfile.glob(\n",
        "        os.path.join(args.work_dir, \"transformed_data/eval/part-*\")\n",
        "    )\n",
        "\n",
        "    train_df = pd.concat(\n",
        "        [\n",
        "            pd.read_csv(\n",
        "                f,\n",
        "                names=[\"text\", \"sentiment\"],\n",
        "                dtype={\"text\": \"string\", \"sentiment\": \"string\"},\n",
        "            )\n",
        "            for f in train_data_files\n",
        "        ]\n",
        "    ).dropna()\n",
        "\n",
        "    eval_df = pd.concat(\n",
        "        [\n",
        "            pd.read_csv(\n",
        "                f,\n",
        "                names=[\"text\", \"sentiment\"],\n",
        "                dtype={\"text\": \"string\", \"sentiment\": \"string\"},\n",
        "            )\n",
        "            for f in eval_data_files\n",
        "        ]\n",
        "    ).dropna()\n",
        "\n",
        "    train_and_evaluate(\n",
        "        args.work_dir,\n",
        "        train_df=train_df,\n",
        "        eval_df=eval_df,\n",
        "        batch_size=args.batch_size,\n",
        "        epochs=args.epochs,\n",
        "        steps=args.steps,\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing trainer/task.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8axPHdHX6d9W"
      },
      "source": [
        "### Validación Train en local\n",
        "\n",
        "Con el comando mostrado a continuación se valida el correcto entrenamiento del modelo usando los datos preprocesados del apartado anterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9997ZzsLmq-",
        "outputId": "0ec0db67-c973-4152-e2b3-cfb7d8bbfc4b"
      },
      "source": [
        "# Explicitly tell `gcloud ai-platform local train` to use Python 3 \n",
        "! gcloud config set ml_engine/local_python $(which python3)\n",
        "\n",
        "# This is similar to `python -m trainer.task --job-dir local-training-output`\n",
        "# but it better replicates the AI Platform environment, especially for\n",
        "# distributed training (not applicable here).\n",
        "! gcloud ai-platform local train \\\n",
        "  --package-path trainer \\\n",
        "  --module-name trainer.task \\\n",
        "  -- \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --epochs 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [ml_engine/local_python].\n",
            "2021-07-03 15:00:56.440423: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "INFO:tensorflow:TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--work-dir', '/content/batch', '--epochs', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}\n",
            "INFO:root:---- Generating word2vec model ----\n",
            "INFO:gensim.models.word2vec:collecting all words and their counts\n",
            "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "INFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 77308 words, keeping 11687 word types\n",
            "INFO:gensim.models.word2vec:collected 13227 word types from a corpus of 91618 raw words and 12750 sentences\n",
            "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
            "INFO:gensim.models.word2vec:effective_min_count=10 retains 1331 unique words (10% of original 13227, drops 11896)\n",
            "INFO:gensim.models.word2vec:effective_min_count=10 leaves 65696 word corpus (71% of original 91618, drops 25922)\n",
            "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 13227 items\n",
            "INFO:gensim.models.word2vec:sample=0.001 downsamples 60 most-common words\n",
            "INFO:gensim.models.word2vec:downsampling leaves estimated 54102 word corpus (82.4% of prior 65696)\n",
            "INFO:gensim.models.base_any2vec:estimated required memory for 1331 words and 300 dimensions: 3859900 bytes\n",
            "INFO:gensim.models.word2vec:resetting layer weights\n",
            "INFO:root:Vocab size: 1331\n",
            "INFO:gensim.models.base_any2vec:training model with 2 workers on 1331 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=7\n",
            "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
            "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
            "INFO:gensim.models.base_any2vec:EPOCH - 1 : training on 91618 raw words (54090 effective words) took 0.1s, 470771 effective words/s\n",
            "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
            "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
            "INFO:gensim.models.base_any2vec:EPOCH - 2 : training on 91618 raw words (54108 effective words) took 0.1s, 445142 effective words/s\n",
            "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
            "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
            "INFO:gensim.models.base_any2vec:EPOCH - 3 : training on 91618 raw words (54142 effective words) took 0.1s, 495496 effective words/s\n",
            "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
            "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
            "INFO:gensim.models.base_any2vec:EPOCH - 4 : training on 91618 raw words (54203 effective words) took 0.1s, 520884 effective words/s\n",
            "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
            "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
            "INFO:gensim.models.base_any2vec:EPOCH - 5 : training on 91618 raw words (54111 effective words) took 0.1s, 470888 effective words/s\n",
            "INFO:gensim.models.base_any2vec:training on a 458090 raw words (270654 effective words) took 0.6s, 453508 effective words/s\n",
            "INFO:root:---- Generating tokenizer ----\n",
            "INFO:root:Total words: 13228\n",
            "INFO:root:---- Tokenizing train data ----\n",
            "INFO:root:---- Tokenizing eval data ----\n",
            "INFO:root:---- Generating label encoder ----\n",
            "INFO:root:---- Encoding train target ----\n",
            "INFO:root:---- Encoding eval target ----\n",
            "INFO:root:---- Generating embedding layer ----\n",
            "INFO:root:---- Generating Sequential model ----\n",
            "2021-07-03 15:00:59.587755: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2021-07-03 15:00:59.695298: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-07-03 15:00:59.695354: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ed70ec286c9a): /proc/driver/nvidia/version does not exist\n",
            "2021-07-03 15:00:59.695771: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-07-03 15:00:59.701082: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200220000 Hz\n",
            "2021-07-03 15:00:59.701327: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562271538a00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-07-03 15:00:59.701365: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 300, 300)          3968400   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 300, 300)          0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               160400    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 4,128,901\n",
            "Trainable params: 160,501\n",
            "Non-trainable params: 3,968,400\n",
            "_________________________________________________________________\n",
            "INFO:root:---- Adding loss function to model ----\n",
            "INFO:root:---- Adding callbacks to model ----\n",
            "INFO:root:---- Training model ----\n",
            "2021-07-03 15:01:02.352477: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 368640000 exceeds 10% of free system memory.\n",
            "2021-07-03 15:01:02.486996: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 368640000 exceeds 10% of free system memory.\n",
            "2021-07-03 15:01:03.192472: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 368640000 exceeds 10% of free system memory.\n",
            "2021-07-03 15:01:03.403101: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 368640000 exceeds 10% of free system memory.\n",
            "  1/100 [..............................] - ETA: 0s - loss: 0.6942 - accuracy: 0.51072021-07-03 15:01:17.557867: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 368640000 exceeds 10% of free system memory.\n",
            " 12/100 [==>...........................] - ETA: 14:32 - loss: 0.6898 - accuracy: 0.5575WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 100 batches). You may need to use the repeat() function when building your dataset.\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 100 batches). You may need to use the repeat() function when building your dataset.\n",
            " 12/100 [==>...........................] - 122s 10s/step - loss: 0.6898 - accuracy: 0.5575 - val_loss: 0.5620 - val_accuracy: 1.0000\n",
            "INFO:root:---- Evaluating model ----\n",
            "4/4 [==============================] - 5s 1s/step - loss: 0.6737 - accuracy: 0.6105\n",
            "INFO:root:ACCURACY: 0.6105362176895142\n",
            "INFO:root:LOSS: 0.673708438873291\n",
            "INFO:root:---- Saving models ----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nhlkcjn62Zo"
      },
      "source": [
        "## Entregable 3 (0.5 puntos)\n",
        "\n",
        "Desarrollar un pipeline de inferencia utilizando Apache Beam para generar predicciones usando los modelos generados en el apartado anterior así como los de test generados en el primer entregable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHBpLXB-OmjT",
        "outputId": "5b037263-1409-425d-f616-133d0b7445e6"
      },
      "source": [
        "%%writefile predict.py\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import tempfile\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import apache_beam as beam\n",
        "from apache_beam.io import ReadFromText\n",
        "from apache_beam.io import WriteToText\n",
        "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "from apache_beam.options.pipeline_options import SetupOptions\n",
        "from apache_beam.coders.coders import Coder\n",
        "\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 300\n",
        "\n",
        "# SENTIMENT\n",
        "TROLL = \"TROLL\"\n",
        "NOTROLL = \"NOTROLL\"\n",
        "#NEUTRAL = \"NEUTRAL\"\n",
        "SENTIMENT_THRESHOLDS = (0.5)\n",
        "\n",
        "# EXPORT\n",
        "KERAS_MODEL = \"model.h5\"\n",
        "TOKENIZER_MODEL = \"tokenizer.pkl\"\n",
        "class Predict(beam.DoFn):\n",
        "    def __init__(\n",
        "        self, model_dir,\n",
        "    ):\n",
        "        self.model_dir = model_dir\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "    def setup(self):\n",
        "        keras_model_path = os.path.join(self.model_dir, KERAS_MODEL)\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".h5\") as local_file:\n",
        "            with tf.io.gfile.GFile(keras_model_path, mode=\"rb\") as gcs_file:\n",
        "                local_file.write(gcs_file.read())\n",
        "                self.model = tf.keras.models.load_model(local_file.name)\n",
        "\n",
        "        tokenizer_path = os.path.join(self.model_dir, TOKENIZER_MODEL)\n",
        "        self.tokenizer = pickle.load(tf.io.gfile.GFile(tokenizer_path, mode=\"rb\"))\n",
        "\n",
        "    def decode_sentiment(self, score, include_neutral=False):\n",
        "        if include_neutral:\n",
        "            label = NEUTRAL\n",
        "            if score <= SENTIMENT_THRESHOLDS[0]:\n",
        "                label = NOTROLL\n",
        "            elif score >= SENTIMENT_THRESHOLDS[1]:\n",
        "                label = TROLL\n",
        "\n",
        "            return label\n",
        "        else:\n",
        "            return NOTROLL if score < 0.5 else TROLL\n",
        "\n",
        "    def process(self, element):\n",
        "        start_at = time.time()\n",
        "        # Tokenize text\n",
        "        x_test = pad_sequences(\n",
        "            self.tokenizer.texts_to_sequences([element]), maxlen=SEQUENCE_LENGTH\n",
        "        )\n",
        "        # Predict\n",
        "        score = self.model.predict([x_test])[0]\n",
        "        # Decode sentiment\n",
        "        label = self.decode_sentiment(score)\n",
        "\n",
        "        yield {\n",
        "            \"text\": element,\n",
        "            \"label\": label,\n",
        "            \"score\": float(score),\n",
        "            \"elapsed_time\": time.time() - start_at,\n",
        "        }\n",
        "\n",
        "\n",
        "class CustomCoder(Coder):\n",
        "    \"\"\"A custom coder used for reading and writing strings\"\"\"\n",
        "\n",
        "    def __init__(self, encoding: str):\n",
        "        # latin-1\n",
        "        # iso-8859-1\n",
        "        self.enconding = encoding\n",
        "\n",
        "    def encode(self, value):\n",
        "        return value.encode(self.enconding)\n",
        "\n",
        "    def decode(self, value):\n",
        "        return value.decode(self.enconding)\n",
        "\n",
        "    def is_deterministic(self):\n",
        "        return True\n",
        "\n",
        "\n",
        "def run(model_dir, source, sink, beam_options=None):\n",
        "    with beam.Pipeline(options=beam_options) as p:\n",
        "        _ = (\n",
        "            p\n",
        "            | \"Read data\" >> source\n",
        "            # | \"Preprocess\" >> beam.ParDo(PreprocessTextFn(model_dir, \"ID\"))\n",
        "            | \"Predict\" >> beam.ParDo(Predict(model_dir))\n",
        "            | \"Format as JSON\" >> beam.Map(json.dumps)\n",
        "            | \"Write predictions\" >> sink\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\"Main function\"\"\"\n",
        "    parser = argparse.ArgumentParser(\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--work-dir\",\n",
        "        dest=\"work_dir\",\n",
        "        required=True,\n",
        "        help=\"Directory for temporary files and preprocessed datasets to. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--model-dir\",\n",
        "        dest=\"model_dir\",\n",
        "        required=True,\n",
        "        help=\"Path to the exported TensorFlow model. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    verbs = parser.add_subparsers(dest=\"verb\")\n",
        "    batch_verb = verbs.add_parser(\"batch\", help=\"Batch prediction\")\n",
        "    batch_verb.add_argument(\n",
        "        \"--inputs-dir\",\n",
        "        dest=\"inputs_dir\",\n",
        "        required=True,\n",
        "        help=\"Input directory where CSV data files are read from. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "    batch_verb.add_argument(\n",
        "        \"--outputs-dir\",\n",
        "        dest=\"outputs_dir\",\n",
        "        required=True,\n",
        "        help=\"Directory to store prediction results. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    args, pipeline_args = parser.parse_known_args()\n",
        "    print(args)\n",
        "    beam_options = PipelineOptions(pipeline_args)\n",
        "    beam_options.view_as(SetupOptions).save_main_session = True\n",
        "    # beam_options.view_as(DirectOptions).direct_num_workers = 0\n",
        "\n",
        "    project = beam_options.view_as(GoogleCloudOptions).project\n",
        "\n",
        "    if args.verb == \"batch\":\n",
        "        results_prefix = os.path.join(args.outputs_dir, \"part\")\n",
        "\n",
        "        source = ReadFromText(args.inputs_dir, coder=CustomCoder(\"latin-1\"))\n",
        "        sink = WriteToText(results_prefix)\n",
        "\n",
        "    else:\n",
        "        parser.print_usage()\n",
        "        sys.exit(1)\n",
        "\n",
        "    run(args.model_dir, source, sink, beam_options)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing predict.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUdgcLIP7a42"
      },
      "source": [
        "Generamos un timestamp para la ejecución de las predicciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pvKMtuQPOlr"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# current date and time\n",
        "TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkLMdMup70ne"
      },
      "source": [
        "### Validación Predict en local\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta inferencia usando los modelos anteriores y los datos de test generados anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUJN0XCLPR_X",
        "outputId": "4d9878a6-7609-40e2-92e7-a77210923e8a"
      },
      "source": [
        "! python3 predict.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --model-dir $WORK_DIR/model \\\n",
        "  batch \\\n",
        "  --inputs-dir $WORK_DIR/transformed_data/test/part* \\\n",
        "  --outputs-dir $WORK_DIR/predictions/$TIMESTAMP"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-03 15:03:56.572555: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Namespace(inputs_dir='/content/batch/transformed_data/test/part-00000-of-00002', model_dir='/content/batch/model', outputs_dir='/content/batch/predictions/2021-07-03_15-03-50', verb='batch', work_dir='/content/batch')\n",
            "2021-07-03 15:03:59.340629: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2021-07-03 15:03:59.352429: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-07-03 15:03:59.352494: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ed70ec286c9a): /proc/driver/nvidia/version does not exist\n",
            "2021-07-03 15:03:59.352916: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-07-03 15:03:59.359008: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200220000 Hz\n",
            "2021-07-03 15:03:59.359262: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5642351801c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-07-03 15:03:59.359303: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um68jx_F8mjF"
      },
      "source": [
        "##Entregable 4 (1.25 puntos)\n",
        "\n",
        "En este entregable se validará el funcionamiento del código en un proyecto de GCP sobre DataFlow y AI Platform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JHC9dCT83FH"
      },
      "source": [
        "Establecemos el bucket y region de GCP sobre el que trabajaremos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgUpX__KQvt-"
      },
      "source": [
        "GCP_WORK_DIR = 'gs://twitch-practiceeva'\n",
        "GCP_REGION = 'europe-west1'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A66WAfm9DmU"
      },
      "source": [
        "### Validación preprocess train en Dataflow (0.25 puntos)\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de entrenamiento y validación en GCP con el servicio DataFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJYVuWyJP1Ya",
        "outputId": "102d1b10-4182-4acb-8ba7-932a1054ced4"
      },
      "source": [
        "! python3 preprocess.py \\\n",
        "  --project $PROJECT_ID \\\n",
        "  --region $GCP_REGION \\\n",
        "  --runner DataflowRunner \\\n",
        "  --temp_location $GCP_WORK_DIR/beam-temp \\\n",
        "  --setup_file ./setup.py \\\n",
        "  --work-dir $GCP_WORK_DIR \\\n",
        "  --input $GCP_WORK_DIR/data.json \\\n",
        "  --output $GCP_WORK_DIR/transformed_data \\\n",
        "  --mode train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n",
            "INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n",
            "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
            "INFO:oauth2client.client:Refreshing access_token\n",
            "INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', 'setup.py', 'sdist', '--dist-dir', '/tmp/tmpaw76h72_']\n",
            "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
            "\n",
            "warning: check: missing required meta-data: url\n",
            "\n",
            "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
            "\n",
            "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
            "INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmpaw76h72_', 'apache-beam==2.24.0', '--no-deps', '--no-binary', ':all:']\n",
            "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
            "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
            "INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmpaw76h72_', 'apache-beam==2.24.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n",
            "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl\n",
            "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
            "INFO:root:Using Python SDK docker image: apache/beam_python3.7_sdk:2.24.0. If the image is not available at local, we will try to pull from hub.docker.com\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Defaulting to the temp_location as staging_location: gs://twitch-practiceeva/beam-temp\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://twitch-practiceeva/beam-temp/beamapp-root-0702160826-343065.1625242106.343406/pipeline.pb...\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://twitch-practiceeva/beam-temp/beamapp-root-0702160826-343065.1625242106.343406/pipeline.pb in 0 seconds.\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://twitch-practiceeva/beam-temp/beamapp-root-0702160826-343065.1625242106.343406/workflow.tar.gz...\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://twitch-practiceeva/beam-temp/beamapp-root-0702160826-343065.1625242106.343406/workflow.tar.gz in 0 seconds.\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://twitch-practiceeva/beam-temp/beamapp-root-0702160826-343065.1625242106.343406/pickled_main_session...\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://twitch-practiceeva/beam-temp/beamapp-root-0702160826-343065.1625242106.343406/pickled_main_session in 0 seconds.\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://twitch-practiceeva/beam-temp/beamapp-root-0702160826-343065.1625242106.343406/dataflow_python_sdk.tar...\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://twitch-practiceeva/beam-temp/beamapp-root-0702160826-343065.1625242106.343406/dataflow_python_sdk.tar in 1 seconds.\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://twitch-practiceeva/beam-temp/beamapp-root-0702160826-343065.1625242106.343406/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl...\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://twitch-practiceeva/beam-temp/beamapp-root-0702160826-343065.1625242106.343406/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl in 4 seconds.\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
            " createTime: '2021-07-02T16:08:36.758044Z'\n",
            " currentStateTime: '1970-01-01T00:00:00Z'\n",
            " id: '2021-07-02_09_08_34-2947546548626551532'\n",
            " location: 'europe-west1'\n",
            " name: 'beamapp-root-0702160826-343065'\n",
            " projectId: 'twitch-practiceeva'\n",
            " stageStates: []\n",
            " startTime: '2021-07-02T16:08:36.758044Z'\n",
            " steps: []\n",
            " tempFiles: []\n",
            " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2021-07-02_09_08_34-2947546548626551532]\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2021-07-02_09_08_34-2947546548626551532\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/europe-west1/2021-07-02_09_08_34-2947546548626551532?project=twitch-practiceeva\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2021-07-02_09_08_34-2947546548626551532 is in state JOB_STATE_PENDING\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:38.984Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2021-07-02_09_08_34-2947546548626551532. The number of workers will be between 1 and 1000.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:39.071Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2021-07-02_09_08_34-2947546548626551532.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:40.668Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in europe-west1-d.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:41.671Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:41.706Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step EvalWriteToCSV/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:41.738Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step TrainWriteToCSV/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:41.781Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:41.816Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:41.919Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:41.960Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:41.995Z: JOB_MESSAGE_DETAILED: Fusing consumer Split train-test dataset/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn) into ReadTwitchData/Read\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:42.028Z: JOB_MESSAGE_DETAILED: Fusing consumer ExtractColumns into Split train-test dataset/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:42.061Z: JOB_MESSAGE_DETAILED: Fusing consumer Preprocess into ExtractColumns\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:42.094Z: JOB_MESSAGE_DETAILED: Fusing consumer Split dataset/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn) into Preprocess\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:42.138Z: JOB_MESSAGE_DETAILED: Fusing consumer TrainWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn) into Split dataset/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:42.171Z: JOB_MESSAGE_DETAILED: Fusing consumer TrainWriteToCSV/Write/WriteImpl/WriteBundles/WriteBundles into TrainWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:42.215Z: JOB_MESSAGE_DETAILED: Fusing consumer TrainWriteToCSV/Write/WriteImpl/Pair into TrainWriteToCSV/Write/WriteImpl/WriteBundles/WriteBundles\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:42.250Z: JOB_MESSAGE_DETAILED: Fusing consumer TrainWriteToCSV/Write/WriteImpl/GroupByKey/Reify into TrainWriteToCSV/Write/WriteImpl/Pair\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:42.301Z: JOB_MESSAGE_DETAILED: Fusing consumer TrainWriteToCSV/Write/WriteImpl/GroupByKey/Write into TrainWriteToCSV/Write/WriteImpl/GroupByKey/Reify\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:42.329Z: JOB_MESSAGE_DETAILED: Fusing consumer TrainWriteToCSV/Write/WriteImpl/GroupByKey/GroupByWindow into TrainWriteToCSV/Write/WriteImpl/GroupByKey/Read\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:42.362Z: JOB_MESSAGE_DETAILED: Fusing consumer TrainWriteToCSV/Write/WriteImpl/Extract into TrainWriteToCSV/Write/WriteImpl/GroupByKey/GroupByWindow\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:42.396Z: JOB_MESSAGE_DETAILED: Fusing consumer EvalWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn) into Split dataset/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:42.428Z: JOB_MESSAGE_DETAILED: Fusing consumer EvalWriteToCSV/Write/WriteImpl/WriteBundles/WriteBundles into EvalWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:42.464Z: JOB_MESSAGE_DETAILED: Fusing consumer EvalWriteToCSV/Write/WriteImpl/Pair into EvalWriteToCSV/Write/WriteImpl/WriteBundles/WriteBundles\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:42.486Z: JOB_MESSAGE_DETAILED: Fusing consumer EvalWriteToCSV/Write/WriteImpl/GroupByKey/Reify into EvalWriteToCSV/Write/WriteImpl/Pair\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:42.518Z: JOB_MESSAGE_DETAILED: Fusing consumer EvalWriteToCSV/Write/WriteImpl/GroupByKey/Write into EvalWriteToCSV/Write/WriteImpl/GroupByKey/Reify\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:42.552Z: JOB_MESSAGE_DETAILED: Fusing consumer EvalWriteToCSV/Write/WriteImpl/GroupByKey/GroupByWindow into EvalWriteToCSV/Write/WriteImpl/GroupByKey/Read\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:42.585Z: JOB_MESSAGE_DETAILED: Fusing consumer EvalWriteToCSV/Write/WriteImpl/Extract into EvalWriteToCSV/Write/WriteImpl/GroupByKey/GroupByWindow\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:42.618Z: JOB_MESSAGE_DETAILED: Fusing consumer TrainWriteToCSV/Write/WriteImpl/InitializeWrite into TrainWriteToCSV/Write/WriteImpl/DoOnce/Read\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:42.661Z: JOB_MESSAGE_DETAILED: Fusing consumer EvalWriteToCSV/Write/WriteImpl/InitializeWrite into EvalWriteToCSV/Write/WriteImpl/DoOnce/Read\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:42.702Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:42.729Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:42.762Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:42.795Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:43.120Z: JOB_MESSAGE_DEBUG: Executing wait step start29\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:43.186Z: JOB_MESSAGE_BASIC: Executing operation EvalWriteToCSV/Write/WriteImpl/DoOnce/Read+EvalWriteToCSV/Write/WriteImpl/InitializeWrite\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:43.218Z: JOB_MESSAGE_BASIC: Executing operation TrainWriteToCSV/Write/WriteImpl/DoOnce/Read+TrainWriteToCSV/Write/WriteImpl/InitializeWrite\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:43.232Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:08:43.262Z: JOB_MESSAGE_BASIC: Starting 1 workers in europe-west1-d...\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2021-07-02_09_08_34-2947546548626551532 is in state JOB_STATE_RUNNING\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:10:03.673Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:10:03.704Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:55.906Z: JOB_MESSAGE_BASIC: Finished operation EvalWriteToCSV/Write/WriteImpl/DoOnce/Read+EvalWriteToCSV/Write/WriteImpl/InitializeWrite\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:55.965Z: JOB_MESSAGE_DEBUG: Value \"EvalWriteToCSV/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:55.988Z: JOB_MESSAGE_DEBUG: Value \"EvalWriteToCSV/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:56.058Z: JOB_MESSAGE_BASIC: Executing operation EvalWriteToCSV/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:56.090Z: JOB_MESSAGE_BASIC: Executing operation EvalWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:56.110Z: JOB_MESSAGE_BASIC: Finished operation EvalWriteToCSV/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:56.123Z: JOB_MESSAGE_BASIC: Executing operation EvalWriteToCSV/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:56.142Z: JOB_MESSAGE_BASIC: Finished operation EvalWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:56.180Z: JOB_MESSAGE_DEBUG: Value \"EvalWriteToCSV/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:56.185Z: JOB_MESSAGE_BASIC: Finished operation EvalWriteToCSV/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:56.214Z: JOB_MESSAGE_DEBUG: Value \"EvalWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:56.246Z: JOB_MESSAGE_DEBUG: Value \"EvalWriteToCSV/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:59.316Z: JOB_MESSAGE_BASIC: Finished operation TrainWriteToCSV/Write/WriteImpl/DoOnce/Read+TrainWriteToCSV/Write/WriteImpl/InitializeWrite\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:59.376Z: JOB_MESSAGE_DEBUG: Value \"TrainWriteToCSV/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:59.410Z: JOB_MESSAGE_DEBUG: Value \"TrainWriteToCSV/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:59.481Z: JOB_MESSAGE_BASIC: Executing operation TrainWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:59.509Z: JOB_MESSAGE_BASIC: Executing operation TrainWriteToCSV/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:59.528Z: JOB_MESSAGE_BASIC: Finished operation TrainWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:59.542Z: JOB_MESSAGE_BASIC: Executing operation TrainWriteToCSV/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:59.551Z: JOB_MESSAGE_BASIC: Finished operation TrainWriteToCSV/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:59.594Z: JOB_MESSAGE_BASIC: Finished operation TrainWriteToCSV/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:59.595Z: JOB_MESSAGE_DEBUG: Value \"TrainWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:59.629Z: JOB_MESSAGE_DEBUG: Value \"TrainWriteToCSV/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:59.662Z: JOB_MESSAGE_DEBUG: Value \"TrainWriteToCSV/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:59.706Z: JOB_MESSAGE_BASIC: Executing operation EvalWriteToCSV/Write/WriteImpl/GroupByKey/Create\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:15:59.737Z: JOB_MESSAGE_BASIC: Executing operation TrainWriteToCSV/Write/WriteImpl/GroupByKey/Create\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:00.015Z: JOB_MESSAGE_BASIC: Finished operation TrainWriteToCSV/Write/WriteImpl/GroupByKey/Create\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:00.015Z: JOB_MESSAGE_BASIC: Finished operation EvalWriteToCSV/Write/WriteImpl/GroupByKey/Create\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:00.100Z: JOB_MESSAGE_DEBUG: Value \"EvalWriteToCSV/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:00.123Z: JOB_MESSAGE_DEBUG: Value \"TrainWriteToCSV/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:00.199Z: JOB_MESSAGE_BASIC: Executing operation ReadTwitchData/Read+Split train-test dataset/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)+ExtractColumns+Preprocess+Split dataset/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)+TrainWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)+TrainWriteToCSV/Write/WriteImpl/WriteBundles/WriteBundles+TrainWriteToCSV/Write/WriteImpl/Pair+TrainWriteToCSV/Write/WriteImpl/GroupByKey/Reify+TrainWriteToCSV/Write/WriteImpl/GroupByKey/Write+EvalWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)+EvalWriteToCSV/Write/WriteImpl/WriteBundles/WriteBundles+EvalWriteToCSV/Write/WriteImpl/Pair+EvalWriteToCSV/Write/WriteImpl/GroupByKey/Reify+EvalWriteToCSV/Write/WriteImpl/GroupByKey/Write\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:17.267Z: JOB_MESSAGE_BASIC: Finished operation ReadTwitchData/Read+Split train-test dataset/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)+ExtractColumns+Preprocess+Split dataset/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)+TrainWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)+TrainWriteToCSV/Write/WriteImpl/WriteBundles/WriteBundles+TrainWriteToCSV/Write/WriteImpl/Pair+TrainWriteToCSV/Write/WriteImpl/GroupByKey/Reify+TrainWriteToCSV/Write/WriteImpl/GroupByKey/Write+EvalWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)+EvalWriteToCSV/Write/WriteImpl/WriteBundles/WriteBundles+EvalWriteToCSV/Write/WriteImpl/Pair+EvalWriteToCSV/Write/WriteImpl/GroupByKey/Reify+EvalWriteToCSV/Write/WriteImpl/GroupByKey/Write\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:17.337Z: JOB_MESSAGE_BASIC: Executing operation EvalWriteToCSV/Write/WriteImpl/GroupByKey/Close\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:17.359Z: JOB_MESSAGE_BASIC: Executing operation TrainWriteToCSV/Write/WriteImpl/GroupByKey/Close\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:17.394Z: JOB_MESSAGE_BASIC: Finished operation EvalWriteToCSV/Write/WriteImpl/GroupByKey/Close\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:17.419Z: JOB_MESSAGE_BASIC: Finished operation TrainWriteToCSV/Write/WriteImpl/GroupByKey/Close\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:17.476Z: JOB_MESSAGE_BASIC: Executing operation EvalWriteToCSV/Write/WriteImpl/GroupByKey/Read+EvalWriteToCSV/Write/WriteImpl/GroupByKey/GroupByWindow+EvalWriteToCSV/Write/WriteImpl/Extract\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:17.501Z: JOB_MESSAGE_BASIC: Executing operation TrainWriteToCSV/Write/WriteImpl/GroupByKey/Read+TrainWriteToCSV/Write/WriteImpl/GroupByKey/GroupByWindow+TrainWriteToCSV/Write/WriteImpl/Extract\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:34.135Z: JOB_MESSAGE_BASIC: Finished operation TrainWriteToCSV/Write/WriteImpl/GroupByKey/Read+TrainWriteToCSV/Write/WriteImpl/GroupByKey/GroupByWindow+TrainWriteToCSV/Write/WriteImpl/Extract\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:34.206Z: JOB_MESSAGE_DEBUG: Value \"TrainWriteToCSV/Write/WriteImpl/Extract.out\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:34.282Z: JOB_MESSAGE_BASIC: Executing operation TrainWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:34.315Z: JOB_MESSAGE_BASIC: Executing operation TrainWriteToCSV/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:34.335Z: JOB_MESSAGE_BASIC: Finished operation TrainWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:34.366Z: JOB_MESSAGE_BASIC: Finished operation TrainWriteToCSV/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:34.402Z: JOB_MESSAGE_DEBUG: Value \"TrainWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0).output\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:34.435Z: JOB_MESSAGE_DEBUG: Value \"TrainWriteToCSV/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0).output\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:34.512Z: JOB_MESSAGE_BASIC: Executing operation TrainWriteToCSV/Write/WriteImpl/PreFinalize/PreFinalize\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:37.684Z: JOB_MESSAGE_BASIC: Finished operation EvalWriteToCSV/Write/WriteImpl/GroupByKey/Read+EvalWriteToCSV/Write/WriteImpl/GroupByKey/GroupByWindow+EvalWriteToCSV/Write/WriteImpl/Extract\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:37.748Z: JOB_MESSAGE_DEBUG: Value \"EvalWriteToCSV/Write/WriteImpl/Extract.out\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:37.823Z: JOB_MESSAGE_BASIC: Executing operation EvalWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:37.854Z: JOB_MESSAGE_BASIC: Executing operation EvalWriteToCSV/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:37.876Z: JOB_MESSAGE_BASIC: Finished operation EvalWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:37.902Z: JOB_MESSAGE_BASIC: Finished operation EvalWriteToCSV/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:37.943Z: JOB_MESSAGE_DEBUG: Value \"EvalWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0).output\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:37.976Z: JOB_MESSAGE_DEBUG: Value \"EvalWriteToCSV/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0).output\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:38.045Z: JOB_MESSAGE_BASIC: Executing operation EvalWriteToCSV/Write/WriteImpl/PreFinalize/PreFinalize\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:41.527Z: JOB_MESSAGE_BASIC: Finished operation TrainWriteToCSV/Write/WriteImpl/PreFinalize/PreFinalize\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:41.585Z: JOB_MESSAGE_DEBUG: Value \"TrainWriteToCSV/Write/WriteImpl/PreFinalize.out\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:41.652Z: JOB_MESSAGE_BASIC: Executing operation TrainWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:41.700Z: JOB_MESSAGE_BASIC: Finished operation TrainWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:41.761Z: JOB_MESSAGE_DEBUG: Value \"TrainWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0).output\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:41.827Z: JOB_MESSAGE_BASIC: Executing operation TrainWriteToCSV/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:45.592Z: JOB_MESSAGE_BASIC: Finished operation EvalWriteToCSV/Write/WriteImpl/PreFinalize/PreFinalize\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:45.652Z: JOB_MESSAGE_DEBUG: Value \"EvalWriteToCSV/Write/WriteImpl/PreFinalize.out\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:45.717Z: JOB_MESSAGE_BASIC: Executing operation EvalWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:45.769Z: JOB_MESSAGE_BASIC: Finished operation EvalWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:45.828Z: JOB_MESSAGE_DEBUG: Value \"EvalWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0).output\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:45.896Z: JOB_MESSAGE_BASIC: Executing operation EvalWriteToCSV/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:49.455Z: JOB_MESSAGE_BASIC: Finished operation EvalWriteToCSV/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:53.807Z: JOB_MESSAGE_BASIC: Finished operation TrainWriteToCSV/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:53.867Z: JOB_MESSAGE_DEBUG: Executing success step success27\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:53.954Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:54.009Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:16:54.040Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:17:56.595Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:17:56.621Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2021-07-02_09_08_34-2947546548626551532 is in state JOB_STATE_DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8czKRMb99ZRX"
      },
      "source": [
        "### Validación preprocess test en Dataflow (0.25 puntos)\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de test en GCP con el servicio DataFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45mfSRGdQLBO",
        "outputId": "c462fd76-6535-47c3-e078-b1968a125f1a"
      },
      "source": [
        "! python3 preprocess.py \\\n",
        "  --project $PROJECT_ID \\\n",
        "  --region $GCP_REGION \\\n",
        "  --runner DataflowRunner \\\n",
        "  --temp_location $GCP_WORK_DIR/beam-temp \\\n",
        "  --setup_file ./setup.py \\\n",
        "  --work-dir $GCP_WORK_DIR \\\n",
        "  --input $GCP_WORK_DIR/data.json \\\n",
        "  --output $GCP_WORK_DIR/transformed_data \\\n",
        "  --mode test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n",
            "INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n",
            "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
            "INFO:oauth2client.client:Refreshing access_token\n",
            "INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', 'setup.py', 'sdist', '--dist-dir', '/tmp/tmpyxz5n33p']\n",
            "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
            "\n",
            "warning: check: missing required meta-data: url\n",
            "\n",
            "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
            "\n",
            "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
            "INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmpyxz5n33p', 'apache-beam==2.24.0', '--no-deps', '--no-binary', ':all:']\n",
            "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
            "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
            "INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmpyxz5n33p', 'apache-beam==2.24.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n",
            "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl\n",
            "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
            "INFO:root:Using Python SDK docker image: apache/beam_python3.7_sdk:2.24.0. If the image is not available at local, we will try to pull from hub.docker.com\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Defaulting to the temp_location as staging_location: gs://twitch-practiceeva/beam-temp\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://twitch-practiceeva/beam-temp/beamapp-root-0702162004-239989.1625242804.240396/pipeline.pb...\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://twitch-practiceeva/beam-temp/beamapp-root-0702162004-239989.1625242804.240396/pipeline.pb in 0 seconds.\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://twitch-practiceeva/beam-temp/beamapp-root-0702162004-239989.1625242804.240396/workflow.tar.gz...\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://twitch-practiceeva/beam-temp/beamapp-root-0702162004-239989.1625242804.240396/workflow.tar.gz in 0 seconds.\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://twitch-practiceeva/beam-temp/beamapp-root-0702162004-239989.1625242804.240396/pickled_main_session...\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://twitch-practiceeva/beam-temp/beamapp-root-0702162004-239989.1625242804.240396/pickled_main_session in 0 seconds.\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://twitch-practiceeva/beam-temp/beamapp-root-0702162004-239989.1625242804.240396/dataflow_python_sdk.tar...\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://twitch-practiceeva/beam-temp/beamapp-root-0702162004-239989.1625242804.240396/dataflow_python_sdk.tar in 1 seconds.\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://twitch-practiceeva/beam-temp/beamapp-root-0702162004-239989.1625242804.240396/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl...\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://twitch-practiceeva/beam-temp/beamapp-root-0702162004-239989.1625242804.240396/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl in 3 seconds.\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
            " createTime: '2021-07-02T16:20:12.072238Z'\n",
            " currentStateTime: '1970-01-01T00:00:00Z'\n",
            " id: '2021-07-02_09_20_11-6511883588189039684'\n",
            " location: 'europe-west1'\n",
            " name: 'beamapp-root-0702162004-239989'\n",
            " projectId: 'twitch-practiceeva'\n",
            " stageStates: []\n",
            " startTime: '2021-07-02T16:20:12.072238Z'\n",
            " steps: []\n",
            " tempFiles: []\n",
            " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2021-07-02_09_20_11-6511883588189039684]\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2021-07-02_09_20_11-6511883588189039684\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/europe-west1/2021-07-02_09_20_11-6511883588189039684?project=twitch-practiceeva\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2021-07-02_09_20_11-6511883588189039684 is in state JOB_STATE_PENDING\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:14.424Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2021-07-02_09_20_11-6511883588189039684. The number of workers will be between 1 and 1000.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:14.478Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2021-07-02_09_20_11-6511883588189039684.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:16.189Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in europe-west1-b.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:17.105Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:17.138Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step TestWriteToCSV/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:17.172Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:17.192Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:17.247Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:17.286Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:17.303Z: JOB_MESSAGE_DETAILED: Fusing consumer Split train-test dataset/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn) into ReadTwitchData/Read\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:17.325Z: JOB_MESSAGE_DETAILED: Fusing consumer ExtractColumns into Split train-test dataset/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:17.356Z: JOB_MESSAGE_DETAILED: Fusing consumer Preprocess into ExtractColumns\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:17.390Z: JOB_MESSAGE_DETAILED: Fusing consumer TestWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn) into Preprocess\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:17.424Z: JOB_MESSAGE_DETAILED: Fusing consumer TestWriteToCSV/Write/WriteImpl/WriteBundles/WriteBundles into TestWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:17.450Z: JOB_MESSAGE_DETAILED: Fusing consumer TestWriteToCSV/Write/WriteImpl/Pair into TestWriteToCSV/Write/WriteImpl/WriteBundles/WriteBundles\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:17.473Z: JOB_MESSAGE_DETAILED: Fusing consumer TestWriteToCSV/Write/WriteImpl/GroupByKey/Reify into TestWriteToCSV/Write/WriteImpl/Pair\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:17.498Z: JOB_MESSAGE_DETAILED: Fusing consumer TestWriteToCSV/Write/WriteImpl/GroupByKey/Write into TestWriteToCSV/Write/WriteImpl/GroupByKey/Reify\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:17.520Z: JOB_MESSAGE_DETAILED: Fusing consumer TestWriteToCSV/Write/WriteImpl/GroupByKey/GroupByWindow into TestWriteToCSV/Write/WriteImpl/GroupByKey/Read\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:17.543Z: JOB_MESSAGE_DETAILED: Fusing consumer TestWriteToCSV/Write/WriteImpl/Extract into TestWriteToCSV/Write/WriteImpl/GroupByKey/GroupByWindow\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:17.577Z: JOB_MESSAGE_DETAILED: Fusing consumer TestWriteToCSV/Write/WriteImpl/InitializeWrite into TestWriteToCSV/Write/WriteImpl/DoOnce/Read\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:17.614Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:17.641Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:17.673Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:17.695Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:17.837Z: JOB_MESSAGE_DEBUG: Executing wait step start16\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:17.917Z: JOB_MESSAGE_BASIC: Executing operation TestWriteToCSV/Write/WriteImpl/DoOnce/Read+TestWriteToCSV/Write/WriteImpl/InitializeWrite\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:17.962Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:20:17.992Z: JOB_MESSAGE_BASIC: Starting 1 workers in europe-west1-b...\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2021-07-02_09_20_11-6511883588189039684 is in state JOB_STATE_RUNNING\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:21:25.778Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:21:25.803Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:26:50.185Z: JOB_MESSAGE_BASIC: Finished operation TestWriteToCSV/Write/WriteImpl/DoOnce/Read+TestWriteToCSV/Write/WriteImpl/InitializeWrite\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:26:50.249Z: JOB_MESSAGE_DEBUG: Value \"TestWriteToCSV/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:26:50.272Z: JOB_MESSAGE_DEBUG: Value \"TestWriteToCSV/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:26:50.326Z: JOB_MESSAGE_BASIC: Executing operation TestWriteToCSV/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:26:50.348Z: JOB_MESSAGE_BASIC: Executing operation TestWriteToCSV/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:26:50.374Z: JOB_MESSAGE_BASIC: Executing operation TestWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:26:50.382Z: JOB_MESSAGE_BASIC: Finished operation TestWriteToCSV/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:26:50.393Z: JOB_MESSAGE_BASIC: Finished operation TestWriteToCSV/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:26:50.424Z: JOB_MESSAGE_BASIC: Finished operation TestWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:26:50.426Z: JOB_MESSAGE_DEBUG: Value \"TestWriteToCSV/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:26:50.450Z: JOB_MESSAGE_DEBUG: Value \"TestWriteToCSV/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:26:50.482Z: JOB_MESSAGE_BASIC: Executing operation TestWriteToCSV/Write/WriteImpl/GroupByKey/Create\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:26:50.506Z: JOB_MESSAGE_DEBUG: Value \"TestWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:26:50.876Z: JOB_MESSAGE_BASIC: Finished operation TestWriteToCSV/Write/WriteImpl/GroupByKey/Create\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:26:50.944Z: JOB_MESSAGE_DEBUG: Value \"TestWriteToCSV/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:26:51.017Z: JOB_MESSAGE_BASIC: Executing operation ReadTwitchData/Read+Split train-test dataset/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)+ExtractColumns+Preprocess+TestWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)+TestWriteToCSV/Write/WriteImpl/WriteBundles/WriteBundles+TestWriteToCSV/Write/WriteImpl/Pair+TestWriteToCSV/Write/WriteImpl/GroupByKey/Reify+TestWriteToCSV/Write/WriteImpl/GroupByKey/Write\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:27:06.536Z: JOB_MESSAGE_BASIC: Finished operation ReadTwitchData/Read+Split train-test dataset/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)+ExtractColumns+Preprocess+TestWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)+TestWriteToCSV/Write/WriteImpl/WriteBundles/WriteBundles+TestWriteToCSV/Write/WriteImpl/Pair+TestWriteToCSV/Write/WriteImpl/GroupByKey/Reify+TestWriteToCSV/Write/WriteImpl/GroupByKey/Write\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:27:06.595Z: JOB_MESSAGE_BASIC: Executing operation TestWriteToCSV/Write/WriteImpl/GroupByKey/Close\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:27:06.649Z: JOB_MESSAGE_BASIC: Finished operation TestWriteToCSV/Write/WriteImpl/GroupByKey/Close\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:27:06.705Z: JOB_MESSAGE_BASIC: Executing operation TestWriteToCSV/Write/WriteImpl/GroupByKey/Read+TestWriteToCSV/Write/WriteImpl/GroupByKey/GroupByWindow+TestWriteToCSV/Write/WriteImpl/Extract\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:27:16.820Z: JOB_MESSAGE_BASIC: Finished operation TestWriteToCSV/Write/WriteImpl/GroupByKey/Read+TestWriteToCSV/Write/WriteImpl/GroupByKey/GroupByWindow+TestWriteToCSV/Write/WriteImpl/Extract\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:27:16.879Z: JOB_MESSAGE_DEBUG: Value \"TestWriteToCSV/Write/WriteImpl/Extract.out\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:27:16.934Z: JOB_MESSAGE_BASIC: Executing operation TestWriteToCSV/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:27:16.967Z: JOB_MESSAGE_BASIC: Executing operation TestWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:27:16.991Z: JOB_MESSAGE_BASIC: Finished operation TestWriteToCSV/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:27:17.010Z: JOB_MESSAGE_BASIC: Finished operation TestWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:27:17.056Z: JOB_MESSAGE_DEBUG: Value \"TestWriteToCSV/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0).output\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:27:17.078Z: JOB_MESSAGE_DEBUG: Value \"TestWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0).output\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:27:17.112Z: JOB_MESSAGE_BASIC: Executing operation TestWriteToCSV/Write/WriteImpl/PreFinalize/PreFinalize\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:27:20.675Z: JOB_MESSAGE_BASIC: Finished operation TestWriteToCSV/Write/WriteImpl/PreFinalize/PreFinalize\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:27:20.729Z: JOB_MESSAGE_DEBUG: Value \"TestWriteToCSV/Write/WriteImpl/PreFinalize.out\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:27:20.783Z: JOB_MESSAGE_BASIC: Executing operation TestWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:27:20.822Z: JOB_MESSAGE_BASIC: Finished operation TestWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:27:20.880Z: JOB_MESSAGE_DEBUG: Value \"TestWriteToCSV/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0).output\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:27:20.933Z: JOB_MESSAGE_BASIC: Executing operation TestWriteToCSV/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:27:24.533Z: JOB_MESSAGE_BASIC: Finished operation TestWriteToCSV/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:27:24.591Z: JOB_MESSAGE_DEBUG: Executing success step success14\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:27:24.664Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:27:24.697Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:27:24.727Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:28:25.161Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-07-02T16:28:25.192Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2021-07-02_09_20_11-6511883588189039684 is in state JOB_STATE_DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFNxYtxW9fP9"
      },
      "source": [
        "### Validación Train en AI Platform (0.5 puntos)\n",
        "\n",
        "Con el comando mostrado a continuación se valida el correcto entrenamiento del modelo usando los datos de las ejecuciones anteriores en GCP con los datos obtenidos almacenados en Google Cloud Storage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0fP_5Gy9zfz"
      },
      "source": [
        "Generamos un nombre para el job de entrenamiento y donde se almacenarán los metadatos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eU0FkyLQPt6"
      },
      "source": [
        "JOB = \"troll_detection_batch_$(date +%Y%m%d_%H%M%S)\"\n",
        "JOB_DIR = GCP_WORK_DIR + \"/trainer\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z33nVs5QSEI",
        "outputId": "61ededbb-c56e-489a-b00e-1a4cb85e2849"
      },
      "source": [
        "! gcloud ai-platform jobs submit training $JOB \\\n",
        "  --module-name trainer.task \\\n",
        "  --package-path trainer \\\n",
        "  --scale-tier basic_gpu \\\n",
        "  --python-version 3.7 \\\n",
        "  --runtime-version 2.1 \\\n",
        "  --region $GCP_REGION \\\n",
        "  --job-dir $JOB_DIR \\\n",
        "  --stream-logs \\\n",
        "  -- \\\n",
        "  --work-dir $GCP_WORK_DIR \\\n",
        "  --epochs 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Job [troll_detection_batch_20210702_163306] submitted successfully.\n",
            "INFO\t2021-07-02 16:33:08 +0000\tservice\t\tValidating job requirements...\n",
            "INFO\t2021-07-02 16:33:20 +0000\tservice\t\tJob creation request has been successfully validated.\n",
            "INFO\t2021-07-02 16:33:20 +0000\tservice\t\tJob troll_detection_batch_20210702_163306 is queued.\n",
            "INFO\t2021-07-02 16:33:22 +0000\tservice\t\tWaiting for job to be provisioned.\n",
            "INFO\t2021-07-02 16:33:27 +0000\tservice\t\tWaiting for training program to start.\n",
            "INFO\t2021-07-02 16:34:57 +0000\tmaster-replica-0\t\tUsing mount point: /gcs\n",
            "NOTICE\t2021-07-02 16:34:57 +0000\tmaster-replica-0\t\tOpening GCS connection...\n",
            "INFO\t2021-07-02 16:34:57 +0000\tmaster-replica-0\t\tSet up root directory for all accessible buckets\n",
            "NOTICE\t2021-07-02 16:34:57 +0000\tmaster-replica-0\t\tMounting file system \"gcsfuse\"...\n",
            "NOTICE\t2021-07-02 16:34:57 +0000\tmaster-replica-0\t\tFile system has been successfully mounted.\n",
            "INFO\t2021-07-02 16:35:03 +0000\tmaster-replica-0\t\tRunning task with arguments: --cluster={\"chief\": [\"127.0.0.1:2222\"]} --task={\"type\": \"chief\", \"index\": 0} --job={  \"scale_tier\": \"BASIC_GPU\",  \"package_uris\": [\"gs://twitch-practiceeva/trainer/packages/cd7eb410ee874c52a52a3ff0fb4ac1e2e3c473beb369719a9af456fc2f616dd1/twitchstreaming-0.0.1.tar.gz\"],  \"python_module\": \"trainer.task\",  \"args\": [\"--work-dir\", \"gs://twitch-practiceeva\", \"--epochs\", \"1\"],  \"region\": \"europe-west1\",  \"runtime_version\": \"2.1\",  \"job_dir\": \"gs://twitch-practiceeva/trainer\",  \"run_on_raw_vm\": true,  \"python_version\": \"3.7\"}\n",
            "WARNING\t2021-07-02 16:35:09 +0000\tmaster-replica-0\t\tFrom /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "WARNING\t2021-07-02 16:35:09 +0000\tmaster-replica-0\t\tInstructions for updating:\n",
            "WARNING\t2021-07-02 16:35:09 +0000\tmaster-replica-0\t\tIf using Keras pass *_constraint arguments to layers.\n",
            "INFO\t2021-07-02 16:35:13 +0000\tmaster-replica-0\t\tRunning module trainer.task.\n",
            "INFO\t2021-07-02 16:35:13 +0000\tmaster-replica-0\t\tDownloading the package: gs://twitch-practiceeva/trainer/packages/cd7eb410ee874c52a52a3ff0fb4ac1e2e3c473beb369719a9af456fc2f616dd1/twitchstreaming-0.0.1.tar.gz\n",
            "INFO\t2021-07-02 16:35:13 +0000\tmaster-replica-0\t\tRunning command: gsutil -q cp gs://twitch-practiceeva/trainer/packages/cd7eb410ee874c52a52a3ff0fb4ac1e2e3c473beb369719a9af456fc2f616dd1/twitchstreaming-0.0.1.tar.gz twitchstreaming-0.0.1.tar.gz\n",
            "INFO\t2021-07-02 16:35:14 +0000\tmaster-replica-0\t\tInstalling the package: gs://twitch-practiceeva/trainer/packages/cd7eb410ee874c52a52a3ff0fb4ac1e2e3c473beb369719a9af456fc2f616dd1/twitchstreaming-0.0.1.tar.gz\n",
            "INFO\t2021-07-02 16:35:14 +0000\tmaster-replica-0\t\tRunning command: pip3 install --user --upgrade --force-reinstall --no-deps twitchstreaming-0.0.1.tar.gz\n",
            "INFO\t2021-07-02 16:35:15 +0000\tmaster-replica-0\t\tProcessing ./twitchstreaming-0.0.1.tar.gz\n",
            "INFO\t2021-07-02 16:35:15 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO\t2021-07-02 16:35:15 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "INFO\t2021-07-02 16:35:15 +0000\tmaster-replica-0\t\tBuilding wheels for collected packages: twitchstreaming\n",
            "INFO\t2021-07-02 16:35:15 +0000\tmaster-replica-0\t\t  Building wheel for twitchstreaming (setup.py): started\n",
            "INFO\t2021-07-02 16:35:16 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO\t2021-07-02 16:35:16 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "INFO\t2021-07-02 16:35:16 +0000\tmaster-replica-0\t\t  Building wheel for twitchstreaming (setup.py): finished with status 'done'\n",
            "INFO\t2021-07-02 16:35:16 +0000\tmaster-replica-0\t\t  Created wheel for twitchstreaming: filename=twitchstreaming-0.0.1-py3-none-any.whl size=4020 sha256=f8f13f29365a8f2b2107c31b86c3850c98952b3d924a78fcb009844c70807491\n",
            "INFO\t2021-07-02 16:35:16 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/c3/74/78/094c3d65891811370d10f2463da6ec5f32ea469d2e14797ac9\n",
            "INFO\t2021-07-02 16:35:16 +0000\tmaster-replica-0\t\tSuccessfully built twitchstreaming\n",
            "INFO\t2021-07-02 16:35:16 +0000\tmaster-replica-0\t\tInstalling collected packages: twitchstreaming\n",
            "INFO\t2021-07-02 16:35:16 +0000\tmaster-replica-0\t\tSuccessfully installed twitchstreaming-0.0.1\n",
            "ERROR\t2021-07-02 16:35:16 +0000\tmaster-replica-0\t\tWARNING: You are using pip version 20.1; however, version 21.1.3 is available.\n",
            "ERROR\t2021-07-02 16:35:16 +0000\tmaster-replica-0\t\tYou should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\n",
            "INFO\t2021-07-02 16:35:16 +0000\tmaster-replica-0\t\tRunning command: pip3 install --user twitchstreaming-0.0.1.tar.gz\n",
            "INFO\t2021-07-02 16:35:16 +0000\tmaster-replica-0\t\tProcessing ./twitchstreaming-0.0.1.tar.gz\n",
            "INFO\t2021-07-02 16:35:17 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO\t2021-07-02 16:35:17 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "INFO\t2021-07-02 16:35:17 +0000\tmaster-replica-0\t\tCollecting apache-beam[gcp]==2.24.0\n",
            "INFO\t2021-07-02 16:35:17 +0000\tmaster-replica-0\t\t  Downloading apache_beam-2.24.0-cp37-cp37m-manylinux2010_x86_64.whl (8.5 MB)\n",
            "INFO\t2021-07-02 16:35:18 +0000\tmaster-replica-0\t\tCollecting tensorflow-transform==0.24.1\n",
            "INFO\t2021-07-02 16:35:18 +0000\tmaster-replica-0\t\t  Downloading tensorflow_transform-0.24.1-py3-none-any.whl (373 kB)\n",
            "INFO\t2021-07-02 16:35:18 +0000\tmaster-replica-0\t\tCollecting tensorflow==2.3.0\n",
            "INFO\t2021-07-02 16:35:18 +0000\tmaster-replica-0\t\t  Downloading tensorflow-2.3.0-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\n",
            "INFO\t2021-07-02 16:35:33 +0000\tmaster-replica-0\t\tCollecting tfx==0.24.1\n",
            "INFO\t2021-07-02 16:35:33 +0000\tmaster-replica-0\t\t  Downloading tfx-0.24.1-py3-none-any.whl (1.8 MB)\n",
            "INFO\t2021-07-02 16:35:34 +0000\tmaster-replica-0\t\tCollecting gensim==3.6.0\n",
            "INFO\t2021-07-02 16:35:34 +0000\tmaster-replica-0\t\t  Downloading gensim-3.6.0.tar.gz (23.1 MB)\n",
            "INFO\t2021-07-02 16:35:35 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO\t2021-07-02 16:35:35 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "INFO\t2021-07-02 16:35:36 +0000\tmaster-replica-0\t\tCollecting fsspec==0.8.4\n",
            "INFO\t2021-07-02 16:35:36 +0000\tmaster-replica-0\t\t  Downloading fsspec-0.8.4-py3-none-any.whl (91 kB)\n",
            "INFO\t2021-07-02 16:35:36 +0000\tmaster-replica-0\t\tCollecting gcsfs==0.7.1\n",
            "INFO\t2021-07-02 16:35:36 +0000\tmaster-replica-0\t\t  Downloading gcsfs-0.7.1-py2.py3-none-any.whl (20 kB)\n",
            "INFO\t2021-07-02 16:35:37 +0000\tmaster-replica-0\t\tCollecting grpcio<2,>=1.29.0\n",
            "INFO\t2021-07-02 16:35:37 +0000\tmaster-replica-0\t\t  Downloading grpcio-1.38.1-cp37-cp37m-manylinux2014_x86_64.whl (4.2 MB)\n",
            "INFO\t2021-07-02 16:35:37 +0000\tmaster-replica-0\t\tCollecting pydot<2,>=1.2.0\n",
            "INFO\t2021-07-02 16:35:37 +0000\tmaster-replica-0\t\t  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
            "INFO\t2021-07-02 16:35:37 +0000\tmaster-replica-0\t\tRequirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (2.8.1)\n",
            "INFO\t2021-07-02 16:35:37 +0000\tmaster-replica-0\t\tRequirement already satisfied: future<1.0.0,>=0.18.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (0.18.2)\n",
            "INFO\t2021-07-02 16:35:37 +0000\tmaster-replica-0\t\tCollecting oauth2client<4,>=2.0.1\n",
            "INFO\t2021-07-02 16:35:37 +0000\tmaster-replica-0\t\t  Downloading oauth2client-3.0.0.tar.gz (77 kB)\n",
            "INFO\t2021-07-02 16:35:37 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO\t2021-07-02 16:35:37 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "INFO\t2021-07-02 16:35:37 +0000\tmaster-replica-0\t\tRequirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (2021.1)\n",
            "INFO\t2021-07-02 16:35:37 +0000\tmaster-replica-0\t\tRequirement already satisfied: httplib2<0.18.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (0.15.0)\n",
            "INFO\t2021-07-02 16:35:37 +0000\tmaster-replica-0\t\tCollecting dill<0.3.2,>=0.3.1.1\n",
            "INFO\t2021-07-02 16:35:37 +0000\tmaster-replica-0\t\t  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "INFO\t2021-07-02 16:35:38 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO\t2021-07-02 16:35:38 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "INFO\t2021-07-02 16:35:38 +0000\tmaster-replica-0\t\tCollecting pyarrow<0.18.0,>=0.15.1; python_version >= \"3.0\" or platform_system != \"Windows\"\n",
            "INFO\t2021-07-02 16:35:38 +0000\tmaster-replica-0\t\t  Downloading pyarrow-0.17.1-cp37-cp37m-manylinux2014_x86_64.whl (63.8 MB)\n",
            "INFO\t2021-07-02 16:35:41 +0000\tmaster-replica-0\t\tRequirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (1.7)\n",
            "INFO\t2021-07-02 16:35:41 +0000\tmaster-replica-0\t\tCollecting requests<3.0.0,>=2.24.0\n",
            "INFO\t2021-07-02 16:35:41 +0000\tmaster-replica-0\t\t  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
            "INFO\t2021-07-02 16:35:41 +0000\tmaster-replica-0\t\tRequirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (3.17.3)\n",
            "INFO\t2021-07-02 16:35:42 +0000\tmaster-replica-0\t\tCollecting hdfs<3.0.0,>=2.1.0\n",
            "INFO\t2021-07-02 16:35:42 +0000\tmaster-replica-0\t\t  Downloading hdfs-2.6.0-py3-none-any.whl (33 kB)\n",
            "INFO\t2021-07-02 16:35:42 +0000\tmaster-replica-0\t\tCollecting typing-extensions<3.8.0,>=3.7.0\n",
            "INFO\t2021-07-02 16:35:42 +0000\tmaster-replica-0\t\t  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "INFO\t2021-07-02 16:35:42 +0000\tmaster-replica-0\t\tCollecting fastavro<0.24,>=0.21.4\n",
            "INFO\t2021-07-02 16:35:42 +0000\tmaster-replica-0\t\t  Downloading fastavro-0.23.6-cp37-cp37m-manylinux2010_x86_64.whl (1.4 MB)\n",
            "INFO\t2021-07-02 16:35:42 +0000\tmaster-replica-0\t\tCollecting avro-python3!=1.9.2,<1.10.0,>=1.8.1; python_version >= \"3.0\"\n",
            "INFO\t2021-07-02 16:35:42 +0000\tmaster-replica-0\t\t  Downloading avro-python3-1.9.2.1.tar.gz (37 kB)\n",
            "INFO\t2021-07-02 16:35:44 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO\t2021-07-02 16:35:44 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "INFO\t2021-07-02 16:35:44 +0000\tmaster-replica-0\t\tCollecting mock<3.0.0,>=1.0.1\n",
            "INFO\t2021-07-02 16:35:44 +0000\tmaster-replica-0\t\t  Downloading mock-2.0.0-py2.py3-none-any.whl (56 kB)\n",
            "INFO\t2021-07-02 16:35:44 +0000\tmaster-replica-0\t\tRequirement already satisfied: numpy<2,>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (1.18.0)\n",
            "INFO\t2021-07-02 16:35:45 +0000\tmaster-replica-0\t\tCollecting pymongo<4.0.0,>=3.8.0\n",
            "INFO\t2021-07-02 16:35:45 +0000\tmaster-replica-0\t\t  Downloading pymongo-3.11.4-cp37-cp37m-manylinux2014_x86_64.whl (512 kB)\n",
            "INFO\t2021-07-02 16:35:45 +0000\tmaster-replica-0\t\tRequirement already satisfied: google-cloud-pubsub<2,>=0.39.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (1.1.0)\n",
            "INFO\t2021-07-02 16:35:45 +0000\tmaster-replica-0\t\tCollecting google-cloud-vision<2,>=0.38.0; extra == \"gcp\"\n",
            "INFO\t2021-07-02 16:35:45 +0000\tmaster-replica-0\t\t  Downloading google_cloud_vision-1.0.0-py2.py3-none-any.whl (435 kB)\n",
            "INFO\t2021-07-02 16:35:45 +0000\tmaster-replica-0\t\tRequirement already satisfied: google-cloud-datastore<2,>=1.7.1; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (1.10.0)\n",
            "INFO\t2021-07-02 16:35:45 +0000\tmaster-replica-0\t\tCollecting google-cloud-videointelligence<2,>=1.8.0; extra == \"gcp\"\n",
            "INFO\t2021-07-02 16:35:45 +0000\tmaster-replica-0\t\t  Downloading google_cloud_videointelligence-1.16.1-py2.py3-none-any.whl (183 kB)\n",
            "INFO\t2021-07-02 16:35:45 +0000\tmaster-replica-0\t\tRequirement already satisfied: google-auth<2,>=1.18.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (1.32.0)\n",
            "INFO\t2021-07-02 16:35:45 +0000\tmaster-replica-0\t\tRequirement already satisfied: google-cloud-bigquery<2,>=1.6.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (1.23.1)\n",
            "INFO\t2021-07-02 16:35:45 +0000\tmaster-replica-0\t\tCollecting google-cloud-dlp<2,>=0.12.0; extra == \"gcp\"\n",
            "INFO\t2021-07-02 16:35:45 +0000\tmaster-replica-0\t\t  Downloading google_cloud_dlp-1.0.0-py2.py3-none-any.whl (169 kB)\n",
            "INFO\t2021-07-02 16:35:45 +0000\tmaster-replica-0\t\tRequirement already satisfied: google-cloud-bigtable<2,>=0.31.1; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (1.2.0)\n",
            "INFO\t2021-07-02 16:35:45 +0000\tmaster-replica-0\t\tCollecting grpcio-gcp<1,>=0.2.2; extra == \"gcp\"\n",
            "INFO\t2021-07-02 16:35:45 +0000\tmaster-replica-0\t\t  Downloading grpcio_gcp-0.2.2-py2.py3-none-any.whl (9.4 kB)\n",
            "INFO\t2021-07-02 16:35:45 +0000\tmaster-replica-0\t\tCollecting google-cloud-language<2,>=1.3.0; extra == \"gcp\"\n",
            "INFO\t2021-07-02 16:35:45 +0000\tmaster-replica-0\t\t  Downloading google_cloud_language-1.3.0-py2.py3-none-any.whl (83 kB)\n",
            "INFO\t2021-07-02 16:35:45 +0000\tmaster-replica-0\t\tCollecting google-cloud-spanner<2,>=1.13.0; extra == \"gcp\"\n",
            "INFO\t2021-07-02 16:35:45 +0000\tmaster-replica-0\t\t  Downloading google_cloud_spanner-1.19.1-py2.py3-none-any.whl (255 kB)\n",
            "INFO\t2021-07-02 16:35:46 +0000\tmaster-replica-0\t\tCollecting google-apitools<0.5.32,>=0.5.31; extra == \"gcp\"\n",
            "INFO\t2021-07-02 16:35:46 +0000\tmaster-replica-0\t\t  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n",
            "INFO\t2021-07-02 16:35:46 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO\t2021-07-02 16:35:46 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "INFO\t2021-07-02 16:35:46 +0000\tmaster-replica-0\t\tRequirement already satisfied: google-cloud-core<2,>=0.28.1; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (1.7.0)\n",
            "INFO\t2021-07-02 16:35:46 +0000\tmaster-replica-0\t\tCollecting cachetools<4,>=3.1.0; extra == \"gcp\"\n",
            "INFO\t2021-07-02 16:35:46 +0000\tmaster-replica-0\t\t  Downloading cachetools-3.1.1-py2.py3-none-any.whl (11 kB)\n",
            "INFO\t2021-07-02 16:35:46 +0000\tmaster-replica-0\t\tCollecting absl-py<0.11,>=0.9\n",
            "INFO\t2021-07-02 16:35:46 +0000\tmaster-replica-0\t\t  Downloading absl_py-0.10.0-py3-none-any.whl (127 kB)\n",
            "INFO\t2021-07-02 16:35:46 +0000\tmaster-replica-0\t\tCollecting tfx-bsl<0.25,>=0.24.1\n",
            "INFO\t2021-07-02 16:35:46 +0000\tmaster-replica-0\t\t  Downloading tfx_bsl-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (1.8 MB)\n",
            "INFO\t2021-07-02 16:35:46 +0000\tmaster-replica-0\t\tRequirement already satisfied: six<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow-transform==0.24.1->twitchstreaming==0.0.1) (1.13.0)\n",
            "INFO\t2021-07-02 16:35:46 +0000\tmaster-replica-0\t\tCollecting tensorflow-metadata<0.25,>=0.24\n",
            "INFO\t2021-07-02 16:35:46 +0000\tmaster-replica-0\t\t  Downloading tensorflow_metadata-0.24.0-py3-none-any.whl (44 kB)\n",
            "INFO\t2021-07-02 16:35:46 +0000\tmaster-replica-0\t\tRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0->twitchstreaming==0.0.1) (2.10.0)\n",
            "INFO\t2021-07-02 16:35:46 +0000\tmaster-replica-0\t\tCollecting tensorboard<3,>=2.3.0\n",
            "INFO\t2021-07-02 16:35:46 +0000\tmaster-replica-0\t\t  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
            "INFO\t2021-07-02 16:35:47 +0000\tmaster-replica-0\t\tRequirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0->twitchstreaming==0.0.1) (0.2.0)\n",
            "INFO\t2021-07-02 16:35:47 +0000\tmaster-replica-0\t\tCollecting gast==0.3.3\n",
            "INFO\t2021-07-02 16:35:47 +0000\tmaster-replica-0\t\t  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "INFO\t2021-07-02 16:35:47 +0000\tmaster-replica-0\t\tRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0->twitchstreaming==0.0.1) (0.33.6)\n",
            "INFO\t2021-07-02 16:35:47 +0000\tmaster-replica-0\t\tRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0->twitchstreaming==0.0.1) (3.3.0)\n",
            "INFO\t2021-07-02 16:35:47 +0000\tmaster-replica-0\t\tRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0->twitchstreaming==0.0.1) (1.11.2)\n",
            "INFO\t2021-07-02 16:35:47 +0000\tmaster-replica-0\t\tRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0->twitchstreaming==0.0.1) (1.1.0)\n",
            "INFO\t2021-07-02 16:35:47 +0000\tmaster-replica-0\t\tCollecting keras-preprocessing<1.2,>=1.1.1\n",
            "INFO\t2021-07-02 16:35:47 +0000\tmaster-replica-0\t\t  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "INFO\t2021-07-02 16:35:47 +0000\tmaster-replica-0\t\tCollecting tensorflow-estimator<2.4.0,>=2.3.0\n",
            "INFO\t2021-07-02 16:35:47 +0000\tmaster-replica-0\t\t  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
            "INFO\t2021-07-02 16:35:47 +0000\tmaster-replica-0\t\tRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0->twitchstreaming==0.0.1) (1.4.1)\n",
            "INFO\t2021-07-02 16:35:47 +0000\tmaster-replica-0\t\tCollecting astunparse==1.6.3\n",
            "INFO\t2021-07-02 16:35:47 +0000\tmaster-replica-0\t\t  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "INFO\t2021-07-02 16:35:47 +0000\tmaster-replica-0\t\tCollecting docker<5,>=4.1\n",
            "INFO\t2021-07-02 16:35:47 +0000\tmaster-replica-0\t\t  Downloading docker-4.4.4-py2.py3-none-any.whl (147 kB)\n",
            "INFO\t2021-07-02 16:35:47 +0000\tmaster-replica-0\t\tCollecting jinja2<3,>=2.7.3\n",
            "INFO\t2021-07-02 16:35:47 +0000\tmaster-replica-0\t\t  Downloading Jinja2-2.11.3-py2.py3-none-any.whl (125 kB)\n",
            "INFO\t2021-07-02 16:35:47 +0000\tmaster-replica-0\t\tCollecting attrs<20,>=19.3.0\n",
            "INFO\t2021-07-02 16:35:47 +0000\tmaster-replica-0\t\t  Downloading attrs-19.3.0-py2.py3-none-any.whl (39 kB)\n",
            "INFO\t2021-07-02 16:35:47 +0000\tmaster-replica-0\t\tCollecting keras-tuner<2,>=1\n",
            "INFO\t2021-07-02 16:35:47 +0000\tmaster-replica-0\t\t  Downloading keras_tuner-1.0.3-py3-none-any.whl (96 kB)\n",
            "INFO\t2021-07-02 16:35:47 +0000\tmaster-replica-0\t\tCollecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15\n",
            "INFO\t2021-07-02 16:35:47 +0000\tmaster-replica-0\t\t  Downloading tensorflow_serving_api-2.5.1-py2.py3-none-any.whl (38 kB)\n",
            "INFO\t2021-07-02 16:35:48 +0000\tmaster-replica-0\t\tCollecting kubernetes<12,>=10.0.1\n",
            "INFO\t2021-07-02 16:35:48 +0000\tmaster-replica-0\t\t  Downloading kubernetes-11.0.0-py3-none-any.whl (1.5 MB)\n",
            "INFO\t2021-07-02 16:35:48 +0000\tmaster-replica-0\t\tCollecting ml-metadata<0.25,>=0.24\n",
            "INFO\t2021-07-02 16:35:48 +0000\tmaster-replica-0\t\t  Downloading ml_metadata-0.24.0-cp37-cp37m-manylinux2010_x86_64.whl (2.8 MB)\n",
            "INFO\t2021-07-02 16:35:48 +0000\tmaster-replica-0\t\tRequirement already satisfied: pyyaml<6,>=3.12 in /usr/local/lib/python3.7/dist-packages (from tfx==0.24.1->twitchstreaming==0.0.1) (5.2)\n",
            "INFO\t2021-07-02 16:35:48 +0000\tmaster-replica-0\t\tRequirement already satisfied: google-api-python-client<2,>=1.7.8 in /usr/local/lib/python3.7/dist-packages (from tfx==0.24.1->twitchstreaming==0.0.1) (1.7.11)\n",
            "INFO\t2021-07-02 16:35:48 +0000\tmaster-replica-0\t\tCollecting click<8,>=7\n",
            "INFO\t2021-07-02 16:35:48 +0000\tmaster-replica-0\t\t  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
            "INFO\t2021-07-02 16:35:48 +0000\tmaster-replica-0\t\tCollecting tensorflow-data-validation<0.25,>=0.24.1\n",
            "INFO\t2021-07-02 16:35:48 +0000\tmaster-replica-0\t\t  Downloading tensorflow_data_validation-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (1.3 MB)\n",
            "INFO\t2021-07-02 16:35:49 +0000\tmaster-replica-0\t\tCollecting tensorflow-model-analysis<0.25,>=0.24.3\n",
            "INFO\t2021-07-02 16:35:49 +0000\tmaster-replica-0\t\t  Downloading tensorflow_model_analysis-0.24.3-py3-none-any.whl (1.6 MB)\n",
            "INFO\t2021-07-02 16:35:50 +0000\tmaster-replica-0\t\tCollecting smart_open>=1.2.1\n",
            "INFO\t2021-07-02 16:35:50 +0000\tmaster-replica-0\t\t  Downloading smart_open-5.1.0-py3-none-any.whl (57 kB)\n",
            "INFO\t2021-07-02 16:35:50 +0000\tmaster-replica-0\t\tRequirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->twitchstreaming==0.0.1) (0.4.4)\n",
            "INFO\t2021-07-02 16:35:50 +0000\tmaster-replica-0\t\tCollecting aiohttp\n",
            "INFO\t2021-07-02 16:35:50 +0000\tmaster-replica-0\t\t  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "INFO\t2021-07-02 16:35:51 +0000\tmaster-replica-0\t\tCollecting decorator\n",
            "INFO\t2021-07-02 16:35:51 +0000\tmaster-replica-0\t\t  Downloading decorator-5.0.9-py3-none-any.whl (8.9 kB)\n",
            "INFO\t2021-07-02 16:35:51 +0000\tmaster-replica-0\t\tRequirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pydot<2,>=1.2.0->apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (2.4.7)\n",
            "INFO\t2021-07-02 16:35:51 +0000\tmaster-replica-0\t\tRequirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (0.4.8)\n",
            "INFO\t2021-07-02 16:35:51 +0000\tmaster-replica-0\t\tRequirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (0.2.8)\n",
            "INFO\t2021-07-02 16:35:51 +0000\tmaster-replica-0\t\tRequirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (4.7.2)\n",
            "INFO\t2021-07-02 16:35:51 +0000\tmaster-replica-0\t\tRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (2.8)\n",
            "INFO\t2021-07-02 16:35:51 +0000\tmaster-replica-0\t\tRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (1.25.11)\n",
            "INFO\t2021-07-02 16:35:51 +0000\tmaster-replica-0\t\tRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (2021.5.30)\n",
            "INFO\t2021-07-02 16:35:51 +0000\tmaster-replica-0\t\tRequirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (3.0.4)\n",
            "INFO\t2021-07-02 16:35:51 +0000\tmaster-replica-0\t\tCollecting docopt\n",
            "INFO\t2021-07-02 16:35:51 +0000\tmaster-replica-0\t\t  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "INFO\t2021-07-02 16:35:51 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO\t2021-07-02 16:35:51 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "INFO\t2021-07-02 16:35:51 +0000\tmaster-replica-0\t\tCollecting pbr>=0.11\n",
            "INFO\t2021-07-02 16:35:51 +0000\tmaster-replica-0\t\t  Downloading pbr-5.6.0-py2.py3-none-any.whl (111 kB)\n",
            "INFO\t2021-07-02 16:35:51 +0000\tmaster-replica-0\t\tRequirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-pubsub<2,>=0.39.0; extra == \"gcp\"->apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (0.12.3)\n",
            "INFO\t2021-07-02 16:35:51 +0000\tmaster-replica-0\t\tRequirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-pubsub<2,>=0.39.0; extra == \"gcp\"->apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (1.30.0)\n",
            "INFO\t2021-07-02 16:35:51 +0000\tmaster-replica-0\t\tRequirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.18.0; extra == \"gcp\"->apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (57.0.0)\n",
            "INFO\t2021-07-02 16:35:51 +0000\tmaster-replica-0\t\tRequirement already satisfied: google-resumable-media!=0.4.0,<0.6.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<2,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (0.5.1)\n",
            "INFO\t2021-07-02 16:35:51 +0000\tmaster-replica-0\t\tCollecting fasteners>=0.14\n",
            "INFO\t2021-07-02 16:35:51 +0000\tmaster-replica-0\t\t  Downloading fasteners-0.16.3-py2.py3-none-any.whl (28 kB)\n",
            "INFO\t2021-07-02 16:35:52 +0000\tmaster-replica-0\t\tCollecting pandas<2,>=1.0\n",
            "INFO\t2021-07-02 16:35:52 +0000\tmaster-replica-0\t\t  Downloading pandas-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (10.8 MB)\n",
            "INFO\t2021-07-02 16:35:52 +0000\tmaster-replica-0\t\tRequirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata<0.25,>=0.24->tensorflow-transform==0.24.1->twitchstreaming==0.0.1) (1.53.0)\n",
            "INFO\t2021-07-02 16:35:53 +0000\tmaster-replica-0\t\tCollecting tensorboard-plugin-wit>=1.6.0\n",
            "INFO\t2021-07-02 16:35:53 +0000\tmaster-replica-0\t\t  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
            "INFO\t2021-07-02 16:35:53 +0000\tmaster-replica-0\t\tCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "INFO\t2021-07-02 16:35:53 +0000\tmaster-replica-0\t\t  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "INFO\t2021-07-02 16:35:53 +0000\tmaster-replica-0\t\tRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0->twitchstreaming==0.0.1) (3.3.4)\n",
            "INFO\t2021-07-02 16:35:53 +0000\tmaster-replica-0\t\tRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0->twitchstreaming==0.0.1) (2.0.1)\n",
            "INFO\t2021-07-02 16:35:53 +0000\tmaster-replica-0\t\tCollecting websocket-client>=0.32.0\n",
            "INFO\t2021-07-02 16:35:53 +0000\tmaster-replica-0\t\t  Downloading websocket_client-1.1.0-py2.py3-none-any.whl (68 kB)\n",
            "INFO\t2021-07-02 16:35:53 +0000\tmaster-replica-0\t\tCollecting MarkupSafe>=0.23\n",
            "INFO\t2021-07-02 16:35:53 +0000\tmaster-replica-0\t\t  Downloading MarkupSafe-2.0.1-cp37-cp37m-manylinux2010_x86_64.whl (31 kB)\n",
            "INFO\t2021-07-02 16:35:53 +0000\tmaster-replica-0\t\tCollecting kt-legacy\n",
            "INFO\t2021-07-02 16:35:53 +0000\tmaster-replica-0\t\t  Downloading kt-legacy-1.0.3.tar.gz (5.8 kB)\n",
            "INFO\t2021-07-02 16:35:53 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO\t2021-07-02 16:35:54 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "INFO\t2021-07-02 16:35:54 +0000\tmaster-replica-0\t\tRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner<2,>=1->tfx==0.24.1->twitchstreaming==0.0.1) (20.9)\n",
            "INFO\t2021-07-02 16:35:54 +0000\tmaster-replica-0\t\tCollecting ipython\n",
            "INFO\t2021-07-02 16:35:54 +0000\tmaster-replica-0\t\t  Downloading ipython-7.25.0-py3-none-any.whl (786 kB)\n",
            "INFO\t2021-07-02 16:35:54 +0000\tmaster-replica-0\t\tRequirement already satisfied: requests-oauthlib in /usr/local/lib/python3.7/dist-packages (from kubernetes<12,>=10.0.1->tfx==0.24.1->twitchstreaming==0.0.1) (1.3.0)\n",
            "INFO\t2021-07-02 16:35:54 +0000\tmaster-replica-0\t\tRequirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->tfx==0.24.1->twitchstreaming==0.0.1) (0.1.0)\n",
            "INFO\t2021-07-02 16:35:54 +0000\tmaster-replica-0\t\tRequirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->tfx==0.24.1->twitchstreaming==0.0.1) (3.0.1)\n",
            "INFO\t2021-07-02 16:35:54 +0000\tmaster-replica-0\t\tRequirement already satisfied: joblib<0.15,>=0.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow-data-validation<0.25,>=0.24.1->tfx==0.24.1->twitchstreaming==0.0.1) (0.14.1)\n",
            "INFO\t2021-07-02 16:35:54 +0000\tmaster-replica-0\t\tCollecting ipywidgets<8,>=7\n",
            "INFO\t2021-07-02 16:35:54 +0000\tmaster-replica-0\t\t  Downloading ipywidgets-7.6.3-py2.py3-none-any.whl (121 kB)\n",
            "INFO\t2021-07-02 16:35:54 +0000\tmaster-replica-0\t\tCollecting jupyter<2,>=1\n",
            "INFO\t2021-07-02 16:35:54 +0000\tmaster-replica-0\t\t  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "INFO\t2021-07-02 16:35:54 +0000\tmaster-replica-0\t\tCollecting yarl<2.0,>=1.0\n",
            "INFO\t2021-07-02 16:35:54 +0000\tmaster-replica-0\t\t  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "INFO\t2021-07-02 16:35:55 +0000\tmaster-replica-0\t\tCollecting multidict<7.0,>=4.5\n",
            "INFO\t2021-07-02 16:35:55 +0000\tmaster-replica-0\t\t  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "INFO\t2021-07-02 16:35:55 +0000\tmaster-replica-0\t\tCollecting async-timeout<4.0,>=3.0\n",
            "INFO\t2021-07-02 16:35:55 +0000\tmaster-replica-0\t\t  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "INFO\t2021-07-02 16:35:55 +0000\tmaster-replica-0\t\tRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0->twitchstreaming==0.0.1) (4.5.0)\n",
            "INFO\t2021-07-02 16:35:55 +0000\tmaster-replica-0\t\tCollecting backcall\n",
            "INFO\t2021-07-02 16:35:55 +0000\tmaster-replica-0\t\t  Downloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
            "INFO\t2021-07-02 16:35:55 +0000\tmaster-replica-0\t\tCollecting pickleshare\n",
            "INFO\t2021-07-02 16:35:55 +0000\tmaster-replica-0\t\t  Downloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
            "INFO\t2021-07-02 16:35:55 +0000\tmaster-replica-0\t\tCollecting pygments\n",
            "INFO\t2021-07-02 16:35:55 +0000\tmaster-replica-0\t\t  Downloading Pygments-2.9.0-py3-none-any.whl (1.0 MB)\n",
            "INFO\t2021-07-02 16:35:55 +0000\tmaster-replica-0\t\tCollecting matplotlib-inline\n",
            "INFO\t2021-07-02 16:35:55 +0000\tmaster-replica-0\t\t  Downloading matplotlib_inline-0.1.2-py3-none-any.whl (8.2 kB)\n",
            "INFO\t2021-07-02 16:35:56 +0000\tmaster-replica-0\t\tCollecting pexpect>4.3; sys_platform != \"win32\"\n",
            "INFO\t2021-07-02 16:35:56 +0000\tmaster-replica-0\t\t  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
            "INFO\t2021-07-02 16:35:56 +0000\tmaster-replica-0\t\tCollecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "INFO\t2021-07-02 16:35:56 +0000\tmaster-replica-0\t\t  Downloading prompt_toolkit-3.0.19-py3-none-any.whl (368 kB)\n",
            "INFO\t2021-07-02 16:35:56 +0000\tmaster-replica-0\t\tCollecting traitlets>=4.2\n",
            "INFO\t2021-07-02 16:35:56 +0000\tmaster-replica-0\t\t  Downloading traitlets-5.0.5-py3-none-any.whl (100 kB)\n",
            "INFO\t2021-07-02 16:35:56 +0000\tmaster-replica-0\t\tCollecting jedi>=0.16\n",
            "INFO\t2021-07-02 16:35:56 +0000\tmaster-replica-0\t\t  Downloading jedi-0.18.0-py2.py3-none-any.whl (1.4 MB)\n",
            "INFO\t2021-07-02 16:35:56 +0000\tmaster-replica-0\t\tRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib->kubernetes<12,>=10.0.1->tfx==0.24.1->twitchstreaming==0.0.1) (3.1.1)\n",
            "INFO\t2021-07-02 16:35:56 +0000\tmaster-replica-0\t\tCollecting ipykernel>=4.5.1\n",
            "INFO\t2021-07-02 16:35:56 +0000\tmaster-replica-0\t\t  Downloading ipykernel-6.0.1-py3-none-any.whl (122 kB)\n",
            "INFO\t2021-07-02 16:35:57 +0000\tmaster-replica-0\t\tCollecting jupyterlab-widgets>=1.0.0; python_version >= \"3.6\"\n",
            "INFO\t2021-07-02 16:35:57 +0000\tmaster-replica-0\t\t  Downloading jupyterlab_widgets-1.0.0-py3-none-any.whl (243 kB)\n",
            "INFO\t2021-07-02 16:35:57 +0000\tmaster-replica-0\t\tCollecting nbformat>=4.2.0\n",
            "INFO\t2021-07-02 16:35:57 +0000\tmaster-replica-0\t\t  Downloading nbformat-5.1.3-py3-none-any.whl (178 kB)\n",
            "INFO\t2021-07-02 16:35:57 +0000\tmaster-replica-0\t\tCollecting widgetsnbextension~=3.5.0\n",
            "INFO\t2021-07-02 16:35:57 +0000\tmaster-replica-0\t\t  Downloading widgetsnbextension-3.5.1-py2.py3-none-any.whl (2.2 MB)\n",
            "INFO\t2021-07-02 16:35:57 +0000\tmaster-replica-0\t\tCollecting jupyter-console\n",
            "INFO\t2021-07-02 16:35:57 +0000\tmaster-replica-0\t\t  Downloading jupyter_console-6.4.0-py3-none-any.whl (22 kB)\n",
            "INFO\t2021-07-02 16:35:57 +0000\tmaster-replica-0\t\tCollecting notebook\n",
            "INFO\t2021-07-02 16:35:57 +0000\tmaster-replica-0\t\t  Downloading notebook-6.4.0-py3-none-any.whl (9.5 MB)\n",
            "INFO\t2021-07-02 16:35:58 +0000\tmaster-replica-0\t\tCollecting qtconsole\n",
            "INFO\t2021-07-02 16:35:58 +0000\tmaster-replica-0\t\t  Downloading qtconsole-5.1.1-py3-none-any.whl (119 kB)\n",
            "INFO\t2021-07-02 16:35:58 +0000\tmaster-replica-0\t\tCollecting nbconvert\n",
            "INFO\t2021-07-02 16:35:58 +0000\tmaster-replica-0\t\t  Downloading nbconvert-6.1.0-py3-none-any.whl (551 kB)\n",
            "INFO\t2021-07-02 16:35:58 +0000\tmaster-replica-0\t\tRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0->twitchstreaming==0.0.1) (3.4.1)\n",
            "INFO\t2021-07-02 16:35:58 +0000\tmaster-replica-0\t\tCollecting ptyprocess>=0.5\n",
            "INFO\t2021-07-02 16:35:58 +0000\tmaster-replica-0\t\t  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
            "INFO\t2021-07-02 16:35:58 +0000\tmaster-replica-0\t\tCollecting wcwidth\n",
            "INFO\t2021-07-02 16:35:58 +0000\tmaster-replica-0\t\t  Downloading wcwidth-0.2.5-py2.py3-none-any.whl (30 kB)\n",
            "INFO\t2021-07-02 16:35:59 +0000\tmaster-replica-0\t\tCollecting ipython-genutils\n",
            "INFO\t2021-07-02 16:35:59 +0000\tmaster-replica-0\t\t  Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
            "INFO\t2021-07-02 16:35:59 +0000\tmaster-replica-0\t\tCollecting parso<0.9.0,>=0.8.0\n",
            "INFO\t2021-07-02 16:35:59 +0000\tmaster-replica-0\t\t  Downloading parso-0.8.2-py2.py3-none-any.whl (94 kB)\n",
            "INFO\t2021-07-02 16:35:59 +0000\tmaster-replica-0\t\tRequirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.25,>=0.24.3->tfx==0.24.1->twitchstreaming==0.0.1) (5.1.1)\n",
            "INFO\t2021-07-02 16:35:59 +0000\tmaster-replica-0\t\tCollecting jupyter-client\n",
            "INFO\t2021-07-02 16:35:59 +0000\tmaster-replica-0\t\t  Downloading jupyter_client-6.1.12-py3-none-any.whl (112 kB)\n",
            "INFO\t2021-07-02 16:35:59 +0000\tmaster-replica-0\t\tCollecting debugpy>=1.0.0\n",
            "INFO\t2021-07-02 16:35:59 +0000\tmaster-replica-0\t\t  Downloading debugpy-1.3.0-cp37-cp37m-manylinux2014_x86_64.whl (4.5 MB)\n",
            "INFO\t2021-07-02 16:35:59 +0000\tmaster-replica-0\t\tCollecting jupyter-core\n",
            "INFO\t2021-07-02 16:35:59 +0000\tmaster-replica-0\t\t  Downloading jupyter_core-4.7.1-py3-none-any.whl (82 kB)\n",
            "INFO\t2021-07-02 16:36:00 +0000\tmaster-replica-0\t\tCollecting jsonschema!=2.5.0,>=2.4\n",
            "INFO\t2021-07-02 16:36:00 +0000\tmaster-replica-0\t\t  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
            "INFO\t2021-07-02 16:36:00 +0000\tmaster-replica-0\t\tCollecting prometheus-client\n",
            "INFO\t2021-07-02 16:36:00 +0000\tmaster-replica-0\t\t  Downloading prometheus_client-0.11.0-py2.py3-none-any.whl (56 kB)\n",
            "INFO\t2021-07-02 16:36:00 +0000\tmaster-replica-0\t\tCollecting argon2-cffi\n",
            "INFO\t2021-07-02 16:36:00 +0000\tmaster-replica-0\t\t  Downloading argon2_cffi-20.1.0-cp35-abi3-manylinux1_x86_64.whl (97 kB)\n",
            "INFO\t2021-07-02 16:36:00 +0000\tmaster-replica-0\t\tCollecting pyzmq>=17\n",
            "INFO\t2021-07-02 16:36:00 +0000\tmaster-replica-0\t\t  Downloading pyzmq-22.1.0-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
            "INFO\t2021-07-02 16:36:01 +0000\tmaster-replica-0\t\tCollecting Send2Trash>=1.5.0\n",
            "INFO\t2021-07-02 16:36:01 +0000\tmaster-replica-0\t\t  Downloading Send2Trash-1.7.1-py3-none-any.whl (17 kB)\n",
            "INFO\t2021-07-02 16:36:01 +0000\tmaster-replica-0\t\tCollecting terminado>=0.8.3\n",
            "INFO\t2021-07-02 16:36:01 +0000\tmaster-replica-0\t\t  Downloading terminado-0.10.1-py3-none-any.whl (14 kB)\n",
            "INFO\t2021-07-02 16:36:02 +0000\tmaster-replica-0\t\tCollecting qtpy\n",
            "INFO\t2021-07-02 16:36:02 +0000\tmaster-replica-0\t\t  Downloading QtPy-1.9.0-py2.py3-none-any.whl (54 kB)\n",
            "INFO\t2021-07-02 16:36:02 +0000\tmaster-replica-0\t\tCollecting nbclient<0.6.0,>=0.5.0\n",
            "INFO\t2021-07-02 16:36:02 +0000\tmaster-replica-0\t\t  Downloading nbclient-0.5.3-py3-none-any.whl (82 kB)\n",
            "INFO\t2021-07-02 16:36:03 +0000\tmaster-replica-0\t\tCollecting defusedxml\n",
            "INFO\t2021-07-02 16:36:03 +0000\tmaster-replica-0\t\t  Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
            "INFO\t2021-07-02 16:36:03 +0000\tmaster-replica-0\t\tCollecting mistune<2,>=0.8.1\n",
            "INFO\t2021-07-02 16:36:03 +0000\tmaster-replica-0\t\t  Downloading mistune-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "INFO\t2021-07-02 16:36:03 +0000\tmaster-replica-0\t\tCollecting bleach\n",
            "INFO\t2021-07-02 16:36:03 +0000\tmaster-replica-0\t\t  Downloading bleach-3.3.0-py2.py3-none-any.whl (283 kB)\n",
            "INFO\t2021-07-02 16:36:03 +0000\tmaster-replica-0\t\tCollecting pandocfilters>=1.4.1\n",
            "INFO\t2021-07-02 16:36:03 +0000\tmaster-replica-0\t\t  Downloading pandocfilters-1.4.3.tar.gz (16 kB)\n",
            "INFO\t2021-07-02 16:36:04 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO\t2021-07-02 16:36:04 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "INFO\t2021-07-02 16:36:04 +0000\tmaster-replica-0\t\tCollecting entrypoints>=0.2.2\n",
            "INFO\t2021-07-02 16:36:04 +0000\tmaster-replica-0\t\t  Downloading entrypoints-0.3-py2.py3-none-any.whl (11 kB)\n",
            "INFO\t2021-07-02 16:36:04 +0000\tmaster-replica-0\t\tCollecting jupyterlab-pygments\n",
            "INFO\t2021-07-02 16:36:04 +0000\tmaster-replica-0\t\t  Downloading jupyterlab_pygments-0.1.2-py2.py3-none-any.whl (4.6 kB)\n",
            "INFO\t2021-07-02 16:36:04 +0000\tmaster-replica-0\t\tCollecting testpath\n",
            "INFO\t2021-07-02 16:36:04 +0000\tmaster-replica-0\t\t  Downloading testpath-0.5.0-py3-none-any.whl (84 kB)\n",
            "INFO\t2021-07-02 16:36:04 +0000\tmaster-replica-0\t\tCollecting pyrsistent>=0.14.0\n",
            "INFO\t2021-07-02 16:36:04 +0000\tmaster-replica-0\t\t  Downloading pyrsistent-0.18.0-cp37-cp37m-manylinux1_x86_64.whl (119 kB)\n",
            "INFO\t2021-07-02 16:36:04 +0000\tmaster-replica-0\t\tCollecting cffi>=1.0.0\n",
            "INFO\t2021-07-02 16:36:04 +0000\tmaster-replica-0\t\t  Downloading cffi-1.14.5-cp37-cp37m-manylinux1_x86_64.whl (402 kB)\n",
            "INFO\t2021-07-02 16:36:04 +0000\tmaster-replica-0\t\tCollecting nest-asyncio\n",
            "INFO\t2021-07-02 16:36:04 +0000\tmaster-replica-0\t\t  Downloading nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB)\n",
            "INFO\t2021-07-02 16:36:05 +0000\tmaster-replica-0\t\tCollecting async-generator\n",
            "INFO\t2021-07-02 16:36:05 +0000\tmaster-replica-0\t\t  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "INFO\t2021-07-02 16:36:05 +0000\tmaster-replica-0\t\tCollecting webencodings\n",
            "INFO\t2021-07-02 16:36:05 +0000\tmaster-replica-0\t\t  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
            "INFO\t2021-07-02 16:36:05 +0000\tmaster-replica-0\t\tCollecting pycparser\n",
            "INFO\t2021-07-02 16:36:05 +0000\tmaster-replica-0\t\t  Downloading pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
            "INFO\t2021-07-02 16:36:05 +0000\tmaster-replica-0\t\tBuilding wheels for collected packages: twitchstreaming, gensim, oauth2client, dill, avro-python3, google-apitools, docopt, kt-legacy, pandocfilters\n",
            "INFO\t2021-07-02 16:36:05 +0000\tmaster-replica-0\t\t  Building wheel for twitchstreaming (setup.py): started\n",
            "INFO\t2021-07-02 16:36:05 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO\t2021-07-02 16:36:05 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "INFO\t2021-07-02 16:36:05 +0000\tmaster-replica-0\t\t  Building wheel for twitchstreaming (setup.py): finished with status 'done'\n",
            "INFO\t2021-07-02 16:36:05 +0000\tmaster-replica-0\t\t  Created wheel for twitchstreaming: filename=twitchstreaming-0.0.1-py3-none-any.whl size=4020 sha256=18684a8cfd210e77c29e0a8d7bd770b3a9b95bac3815a69e3a2bf6665305e9a8\n",
            "INFO\t2021-07-02 16:36:05 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/c3/74/78/094c3d65891811370d10f2463da6ec5f32ea469d2e14797ac9\n",
            "INFO\t2021-07-02 16:36:05 +0000\tmaster-replica-0\t\t  Building wheel for gensim (setup.py): started\n",
            "INFO\t2021-07-02 16:36:05 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO\t2021-07-02 16:36:05 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "INFO\t2021-07-02 16:36:22 +0000\tmaster-replica-0\t\t  Building wheel for gensim (setup.py): finished with status 'done'\n",
            "INFO\t2021-07-02 16:36:22 +0000\tmaster-replica-0\t\t  Created wheel for gensim: filename=gensim-3.6.0-cp37-cp37m-linux_x86_64.whl size=24225103 sha256=c87126522da15afb0193eda8aac749f7d9cd657f7ca46ae0c8200c05cd0bfe1b\n",
            "INFO\t2021-07-02 16:36:22 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/53/c8/f9/afb722099bdb5d73e5807019ce1512fd065502ccc15ea2b5bd\n",
            "INFO\t2021-07-02 16:36:22 +0000\tmaster-replica-0\t\t  Building wheel for oauth2client (setup.py): started\n",
            "INFO\t2021-07-02 16:36:23 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO\t2021-07-02 16:36:23 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "INFO\t2021-07-02 16:36:23 +0000\tmaster-replica-0\t\t  Building wheel for oauth2client (setup.py): finished with status 'done'\n",
            "INFO\t2021-07-02 16:36:23 +0000\tmaster-replica-0\t\t  Created wheel for oauth2client: filename=oauth2client-3.0.0-py3-none-any.whl size=106374 sha256=66fd4c402747190c631bb9dffca77c376d19fee2e6b185c8069933cca4a217fa\n",
            "INFO\t2021-07-02 16:36:23 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/86/73/7a/3b3f76a2142176605ff38fbca574327962c71e25a43197a4c1\n",
            "INFO\t2021-07-02 16:36:23 +0000\tmaster-replica-0\t\t  Building wheel for dill (setup.py): started\n",
            "INFO\t2021-07-02 16:36:23 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO\t2021-07-02 16:36:23 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "INFO\t2021-07-02 16:36:23 +0000\tmaster-replica-0\t\t  Building wheel for dill (setup.py): finished with status 'done'\n",
            "INFO\t2021-07-02 16:36:23 +0000\tmaster-replica-0\t\t  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78543 sha256=9c46ea8482eefbfb4294043b413df0720c75846eb9ba95dec5cc64beb2f5eda9\n",
            "INFO\t2021-07-02 16:36:23 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
            "INFO\t2021-07-02 16:36:23 +0000\tmaster-replica-0\t\t  Building wheel for avro-python3 (setup.py): started\n",
            "INFO\t2021-07-02 16:36:24 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO\t2021-07-02 16:36:24 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "INFO\t2021-07-02 16:36:24 +0000\tmaster-replica-0\t\t  Building wheel for avro-python3 (setup.py): finished with status 'done'\n",
            "INFO\t2021-07-02 16:36:24 +0000\tmaster-replica-0\t\t  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-py3-none-any.whl size=43512 sha256=f7d4b49c9fe2cbb6613e6afd1789e905404f493da2f465422d583c00ab25694b\n",
            "INFO\t2021-07-02 16:36:24 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/bc/49/5f/fdb5b9d85055c478213e0158ac122b596816149a02d82e0ab1\n",
            "INFO\t2021-07-02 16:36:24 +0000\tmaster-replica-0\t\t  Building wheel for google-apitools (setup.py): started\n",
            "INFO\t2021-07-02 16:36:24 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO\t2021-07-02 16:36:24 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "INFO\t2021-07-02 16:36:24 +0000\tmaster-replica-0\t\t  Building wheel for google-apitools (setup.py): finished with status 'done'\n",
            "INFO\t2021-07-02 16:36:24 +0000\tmaster-replica-0\t\t  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131040 sha256=8c925d2a43ecf302124a1dda10dc5d18f776eeb2f19a15a9effb97755dc83b93\n",
            "INFO\t2021-07-02 16:36:24 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/19/b5/2f/1cc3cf2b31e7a9cd1508731212526d9550271274d351c96f16\n",
            "INFO\t2021-07-02 16:36:24 +0000\tmaster-replica-0\t\t  Building wheel for docopt (setup.py): started\n",
            "INFO\t2021-07-02 16:36:24 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO\t2021-07-02 16:36:24 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "INFO\t2021-07-02 16:36:24 +0000\tmaster-replica-0\t\t  Building wheel for docopt (setup.py): finished with status 'done'\n",
            "INFO\t2021-07-02 16:36:24 +0000\tmaster-replica-0\t\t  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=44aedf4fc7156b542e78d63e0149dd9020d13cf59fe44132cd8db31be3adf63a\n",
            "INFO\t2021-07-02 16:36:24 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "INFO\t2021-07-02 16:36:24 +0000\tmaster-replica-0\t\t  Building wheel for kt-legacy (setup.py): started\n",
            "INFO\t2021-07-02 16:36:25 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO\t2021-07-02 16:36:25 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "INFO\t2021-07-02 16:36:25 +0000\tmaster-replica-0\t\t  Building wheel for kt-legacy (setup.py): finished with status 'done'\n",
            "INFO\t2021-07-02 16:36:25 +0000\tmaster-replica-0\t\t  Created wheel for kt-legacy: filename=kt_legacy-1.0.3-py3-none-any.whl size=9567 sha256=4e4ddcf43aace65a8a23503d73037825c0b0f541403a6f48b8a6e4b2ce67ada4\n",
            "INFO\t2021-07-02 16:36:25 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/38/5c/e0/13003e68c17f403af40b92a24d20171b95fef13b0fdaba833c\n",
            "INFO\t2021-07-02 16:36:25 +0000\tmaster-replica-0\t\t  Building wheel for pandocfilters (setup.py): started\n",
            "INFO\t2021-07-02 16:36:25 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO\t2021-07-02 16:36:25 +0000\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "INFO\t2021-07-02 16:36:25 +0000\tmaster-replica-0\t\t  Building wheel for pandocfilters (setup.py): finished with status 'done'\n",
            "INFO\t2021-07-02 16:36:25 +0000\tmaster-replica-0\t\t  Created wheel for pandocfilters: filename=pandocfilters-1.4.3-py3-none-any.whl size=8007 sha256=2337777fffccfe6525c41404d6f8fd613e34aeaa040e3bb781ba82eefa06f8bc\n",
            "INFO\t2021-07-02 16:36:25 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/42/81/34/545dc2fbf0e9137811e901108d37fc04650e81d48f97078000\n",
            "INFO\t2021-07-02 16:36:25 +0000\tmaster-replica-0\t\tSuccessfully built twitchstreaming gensim oauth2client dill avro-python3 google-apitools docopt kt-legacy pandocfilters\n",
            "ERROR\t2021-07-02 16:36:26 +0000\tmaster-replica-0\t\tERROR: tensorflow-gpu 2.1.3 has requirement gast==0.2.2, but you'll have gast 0.3.3 which is incompatible.\n",
            "ERROR\t2021-07-02 16:36:26 +0000\tmaster-replica-0\t\tERROR: tensorflow-gpu 2.1.3 has requirement keras-preprocessing==1.1.0, but you'll have keras-preprocessing 1.1.2 which is incompatible.\n",
            "ERROR\t2021-07-02 16:36:26 +0000\tmaster-replica-0\t\tERROR: tensorflow-gpu 2.1.3 has requirement tensorboard<2.2.0,>=2.1.0, but you'll have tensorboard 2.5.0 which is incompatible.\n",
            "ERROR\t2021-07-02 16:36:26 +0000\tmaster-replica-0\t\tERROR: tensorflow-gpu 2.1.3 has requirement tensorflow-estimator<2.2.0,>=2.1.0rc0, but you'll have tensorflow-estimator 2.3.0 which is incompatible.\n",
            "ERROR\t2021-07-02 16:36:26 +0000\tmaster-replica-0\t\tERROR: tensorflow-serving-api 2.5.1 has requirement tensorflow<3,>=2.5.0, but you'll have tensorflow 2.3.0 which is incompatible.\n",
            "ERROR\t2021-07-02 16:36:26 +0000\tmaster-replica-0\t\tERROR: ipykernel 6.0.1 has requirement importlib-metadata<4; python_version < \"3.8.0\", but you'll have importlib-metadata 4.5.0 which is incompatible.\n",
            "ERROR\t2021-07-02 16:36:26 +0000\tmaster-replica-0\t\tERROR: notebook 6.4.0 has requirement tornado>=6.1, but you'll have tornado 5.1.1 which is incompatible.\n",
            "INFO\t2021-07-02 16:36:26 +0000\tmaster-replica-0\t\tInstalling collected packages: grpcio, pydot, oauth2client, dill, pyarrow, requests, docopt, hdfs, typing-extensions, fastavro, avro-python3, pbr, mock, pymongo, google-cloud-vision, google-cloud-videointelligence, google-cloud-dlp, grpcio-gcp, google-cloud-language, google-cloud-spanner, fasteners, google-apitools, cachetools, apache-beam, absl-py, tensorboard-plugin-wit, tensorboard-data-server, tensorboard, gast, keras-preprocessing, tensorflow-estimator, astunparse, tensorflow, tensorflow-serving-api, pandas, tensorflow-metadata, tfx-bsl, tensorflow-transform, websocket-client, docker, MarkupSafe, jinja2, attrs, kt-legacy, backcall, pickleshare, pygments, ipython-genutils, traitlets, matplotlib-inline, ptyprocess, pexpect, wcwidth, prompt-toolkit, decorator, parso, jedi, ipython, keras-tuner, kubernetes, ml-metadata, click, tensorflow-data-validation, pyzmq, jupyter-core, jupyter-client, debugpy, ipykernel, jupyterlab-widgets, pyrsistent, jsonschema, nbformat, prometheus-client, pycparser, cffi, argon2-cffi, Send2Trash, nest-asyncio, async-generator, nbclient, defusedxml, mistune, webencodings, bleach, pandocfilters, entrypoints, jupyterlab-pygments, testpath, nbconvert, terminado, notebook, widgetsnbextension, ipywidgets, jupyter-console, qtpy, qtconsole, jupyter, tensorflow-model-analysis, tfx, smart-open, gensim, fsspec, multidict, yarl, async-timeout, aiohttp, gcsfs, twitchstreaming\n",
            "ERROR\t2021-07-02 16:36:29 +0000\tmaster-replica-0\t\t  WARNING: The script plasma_store is installed in '/root/.local/bin' which is not on PATH.\n",
            "ERROR\t2021-07-02 16:36:29 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR\t2021-07-02 16:36:29 +0000\tmaster-replica-0\t\t  WARNING: The scripts hdfscli and hdfscli-avro are installed in '/root/.local/bin' which is not on PATH.\n",
            "ERROR\t2021-07-02 16:36:29 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR\t2021-07-02 16:36:29 +0000\tmaster-replica-0\t\t  WARNING: The script fastavro is installed in '/root/.local/bin' which is not on PATH.\n",
            "ERROR\t2021-07-02 16:36:29 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR\t2021-07-02 16:36:30 +0000\tmaster-replica-0\t\t  WARNING: The script pbr is installed in '/root/.local/bin' which is not on PATH.\n",
            "ERROR\t2021-07-02 16:36:30 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR\t2021-07-02 16:36:31 +0000\tmaster-replica-0\t\t  WARNING: The script gen_client is installed in '/root/.local/bin' which is not on PATH.\n",
            "ERROR\t2021-07-02 16:36:31 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR\t2021-07-02 16:36:34 +0000\tmaster-replica-0\t\t  WARNING: The script tensorboard is installed in '/root/.local/bin' which is not on PATH.\n",
            "ERROR\t2021-07-02 16:36:34 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR\t2021-07-02 16:36:49 +0000\tmaster-replica-0\t\t  WARNING: The scripts estimator_ckpt_converter, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/root/.local/bin' which is not on PATH.\n",
            "ERROR\t2021-07-02 16:36:49 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR\t2021-07-02 16:36:56 +0000\tmaster-replica-0\t\t  WARNING: The script pygmentize is installed in '/root/.local/bin' which is not on PATH.\n",
            "ERROR\t2021-07-02 16:36:56 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR\t2021-07-02 16:36:58 +0000\tmaster-replica-0\t\t  WARNING: The scripts iptest, iptest3, ipython and ipython3 are installed in '/root/.local/bin' which is not on PATH.\n",
            "ERROR\t2021-07-02 16:36:58 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR\t2021-07-02 16:37:01 +0000\tmaster-replica-0\t\t  WARNING: The scripts jupyter, jupyter-migrate and jupyter-troubleshoot are installed in '/root/.local/bin' which is not on PATH.\n",
            "ERROR\t2021-07-02 16:37:01 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR\t2021-07-02 16:37:01 +0000\tmaster-replica-0\t\t  WARNING: The scripts jupyter-kernel, jupyter-kernelspec and jupyter-run are installed in '/root/.local/bin' which is not on PATH.\n",
            "ERROR\t2021-07-02 16:37:01 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR\t2021-07-02 16:37:02 +0000\tmaster-replica-0\t\t  WARNING: The script jsonschema is installed in '/root/.local/bin' which is not on PATH.\n",
            "ERROR\t2021-07-02 16:37:02 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR\t2021-07-02 16:37:02 +0000\tmaster-replica-0\t\t  WARNING: The script jupyter-trust is installed in '/root/.local/bin' which is not on PATH.\n",
            "ERROR\t2021-07-02 16:37:02 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR\t2021-07-02 16:37:03 +0000\tmaster-replica-0\t\t  WARNING: The script send2trash is installed in '/root/.local/bin' which is not on PATH.\n",
            "ERROR\t2021-07-02 16:37:03 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR\t2021-07-02 16:37:03 +0000\tmaster-replica-0\t\t  WARNING: The script jupyter-nbconvert is installed in '/root/.local/bin' which is not on PATH.\n",
            "ERROR\t2021-07-02 16:37:03 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR\t2021-07-02 16:37:05 +0000\tmaster-replica-0\t\t  WARNING: The scripts jupyter-bundlerextension, jupyter-nbextension, jupyter-notebook and jupyter-serverextension are installed in '/root/.local/bin' which is not on PATH.\n",
            "ERROR\t2021-07-02 16:37:05 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR\t2021-07-02 16:37:05 +0000\tmaster-replica-0\t\t  WARNING: The script jupyter-console is installed in '/root/.local/bin' which is not on PATH.\n",
            "ERROR\t2021-07-02 16:37:05 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR\t2021-07-02 16:37:07 +0000\tmaster-replica-0\t\t  WARNING: The script tfx is installed in '/root/.local/bin' which is not on PATH.\n",
            "ERROR\t2021-07-02 16:37:07 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "INFO\t2021-07-02 16:37:09 +0000\tmaster-replica-0\t\t  Attempting uninstall: twitchstreaming\n",
            "INFO\t2021-07-02 16:37:09 +0000\tmaster-replica-0\t\t    Found existing installation: twitchstreaming 0.0.1\n",
            "INFO\t2021-07-02 16:37:09 +0000\tmaster-replica-0\t\t    Uninstalling twitchstreaming-0.0.1:\n",
            "INFO\t2021-07-02 16:37:09 +0000\tmaster-replica-0\t\t      Successfully uninstalled twitchstreaming-0.0.1\n",
            "INFO\t2021-07-02 16:37:09 +0000\tmaster-replica-0\t\tSuccessfully installed MarkupSafe-2.0.1 Send2Trash-1.7.1 absl-py-0.10.0 aiohttp-3.7.4.post0 apache-beam-2.24.0 argon2-cffi-20.1.0 astunparse-1.6.3 async-generator-1.10 async-timeout-3.0.1 attrs-19.3.0 avro-python3-1.9.2.1 backcall-0.2.0 bleach-3.3.0 cachetools-3.1.1 cffi-1.14.5 click-7.1.2 debugpy-1.3.0 decorator-5.0.9 defusedxml-0.7.1 dill-0.3.1.1 docker-4.4.4 docopt-0.6.2 entrypoints-0.3 fastavro-0.23.6 fasteners-0.16.3 fsspec-0.8.4 gast-0.3.3 gcsfs-0.7.1 gensim-3.6.0 google-apitools-0.5.31 google-cloud-dlp-1.0.0 google-cloud-language-1.3.0 google-cloud-spanner-1.19.1 google-cloud-videointelligence-1.16.1 google-cloud-vision-1.0.0 grpcio-1.38.1 grpcio-gcp-0.2.2 hdfs-2.6.0 ipykernel-6.0.1 ipython-7.25.0 ipython-genutils-0.2.0 ipywidgets-7.6.3 jedi-0.18.0 jinja2-2.11.3 jsonschema-3.2.0 jupyter-1.0.0 jupyter-client-6.1.12 jupyter-console-6.4.0 jupyter-core-4.7.1 jupyterlab-pygments-0.1.2 jupyterlab-widgets-1.0.0 keras-preprocessing-1.1.2 keras-tuner-1.0.3 kt-legacy-1.0.3 kubernetes-11.0.0 matplotlib-inline-0.1.2 mistune-0.8.4 ml-metadata-0.24.0 mock-2.0.0 multidict-5.1.0 nbclient-0.5.3 nbconvert-6.1.0 nbformat-5.1.3 nest-asyncio-1.5.1 notebook-6.4.0 oauth2client-3.0.0 pandas-1.3.0 pandocfilters-1.4.3 parso-0.8.2 pbr-5.6.0 pexpect-4.8.0 pickleshare-0.7.5 prometheus-client-0.11.0 prompt-toolkit-3.0.19 ptyprocess-0.7.0 pyarrow-0.17.1 pycparser-2.20 pydot-1.4.2 pygments-2.9.0 pymongo-3.11.4 pyrsistent-0.18.0 pyzmq-22.1.0 qtconsole-5.1.1 qtpy-1.9.0 requests-2.25.1 smart-open-5.1.0 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.3.0 tensorflow-data-validation-0.24.1 tensorflow-estimator-2.3.0 tensorflow-metadata-0.24.0 tensorflow-model-analysis-0.24.3 tensorflow-serving-api-2.5.1 tensorflow-transform-0.24.1 terminado-0.10.1 testpath-0.5.0 tfx-0.24.1 tfx-bsl-0.24.1 traitlets-5.0.5 twitchstreaming-0.0.1 typing-extensions-3.7.4.3 wcwidth-0.2.5 webencodings-0.5.1 websocket-client-1.1.0 widgetsnbextension-3.5.1 yarl-1.6.3\n",
            "ERROR\t2021-07-02 16:37:09 +0000\tmaster-replica-0\t\tWARNING: You are using pip version 20.1; however, version 21.1.3 is available.\n",
            "ERROR\t2021-07-02 16:37:09 +0000\tmaster-replica-0\t\tYou should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\n",
            "INFO\t2021-07-02 16:37:10 +0000\tmaster-replica-0\t\tRunning command: python3 -m trainer.task --work-dir gs://twitch-practiceeva --epochs 1 --job-dir gs://twitch-practiceeva/trainer\n",
            "INFO\t2021-07-02 16:37:11 +0000\tmaster-replica-0\t\t'pattern' package not found; tag filters are not available for English\n",
            "INFO\t2021-07-02 16:37:12 +0000\tmaster-replica-0\t\tSuccessfully opened dynamic library libcudart.so.10.1\n",
            "INFO\t2021-07-02 16:37:14 +0000\tmaster-replica-0\t\tTF_CONFIG environment variable: {'cluster': {'chief': ['127.0.0.1:2222']}, 'task': {'type': 'chief', 'index': 0, 'cloud': 'n31c519c025b74626p-tp'}, 'job': {'scale_tier': 'BASIC_GPU', 'package_uris': ['gs://twitch-practiceeva/trainer/packages/cd7eb410ee874c52a52a3ff0fb4ac1e2e3c473beb369719a9af456fc2f616dd1/twitchstreaming-0.0.1.tar.gz'], 'python_module': 'trainer.task', 'args': ['--work-dir', 'gs://twitch-practiceeva', '--epochs', '1', '--job-dir', 'gs://twitch-practiceeva/trainer'], 'region': 'europe-west1', 'runtime_version': '2.1', 'job_dir': 'gs://twitch-practiceeva/trainer', 'run_on_raw_vm': True, 'python_version': '3.7'}, 'environment': 'cloud'}\n",
            "INFO\t2021-07-02 16:37:14 +0000\tmaster-replica-0\t\t---- Generating word2vec model ----\n",
            "INFO\t2021-07-02 16:37:14 +0000\tmaster-replica-0\t\tcollecting all words and their counts\n",
            "INFO\t2021-07-02 16:37:14 +0000\tmaster-replica-0\t\tPROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "INFO\t2021-07-02 16:37:14 +0000\tmaster-replica-0\t\tPROGRESS: at sentence #10000, processed 68479 words, keeping 10280 word types\n",
            "INFO\t2021-07-02 16:37:14 +0000\tmaster-replica-0\t\tcollected 13119 word types from a corpus of 91545 raw words and 12754 sentences\n",
            "INFO\t2021-07-02 16:37:14 +0000\tmaster-replica-0\t\tLoading a fresh vocabulary\n",
            "INFO\t2021-07-02 16:37:14 +0000\tmaster-replica-0\t\teffective_min_count=10 retains 1349 unique words (10% of original 13119, drops 11770)\n",
            "INFO\t2021-07-02 16:37:14 +0000\tmaster-replica-0\t\teffective_min_count=10 leaves 65902 word corpus (71% of original 91545, drops 25643)\n",
            "INFO\t2021-07-02 16:37:14 +0000\tmaster-replica-0\t\tdeleting the raw counts dictionary of 13119 items\n",
            "INFO\t2021-07-02 16:37:14 +0000\tmaster-replica-0\t\tsample=0.001 downsamples 60 most-common words\n",
            "INFO\t2021-07-02 16:37:14 +0000\tmaster-replica-0\t\tdownsampling leaves estimated 54283 word corpus (82.4% of prior 65902)\n",
            "INFO\t2021-07-02 16:37:14 +0000\tmaster-replica-0\t\testimated required memory for 1349 words and 300 dimensions: 3912100 bytes\n",
            "INFO\t2021-07-02 16:37:14 +0000\tmaster-replica-0\t\tresetting layer weights\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tVocab size: 1349\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\ttraining model with 8 workers on 1349 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=7\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 7 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 6 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 5 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 4 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 3 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 2 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 1 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 0 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tEPOCH - 1 : training on 91545 raw words (54286 effective words) took 0.1s, 877545 effective words/s\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 7 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 6 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 5 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 4 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 3 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 2 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 1 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 0 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tEPOCH - 2 : training on 91545 raw words (54157 effective words) took 0.1s, 956535 effective words/s\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 7 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 6 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 5 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 4 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 3 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 2 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 1 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 0 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tEPOCH - 3 : training on 91545 raw words (54306 effective words) took 0.1s, 948649 effective words/s\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 7 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 6 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 5 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 4 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 3 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 2 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 1 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 0 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tEPOCH - 4 : training on 91545 raw words (54270 effective words) took 0.1s, 1012735 effective words/s\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 7 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 6 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 5 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 4 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 3 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 2 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 1 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 0 more threads\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tEPOCH - 5 : training on 91545 raw words (54325 effective words) took 0.1s, 980874 effective words/s\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\ttraining on a 457725 raw words (271344 effective words) took 0.3s, 818436 effective words/s\n",
            "WARNING\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tunder 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\t---- Generating tokenizer ----\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\tTotal words: 13120\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\t---- Tokenizing train data ----\n",
            "INFO\t2021-07-02 16:37:15 +0000\tmaster-replica-0\t\t---- Tokenizing eval data ----\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\t---- Generating label encoder ----\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\t---- Encoding train target ----\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\t---- Encoding eval target ----\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\t---- Generating embedding layer ----\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\t---- Generating Sequential model ----\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tSuccessfully opened dynamic library libcuda.so.1\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tsuccessful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tFound device 0 with properties: \n",
            "ERROR\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tpciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "ERROR\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tcoreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tSuccessfully opened dynamic library libcudart.so.10.1\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tSuccessfully opened dynamic library libcublas.so.10\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tSuccessfully opened dynamic library libcufft.so.10\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tSuccessfully opened dynamic library libcurand.so.10\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tSuccessfully opened dynamic library libcusolver.so.10\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tSuccessfully opened dynamic library libcusparse.so.10\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tSuccessfully opened dynamic library libcudnn.so.7\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tsuccessful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tsuccessful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tAdding visible gpu devices: 0\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tThis TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "ERROR\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tCPU Frequency: 2299995000 Hz\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tXLA service 0x745f2d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\t  StreamExecutor device (0): Host, Default Version\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tsuccessful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tXLA service 0x748af40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\t  StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tsuccessful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tFound device 0 with properties: \n",
            "ERROR\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tpciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "ERROR\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tcoreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tSuccessfully opened dynamic library libcudart.so.10.1\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tSuccessfully opened dynamic library libcublas.so.10\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tSuccessfully opened dynamic library libcufft.so.10\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tSuccessfully opened dynamic library libcurand.so.10\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tSuccessfully opened dynamic library libcusolver.so.10\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tSuccessfully opened dynamic library libcusparse.so.10\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tSuccessfully opened dynamic library libcudnn.so.7\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tsuccessful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tsuccessful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tAdding visible gpu devices: 0\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tSuccessfully opened dynamic library libcudart.so.10.1\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tDevice interconnect StreamExecutor with strength 1 edge matrix:\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\t     0 \n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\t0:   N \n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tsuccessful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tsuccessful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "INFO\t2021-07-02 16:37:16 +0000\tmaster-replica-0\t\tCreated TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10625 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING\t2021-07-02 16:37:17 +0000\tmaster-replica-0\t\tLayer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "INFO\t2021-07-02 16:37:17 +0000\tmaster-replica-0\t\t---- Adding loss function to model ----\n",
            "INFO\t2021-07-02 16:37:17 +0000\tmaster-replica-0\t\t---- Adding callbacks to model ----\n",
            "INFO\t2021-07-02 16:37:17 +0000\tmaster-replica-0\t\t---- Training model ----\n",
            "INFO\t2021-07-02 16:37:19 +0000\tmaster-replica-0\t\tSuccessfully opened dynamic library libcublas.so.10\n",
            "INFO\t2021-07-02 16:37:21 +0000\tmaster-replica-0\t\tModel: \"sequential\"\n",
            "INFO\t2021-07-02 16:37:21 +0000\tmaster-replica-0\t\t_________________________________________________________________\n",
            "INFO\t2021-07-02 16:37:21 +0000\tmaster-replica-0\t\tLayer (type)                 Output Shape              Param #   \n",
            "INFO\t2021-07-02 16:37:21 +0000\tmaster-replica-0\t\t=================================================================\n",
            "INFO\t2021-07-02 16:37:21 +0000\tmaster-replica-0\t\tembedding (Embedding)        (None, 300, 300)          3936000   \n",
            "INFO\t2021-07-02 16:37:21 +0000\tmaster-replica-0\t\t_________________________________________________________________\n",
            "INFO\t2021-07-02 16:37:21 +0000\tmaster-replica-0\t\tdropout (Dropout)            (None, 300, 300)          0         \n",
            "INFO\t2021-07-02 16:37:21 +0000\tmaster-replica-0\t\t_________________________________________________________________\n",
            "INFO\t2021-07-02 16:37:21 +0000\tmaster-replica-0\t\tlstm (LSTM)                  (None, 100)               160400    \n",
            "INFO\t2021-07-02 16:37:21 +0000\tmaster-replica-0\t\t_________________________________________________________________\n",
            "INFO\t2021-07-02 16:37:21 +0000\tmaster-replica-0\t\tdense (Dense)                (None, 1)                 101       \n",
            "INFO\t2021-07-02 16:37:21 +0000\tmaster-replica-0\t\t=================================================================\n",
            "INFO\t2021-07-02 16:37:21 +0000\tmaster-replica-0\t\tTotal params: 4,096,501\n",
            "INFO\t2021-07-02 16:37:21 +0000\tmaster-replica-0\t\tTrainable params: 160,501\n",
            "INFO\t2021-07-02 16:37:21 +0000\tmaster-replica-0\t\tNon-trainable params: 3,936,000\n",
            "INFO\t2021-07-02 16:37:21 +0000\tmaster-replica-0\t\t_________________________________________________________________\n",
            "INFO\t2021-07-02 16:37:22 +0000\tmaster-replica-0\t\t  1/100 [..............................] - ETA: 0s - loss: 0.6965 - accuracy: 0.4863\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "INFO\t2021-07-02 16:37:23 +0000\tmaster-replica-0\t\t  2/100 [..............................] - ETA: 56s - loss: 0.6990 - accuracy: 0.5029\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "INFO\t2021-07-02 16:37:25 +0000\tmaster-replica-0\t\t  3/100 [..............................] - ETA: 1:15 - loss: 0.6971 - accuracy: 0.5173\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "INFO\t2021-07-02 16:37:26 +0000\tmaster-replica-0\t\t  4/100 [>.............................] - ETA: 1:24 - loss: 0.6958 - accuracy: 0.5242\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "INFO\t2021-07-02 16:37:27 +0000\tmaster-replica-0\t\t  5/100 [>.............................] - ETA: 1:28 - loss: 0.6950 - accuracy: 0.5295\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "INFO\t2021-07-02 16:37:28 +0000\tmaster-replica-0\t\t  6/100 [>.............................] - ETA: 1:31 - loss: 0.6945 - accuracy: 0.5345\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "INFO\t2021-07-02 16:37:29 +0000\tmaster-replica-0\t\t  7/100 [=>............................] - ETA: 1:33 - loss: 0.6939 - accuracy: 0.5361\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "INFO\t2021-07-02 16:37:30 +0000\tmaster-replica-0\t\t  8/100 [=>............................] - ETA: 1:34 - loss: 0.6933 - accuracy: 0.5403\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "INFO\t2021-07-02 16:37:32 +0000\tmaster-replica-0\t\t  9/100 [=>............................] - ETA: 1:34 - loss: 0.6924 - accuracy: 0.5458\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "INFO\t2021-07-02 16:37:33 +0000\tmaster-replica-0\t\t 10/100 [==>...........................] - ETA: 1:35 - loss: 0.6915 - accuracy: 0.5488\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "INFO\t2021-07-02 16:37:34 +0000\tmaster-replica-0\t\t 11/100 [==>...........................] - ETA: 1:34 - loss: 0.6911 - accuracy: 0.5498\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "WARNING\t2021-07-02 16:37:34 +0000\tmaster-replica-0\t\tYour input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 100 batches). You may need to use the repeat() function when building your dataset.\n",
            "INFO\t2021-07-02 16:37:35 +0000\tmaster-replica-0\t\t 12/100 [==>...........................] - ETA: 1:33 - loss: 0.6908 - accuracy: 0.5511\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "INFO\t2021-07-02 16:37:35 +0000\tmaster-replica-0\t\t 12/100 [==>...........................] - 14s 1s/step - loss: 0.6908 - accuracy: 0.5511 - val_loss: 0.5706 - val_accuracy: 0.9953\n",
            "INFO\t2021-07-02 16:37:35 +0000\tmaster-replica-0\t\t---- Evaluating model ----\n",
            "INFO\t2021-07-02 16:37:35 +0000\tmaster-replica-0\t\t1/4 [======>.......................] - ETA: 0s - loss: 0.6154 - accuracy: 0.7949\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "INFO\t2021-07-02 16:37:36 +0000\tmaster-replica-0\t\t2/4 [==============>...............] - ETA: 0s - loss: 0.5891 - accuracy: 0.8970\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "INFO\t2021-07-02 16:37:36 +0000\tmaster-replica-0\t\t3/4 [=====================>........] - ETA: 0s - loss: 0.6635 - accuracy: 0.6442\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "INFO\t2021-07-02 16:37:36 +0000\tmaster-replica-0\t\t4/4 [==============================] - ETA: 0s - loss: 0.6704 - accuracy: 0.6198\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "INFO\t2021-07-02 16:37:36 +0000\tmaster-replica-0\t\t4/4 [==============================] - 1s 178ms/step - loss: 0.6704 - accuracy: 0.6198\n",
            "INFO\t2021-07-02 16:37:36 +0000\tmaster-replica-0\t\tACCURACY: 0.6197932958602905\n",
            "INFO\t2021-07-02 16:37:36 +0000\tmaster-replica-0\t\tLOSS: 0.6704490780830383\n",
            "INFO\t2021-07-02 16:37:36 +0000\tmaster-replica-0\t\t---- Saving models ----\n",
            "INFO\t2021-07-02 16:37:37 +0000\tmaster-replica-0\t\tModule completed; cleaning up.\n",
            "INFO\t2021-07-02 16:37:37 +0000\tmaster-replica-0\t\tClean up finished.\n",
            "INFO\t2021-07-02 16:37:37 +0000\tmaster-replica-0\t\tTask completed successfully.\n",
            "INFO\t2021-07-02 16:40:13 +0000\tservice\t\tJob completed successfully.\n",
            "endTime: '2021-07-02T16:40:13'\n",
            "jobId: troll_detection_batch_20210702_163306\n",
            "startTime: '2021-07-02T16:35:10'\n",
            "state: SUCCEEDED\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu5SOWsy9886"
      },
      "source": [
        "### Validación predict en Dataflow (0.25 puntos)\n",
        "\n",
        "Con el comando mostrado a continuación se valida la predicción correcta de los datos de test usando los modelos generados en el comando anterior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXoHCgMg-LUD"
      },
      "source": [
        "Generamos un timestamp para el almacenamiento de las inferencias en Google Cloud Storage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmcetQtnQjVo"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# current date and time\n",
        "TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zT_NxsKDQlxD",
        "outputId": "dd953a5d-35e8-48b0-fb50-80c8ef9c4945"
      },
      "source": [
        "# For using sample models: --model-dir gs://$BUCKET_NAME/models/\n",
        "! python3 predict.py \\\n",
        "  --work-dir $GCP_WORK_DIR \\\n",
        "  --model-dir $GCP_WORK_DIR/model/ \\\n",
        "  batch \\\n",
        "  --project $PROJECT_ID \\\n",
        "  --region $GCP_REGION \\\n",
        "  --runner DataflowRunner \\\n",
        "  --temp_location $GCP_WORK_DIR/beam-temp \\\n",
        "  --setup_file ./setup.py \\\n",
        "  --inputs-dir $GCP_WORK_DIR/transformed_data/test/part* \\\n",
        "  --outputs-dir $GCP_WORK_DIR/predictions/$TIMESTAMP"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-02 16:46:39.519989: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Namespace(inputs_dir='gs://twitch-practiceeva/transformed_data/test/part*', model_dir='gs://twitch-practiceeva/model/', outputs_dir='gs://twitch-practiceeva/predictions/2021-07-02_16-46-38', verb='batch', work_dir='gs://twitch-practiceeva')\n",
            "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
            "\n",
            "warning: check: missing required meta-data: url\n",
            "\n",
            "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
            "\n",
            "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GWbfFvwQTGS"
      },
      "source": [
        "# Inferencia online\n",
        "\n",
        "En esta segunda parte de la práctica se realizará un microservicio de inferencia online usando los modelos generados en la primera parte. Para esta parte de la práctica el código de vuestro microservicio deberá estar subido en un repositorio. En la variable de debajo deberéis dejar la URL a vuestro repositorrio pues será el contenido con el que serás evaluado. \n",
        "\n",
        "**Importante:** asegúrate de crear el repositorio de manera pública para poder clonarlo.\n",
        "\n",
        "A continuación os dejo un diagrama con la arquitectura que se va a desarrollar:\n",
        "\n",
        "![online_diagram](https://drive.google.com/uc?export=view&id=1zR7Cwp0Vq1QeTxwLoJ8YJNRM9G5KVh2S)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REn23OCIBjXF"
      },
      "source": [
        "REPOSITORIO = \"https://github.com/eesquivias/eva_practica_implementacion_algoritmos.git\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqBT-b_8BsIR"
      },
      "source": [
        "Creamos el directorio donde trabajaremos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAskyBUAQZHx",
        "outputId": "191c5548-73b7-4ae1-c5e8-1d5c88d5eda8"
      },
      "source": [
        "%mkdir /content/online\n",
        "%cd /content/online"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/online’: File exists\n",
            "/content/online\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlj7lGW3Q6CS",
        "outputId": "5c07fb7c-2ac0-47bb-fd4c-0d1aab9fe895"
      },
      "source": [
        "# Clone the repository\n",
        "! git clone $REPOSITORIO\n",
        "\n",
        "# Set the working directory to the sample code directory\n",
        "%cd ./eva_practica_implementacion_algoritmos\n",
        "\n",
        "# Change to develop\n",
        "! git checkout develop"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'eva_practica_implementacion_algoritmos'...\n",
            "remote: Enumerating objects: 72, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 72 (delta 31), reused 17 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (72/72), done.\n",
            "/content/online/eva_practica_implementacion_algoritmos\n",
            "error: pathspec 'develop' did not match any file(s) known to git.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFDctnBo7bhs",
        "outputId": "9720b338-cf07-41a7-ae04-453a4ddd461a"
      },
      "source": [
        "#para después de reiniciar al instalar los requirements\n",
        "%cd ./eva_practica_implementacion_algoritmos "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/online/eva_practica_implementacion_algoritmos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL2U9pKCWfHq",
        "outputId": "90bedbc6-bfe6-439c-f35d-3b42c4a91ba8"
      },
      "source": [
        "#! git pull $REPOSITORIO después de un cambio lo he necesitado. lo dejo por si en un futuro..."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FndkRszYQ8_f",
        "outputId": "79adb9ec-5457-4454-d4d3-093cf2c50996"
      },
      "source": [
        "! pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting requests==2.25.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/fc/f91eac5a39a65f75a7adb58eac7fa78871ea9872283fb9c44e6545998134/requests-2.25.0-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.3MB/s \n",
            "\u001b[?25hCollecting uvicorn==0.12.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/cc/01cc4cb980dfcf04eb283b6497c7f280928a0b02c68c0f85b6901e7716ae/uvicorn-0.12.2-py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.1MB/s \n",
            "\u001b[?25hCollecting fastapi==0.61.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/0b/5df17eaadb7fe39dad349f484e551e802ce0581be672822f010c530d5e75/fastapi-0.61.2-py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (2.10.0)\n",
            "Collecting scikit-learn==0.23.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/cb/64623369f348e9bfb29ff898a57ac7c91ed4921f228e9726546614d63ccb/scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 19.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (1.4.1)\n",
            "Collecting tensorflow==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/56/0dbdae2a3c527a119bec0d5cf441655fe030ce1daa6fa6b9542f7dbd8664/tensorflow-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 32kB/s \n",
            "\u001b[?25hCollecting loguru==0.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/48/0a7d5847e3de329f1d0134baf707b689700b53bd3066a5a8cfd94b3c9fc8/loguru-0.5.3-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim==3.6.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (3.6.0)\n",
            "Requirement already satisfied: fsspec==0.8.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (0.8.4)\n",
            "Requirement already satisfied: gcsfs==0.7.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (0.7.1)\n",
            "Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (1.19.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.0->-r requirements.txt (line 1)) (2021.5.30)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.0->-r requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.0->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.0->-r requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: click==7.* in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.12.2->-r requirements.txt (line 2)) (7.1.2)\n",
            "Collecting h11>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/0f/7a0eeea938eaf61074f29fed9717f2010e8d0e0905d36b38d3275a1e4622/h11-0.12.0-py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.12.2->-r requirements.txt (line 2)) (3.7.4.3)\n",
            "Collecting pydantic<2.0.0,>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/f2/2d5425efe57f6c4e06cbe5e587c1fd16929dcf0eb90bd4d3d1e1c97d1151/pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1MB 95kB/s \n",
            "\u001b[?25hCollecting starlette==0.13.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/a4/c9e228d7d47044ce4c83ba002f28ff479e542455f0499198a3f77c94f564/starlette-0.13.6-py3-none-any.whl (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0->-r requirements.txt (line 4)) (1.15.0)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->-r requirements.txt (line 5)) (0.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (1.12.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (1.34.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (0.2.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (3.12.4)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 47.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (0.36.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (0.10.0)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 21.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.6.0->-r requirements.txt (line 9)) (5.1.0)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 11)) (1.31.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 11)) (0.4.4)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 11)) (4.4.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 11)) (3.7.4.post0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow==2.1.0->-r requirements.txt (line 7)) (57.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements.txt (line 7)) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 11)) (3.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 11)) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 11)) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib->gcsfs==0.7.1->-r requirements.txt (line 11)) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 11)) (19.3.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 11)) (3.0.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 11)) (5.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 11)) (1.6.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements.txt (line 7)) (4.5.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 11)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==0.7.1->-r requirements.txt (line 11)) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements.txt (line 7)) (3.4.1)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7557 sha256=2c795f067a2ccdca2c71b97d4641545c7f818d792884aa331c467c970f0005c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tfx 0.24.1 has requirement tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15.2, but you'll have tensorflow 2.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tfx-bsl 0.24.1 has requirement tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15.2, but you'll have tensorflow 2.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-transform 0.24.1 has requirement tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2, but you'll have tensorflow 2.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-serving-api 2.5.1 has requirement tensorflow<3,>=2.5.0, but you'll have tensorflow 2.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-model-analysis 0.24.3 has requirement tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15.2, but you'll have tensorflow 2.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-data-validation 0.24.1 has requirement tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15.2, but you'll have tensorflow 2.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: requests, h11, uvicorn, pydantic, starlette, fastapi, threadpoolctl, scikit-learn, gast, keras-applications, tensorflow-estimator, tensorboard, tensorflow, loguru\n",
            "  Found existing installation: requests 2.25.1\n",
            "    Uninstalling requests-2.25.1:\n",
            "      Successfully uninstalled requests-2.25.1\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed fastapi-0.61.2 gast-0.2.2 h11-0.12.0 keras-applications-1.0.8 loguru-0.5.3 pydantic-1.8.2 requests-2.25.0 scikit-learn-0.23.2 starlette-0.13.6 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0 threadpoolctl-2.1.0 uvicorn-0.12.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IefrYp6tJ4PE",
        "outputId": "9240a976-1f9a-4f07-f286-ee240fd80392"
      },
      "source": [
        "! pip install pyngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/4e/a2fe095bbe17cf26424c4abcd22a0490e22d01cc628f25af5e220ddbf6f0/pyngrok-5.0.5.tar.gz (745kB)\n",
            "\r\u001b[K     |▍                               | 10kB 14.4MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 18.6MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30kB 21.3MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40kB 23.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51kB 24.3MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61kB 26.2MB/s eta 0:00:01\r\u001b[K     |███                             | 71kB 26.2MB/s eta 0:00:01\r\u001b[K     |███▌                            | 81kB 25.5MB/s eta 0:00:01\r\u001b[K     |████                            | 92kB 26.3MB/s eta 0:00:01\r\u001b[K     |████▍                           | 102kB 26.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 112kB 26.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 122kB 26.4MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 133kB 26.4MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 143kB 26.4MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 153kB 26.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 163kB 26.4MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 174kB 26.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 184kB 26.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 194kB 26.4MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 204kB 26.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 215kB 26.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 225kB 26.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 235kB 26.4MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 245kB 26.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 256kB 26.4MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 266kB 26.4MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 276kB 26.4MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 286kB 26.4MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 296kB 26.4MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 307kB 26.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 317kB 26.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 327kB 26.4MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 337kB 26.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 348kB 26.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 358kB 26.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 368kB 26.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 378kB 26.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 389kB 26.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 399kB 26.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 409kB 26.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 419kB 26.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 430kB 26.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 440kB 26.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 450kB 26.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 460kB 26.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 471kB 26.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 481kB 26.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 491kB 26.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 501kB 26.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 512kB 26.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 522kB 26.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 532kB 26.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 542kB 26.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 552kB 26.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 563kB 26.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 573kB 26.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 583kB 26.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 593kB 26.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 604kB 26.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 614kB 26.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 624kB 26.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 634kB 26.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 645kB 26.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 655kB 26.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 665kB 26.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 675kB 26.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 686kB 26.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 696kB 26.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 706kB 26.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 716kB 26.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 727kB 26.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 737kB 26.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 747kB 26.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (3.13)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.0.5-cp37-none-any.whl size=19262 sha256=4a32c28815651ded5a76e69f9c94e4c1245a95450860959b6727a7397f0a068b\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/13/64/5ebbcc22eaf53fdf5766b397c1fb17c83f5775fdccf0ea1b88\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-5.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNde3TEzCmjg"
      },
      "source": [
        "Para cuando estes modificando, probando y ejecutando ficheros os dejo en las celdas de abajo los comandos de git necesarios para interaccionar con vuestro repositorio en caso de que queráis:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsjWKcYZC6mm",
        "outputId": "7c4428d3-bf4e-4505-8857-a199de9bbc88"
      },
      "source": [
        "! git status"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuVHFtqvC8Kw"
      },
      "source": [
        "! git add <files>\n",
        "! git commit -m \"Nuevos cambios\"\n",
        "! git push origin master"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H88vcTU_DDpv"
      },
      "source": [
        "Será necesario definir y establecer la variable de entorno `DEFAULT_MODEL_PATH` para definir donde están almacenados nuestros modelos para hacer inferencia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn9xsgx4u5xP"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"DEFAULT_MODEL_PATH\"] = \"/content/batch/model/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8vXOFqqXv8L7",
        "outputId": "e70ae4a2-eb2c-4cab-8580-e65740d0a433"
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/online/eva_practica_implementacion_algoritmos'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0H-kCBzEiDU",
        "outputId": "e4fbb01f-bb6d-46bf-edf7-62c95cbd7f6d"
      },
      "source": [
        "#si no los tengo cargados en local (se me bloquea constantemente colab) los cargo de cloud\n",
        "! gsutil -m cp \\\n",
        "  \"gs://$BUCKET_NAME/model/model.h5\" \\\n",
        "  \"gs://$BUCKET_NAME/model/tokenizer.pkl\" \\\n",
        "  ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://twitch-practiceeva/model/model.h5...\n",
            "Copying gs://twitch-practiceeva/model/tokenizer.pkl...\n",
            "/ [2/2 files][ 17.7 MiB/ 17.7 MiB] 100% Done                                    \n",
            "Operation completed over 2 objects/17.7 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-IslHWwDSTU"
      },
      "source": [
        "### Validación inferencia online en local (1.75 puntos)\n",
        "\n",
        "Se validará la correcta inferencia del microservio en local utilizando Swagger. Para ejecutar en local solo hay que ejecutar los comandos a continuación. Después, entrar en la URL proporcionada por ngrock `<ngrok_url>/docs` para acceder a swagger y probar la inferencia como vimos en clase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81vGToBWJ_h-",
        "outputId": "e344fa2b-81be-4dce-a6de-d22bc6dfa94f"
      },
      "source": [
        "# For testing purposes\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Public URL: http://8a319555ab9f.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFeSh7rcuG7L",
        "outputId": "c26838cc-8348-4749-d3bb-3617759fb960"
      },
      "source": [
        "! uvicorn app.main:app --port 8000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-03 15:56:33.974553: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2021-07-03 15:56:33.974798: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2021-07-03 15:56:33.974826: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m6358\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32m2021-07-03 15:56:35.281\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.core.event_handlers\u001b[0m:\u001b[36mstartup\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1mRunning app start handler.\u001b[0m\n",
            "2021-07-03 15:56:35.794973: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2021-07-03 15:56:35.806093: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-07-03 15:56:35.806145: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ed70ec286c9a): /proc/driver/nvidia/version does not exist\n",
            "2021-07-03 15:56:35.807065: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2021-07-03 15:56:35.816781: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200220000 Hz\n",
            "2021-07-03 15:56:35.817832: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56166092a840 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-07-03 15:56:35.817878: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8000\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     Shutting down\n",
            "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m6358\u001b[0m]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuLtOjukEMIl"
      },
      "source": [
        "### Validación inferencia online en GCP (1.75 puntos)\n",
        "\n",
        "Se validará el correcto funcionamiento del microservicio haciendo una petición POST de inferencia a través de curl al microservicio desplegado en GCP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX1MiTGuD2KS"
      },
      "source": [
        "Primero, contruiremos una imagen Docker con el microservicio y subiremos el desarrollo al Container Repository en GCP a través de Cloud Build."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRhq-d_E7-CY",
        "outputId": "9a90e377-68e6-4dbe-e9ce-9c82c324c99d"
      },
      "source": [
        "! gcloud builds submit --tag gcr.io/$PROJECT_ID/troll-detection-online-service"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating temporary tarball archive of 40 file(s) totalling 16.5 KiB before compression.\n",
            "Uploading tarball of [.] to [gs://twitch-practiceeva_cloudbuild/source/1625306139.335005-30fcf16ba0a040b99f43e23c01ddd104.tgz]\n",
            "Created [https://cloudbuild.googleapis.com/v1/projects/twitch-practiceeva/locations/global/builds/dc288610-b9fd-45e9-8a2a-b03625dff04c].\n",
            "Logs are available at [https://console.cloud.google.com/cloud-build/builds/dc288610-b9fd-45e9-8a2a-b03625dff04c?project=178865046960].\n",
            " REMOTE BUILD OUTPUT\n",
            "starting build \"dc288610-b9fd-45e9-8a2a-b03625dff04c\"\n",
            "\n",
            "FETCHSOURCE\n",
            "Fetching storage object: gs://twitch-practiceeva_cloudbuild/source/1625306139.335005-30fcf16ba0a040b99f43e23c01ddd104.tgz#1625306140586611\n",
            "Copying gs://twitch-practiceeva_cloudbuild/source/1625306139.335005-30fcf16ba0a040b99f43e23c01ddd104.tgz#1625306140586611...\n",
            "/ [1 files][  7.1 KiB/  7.1 KiB]                                                \n",
            "Operation completed over 1 objects/7.1 KiB.\n",
            "BUILD\n",
            "Already have image (with digest): gcr.io/cloud-builders/docker\n",
            "Sending build context to Docker daemon  54.27kB\n",
            "Step 1/4 : FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\n",
            "python3.7: Pulling from tiangolo/uvicorn-gunicorn-fastapi\n",
            "6c33745f49b4: Pulling fs layer\n",
            "ef072fc32a84: Pulling fs layer\n",
            "c0afb8e68e0b: Pulling fs layer\n",
            "d599c07d28e6: Pulling fs layer\n",
            "f2ecc74db11a: Pulling fs layer\n",
            "26856d31ce86: Pulling fs layer\n",
            "d4db05d8da44: Pulling fs layer\n",
            "a63ae5575b2c: Pulling fs layer\n",
            "a8c72204fafe: Pulling fs layer\n",
            "588d4fc87a50: Pulling fs layer\n",
            "0d7a4a6073cb: Pulling fs layer\n",
            "3a7e0859d882: Pulling fs layer\n",
            "644da687b92a: Pulling fs layer\n",
            "f6e61dbee877: Pulling fs layer\n",
            "3e19947ae254: Pulling fs layer\n",
            "4a4150ad76a8: Pulling fs layer\n",
            "331fe1ea8d9a: Pulling fs layer\n",
            "edff73282333: Pulling fs layer\n",
            "d599c07d28e6: Waiting\n",
            "f2ecc74db11a: Waiting\n",
            "26856d31ce86: Waiting\n",
            "d4db05d8da44: Waiting\n",
            "a63ae5575b2c: Waiting\n",
            "a8c72204fafe: Waiting\n",
            "588d4fc87a50: Waiting\n",
            "0d7a4a6073cb: Waiting\n",
            "3a7e0859d882: Waiting\n",
            "644da687b92a: Waiting\n",
            "f6e61dbee877: Waiting\n",
            "3e19947ae254: Waiting\n",
            "4a4150ad76a8: Waiting\n",
            "331fe1ea8d9a: Waiting\n",
            "edff73282333: Waiting\n",
            "ef072fc32a84: Verifying Checksum\n",
            "ef072fc32a84: Download complete\n",
            "c0afb8e68e0b: Verifying Checksum\n",
            "c0afb8e68e0b: Download complete\n",
            "6c33745f49b4: Verifying Checksum\n",
            "6c33745f49b4: Download complete\n",
            "26856d31ce86: Verifying Checksum\n",
            "26856d31ce86: Download complete\n",
            "d599c07d28e6: Verifying Checksum\n",
            "d599c07d28e6: Download complete\n",
            "a63ae5575b2c: Verifying Checksum\n",
            "a63ae5575b2c: Download complete\n",
            "d4db05d8da44: Verifying Checksum\n",
            "d4db05d8da44: Download complete\n",
            "a8c72204fafe: Verifying Checksum\n",
            "a8c72204fafe: Download complete\n",
            "588d4fc87a50: Verifying Checksum\n",
            "588d4fc87a50: Download complete\n",
            "0d7a4a6073cb: Verifying Checksum\n",
            "0d7a4a6073cb: Download complete\n",
            "3a7e0859d882: Verifying Checksum\n",
            "3a7e0859d882: Download complete\n",
            "644da687b92a: Verifying Checksum\n",
            "644da687b92a: Download complete\n",
            "f6e61dbee877: Verifying Checksum\n",
            "f6e61dbee877: Download complete\n",
            "f2ecc74db11a: Verifying Checksum\n",
            "f2ecc74db11a: Download complete\n",
            "3e19947ae254: Verifying Checksum\n",
            "3e19947ae254: Download complete\n",
            "4a4150ad76a8: Verifying Checksum\n",
            "4a4150ad76a8: Download complete\n",
            "edff73282333: Verifying Checksum\n",
            "edff73282333: Download complete\n",
            "331fe1ea8d9a: Verifying Checksum\n",
            "331fe1ea8d9a: Download complete\n",
            "6c33745f49b4: Pull complete\n",
            "ef072fc32a84: Pull complete\n",
            "c0afb8e68e0b: Pull complete\n",
            "d599c07d28e6: Pull complete\n",
            "f2ecc74db11a: Pull complete\n",
            "26856d31ce86: Pull complete\n",
            "d4db05d8da44: Pull complete\n",
            "a63ae5575b2c: Pull complete\n",
            "a8c72204fafe: Pull complete\n",
            "588d4fc87a50: Pull complete\n",
            "0d7a4a6073cb: Pull complete\n",
            "3a7e0859d882: Pull complete\n",
            "644da687b92a: Pull complete\n",
            "f6e61dbee877: Pull complete\n",
            "3e19947ae254: Pull complete\n",
            "4a4150ad76a8: Pull complete\n",
            "331fe1ea8d9a: Pull complete\n",
            "edff73282333: Pull complete\n",
            "Digest: sha256:8efb7b7c2419090159955d01c9978b2d59ef0980605f237673d7ae1ed5d76b4a\n",
            "Status: Downloaded newer image for tiangolo/uvicorn-gunicorn-fastapi:python3.7\n",
            " ---> 86ade7dea2c7\n",
            "Step 2/4 : COPY ./requirements.txt /requirements.txt\n",
            " ---> ceb0447427e1\n",
            "Step 3/4 : RUN pip install -r /requirements.txt\n",
            " ---> Running in 94dc1a471a99\n",
            "Collecting fastapi==0.61.2\n",
            "  Downloading fastapi-0.61.2-py3-none-any.whl (48 kB)\n",
            "Requirement already satisfied: pydantic<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/site-packages (from fastapi==0.61.2->-r /requirements.txt (line 3)) (1.8.2)\n",
            "Collecting fsspec==0.8.4\n",
            "  Downloading fsspec-0.8.4-py3-none-any.whl (91 kB)\n",
            "Collecting gcsfs==0.7.1\n",
            "  Downloading gcsfs-0.7.1-py2.py3-none-any.whl (20 kB)\n",
            "Collecting gensim==3.6.0\n",
            "  Downloading gensim-3.6.0.tar.gz (23.1 MB)\n",
            "Collecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "Collecting loguru==0.5.3\n",
            "  Downloading loguru-0.5.3-py3-none-any.whl (57 kB)\n",
            "Collecting numpy==1.19.5\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "Collecting requests==2.25.0\n",
            "  Downloading requests-2.25.0-py2.py3-none-any.whl (61 kB)\n",
            "Collecting scikit-learn==0.23.2\n",
            "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "Collecting scipy==1.4.1\n",
            "  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
            "Collecting tensorflow==2.1.0\n",
            "  Downloading tensorflow-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.1.0->-r /requirements.txt (line 7)) (0.36.2)\n",
            "Collecting uvicorn==0.12.2\n",
            "  Downloading uvicorn-0.12.2-py3-none-any.whl (45 kB)\n",
            "Requirement already satisfied: click==7.* in /usr/local/lib/python3.7/site-packages (from uvicorn==0.12.2->-r /requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.7/site-packages (from uvicorn==0.12.2->-r /requirements.txt (line 2)) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/site-packages (from uvicorn==0.12.2->-r /requirements.txt (line 2)) (3.7.4.3)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Collecting starlette==0.13.6\n",
            "  Downloading starlette-0.13.6-py3-none-any.whl (59 kB)\n",
            "Collecting absl-py>=0.7.0\n",
            "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
            "Collecting astor>=0.6.0\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Downloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
            "Collecting chardet<4,>=3.0.2\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "Collecting google-auth>=1.2\n",
            "  Downloading google_auth-1.32.1-py2.py3-none-any.whl (147 kB)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs==0.7.1->-r /requirements.txt (line 11)) (51.0.0)\n",
            "Collecting cachetools<5.0,>=2.0.0\n",
            "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
            "Collecting google-pasta>=0.1.6\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Collecting grpcio>=1.8.6\n",
            "  Downloading grpcio-1.38.1-cp37-cp37m-manylinux2014_x86_64.whl (4.2 MB)\n",
            "Collecting idna<3,>=2.5\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "Collecting joblib>=0.11\n",
            "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "Collecting keras-preprocessing>=1.1.0\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "Collecting opt-einsum>=2.3.2\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "Collecting protobuf>=3.8.0\n",
            "  Downloading protobuf-3.17.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "Collecting pyasn1<0.5.0,>=0.4.6\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "Collecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Collecting six>=1.5.0\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting smart_open>=1.2.1\n",
            "  Downloading smart_open-5.1.0-py3-none-any.whl (57 kB)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
            "Collecting google-auth-oauthlib\n",
            "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
            "Collecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
            "Collecting termcolor>=1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "Collecting werkzeug>=0.11.15\n",
            "  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
            "Collecting wrapt>=1.11.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting attrs>=17.3.0\n",
            "  Downloading attrs-21.2.0-py2.py3-none-any.whl (53 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "Collecting decorator\n",
            "  Downloading decorator-5.0.9-py3-none-any.whl (8.9 kB)\n",
            "Collecting importlib-metadata\n",
            "  Downloading importlib_metadata-4.6.0-py3-none-any.whl (17 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Downloading zipp-3.5.0-py3-none-any.whl (5.7 kB)\n",
            "Building wheels for collected packages: gensim, gast, termcolor, wrapt\n",
            "  Building wheel for gensim (setup.py): started\n",
            "  Building wheel for gensim (setup.py): finished with status 'done'\n",
            "  Created wheel for gensim: filename=gensim-3.6.0-cp37-cp37m-linux_x86_64.whl size=24445187 sha256=6f431da7804a6bdd3adc321e017c9e1517ee044626b2254f89bc326edbec2914\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/c8/f9/afb722099bdb5d73e5807019ce1512fd065502ccc15ea2b5bd\n",
            "  Building wheel for gast (setup.py): started\n",
            "  Building wheel for gast (setup.py): finished with status 'done'\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7538 sha256=aa5599e4addb50b6c35921aaba0e34df5ed27bcb1ca0a179739207ca06170c15\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "  Building wheel for termcolor (setup.py): started\n",
            "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=7032627130e45eb11489ab0fc96a7c042476798a11eb18ecb2efd354b905b478\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
            "  Building wheel for wrapt (setup.py): started\n",
            "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=76470 sha256=e2bc3a798e3ec185e469d78fd6b8e6eb01f6d79ceeb48afc19b457fa136e902d\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "Successfully built gensim gast termcolor wrapt\n",
            "Installing collected packages: urllib3, pyasn1, idna, chardet, certifi, zipp, six, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, numpy, multidict, importlib-metadata, google-auth, yarl, werkzeug, protobuf, markdown, h5py, grpcio, google-auth-oauthlib, attrs, async-timeout, absl-py, wrapt, threadpoolctl, termcolor, tensorflow-estimator, tensorboard, starlette, smart-open, scipy, opt-einsum, keras-preprocessing, keras-applications, joblib, google-pasta, gast, fsspec, decorator, astor, aiohttp, uvicorn, tensorflow, scikit-learn, loguru, gensim, gcsfs, fastapi\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.14.2\n",
            "    Uninstalling starlette-0.14.2:\n",
            "      Successfully uninstalled starlette-0.14.2\n",
            "  Attempting uninstall: uvicorn\n",
            "    Found existing installation: uvicorn 0.13.1\n",
            "    Uninstalling uvicorn-0.13.1:\n",
            "      Successfully uninstalled uvicorn-0.13.1\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.65.2\n",
            "    Uninstalling fastapi-0.65.2:\n",
            "      Successfully uninstalled fastapi-0.65.2\n",
            "Successfully installed absl-py-0.13.0 aiohttp-3.7.4.post0 astor-0.8.1 async-timeout-3.0.1 attrs-21.2.0 cachetools-4.2.2 certifi-2021.5.30 chardet-3.0.4 decorator-5.0.9 fastapi-0.61.2 fsspec-0.8.4 gast-0.2.2 gcsfs-0.7.1 gensim-3.6.0 google-auth-1.32.1 google-auth-oauthlib-0.4.4 google-pasta-0.2.0 grpcio-1.38.1 h5py-2.10.0 idna-2.10 importlib-metadata-4.6.0 joblib-1.0.1 keras-applications-1.0.8 keras-preprocessing-1.1.2 loguru-0.5.3 markdown-3.3.4 multidict-5.1.0 numpy-1.19.5 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.17.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.25.0 requests-oauthlib-1.3.0 rsa-4.7.2 scikit-learn-0.23.2 scipy-1.4.1 six-1.16.0 smart-open-5.1.0 starlette-0.13.6 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0 termcolor-1.1.0 threadpoolctl-2.1.0 urllib3-1.26.6 uvicorn-0.12.2 werkzeug-2.0.1 wrapt-1.12.1 yarl-1.6.3 zipp-3.5.0\n",
            "\u001b[91mWARNING: You are using pip version 20.3.3; however, version 21.1.3 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
            "\u001b[0mRemoving intermediate container 94dc1a471a99\n",
            " ---> 5e2c7f0c8049\n",
            "Step 4/4 : COPY ./app /app/app\n",
            " ---> 7fca61ec5fc0\n",
            "Successfully built 7fca61ec5fc0\n",
            "Successfully tagged gcr.io/twitch-practiceeva/troll-detection-online-service:latest\n",
            "PUSH\n",
            "Pushing gcr.io/twitch-practiceeva/troll-detection-online-service\n",
            "The push refers to repository [gcr.io/twitch-practiceeva/troll-detection-online-service]\n",
            "ca908f5f2887: Preparing\n",
            "45ceccb827d5: Preparing\n",
            "36d8c4be150a: Preparing\n",
            "b0cbd55a00a8: Preparing\n",
            "5c1526d87912: Preparing\n",
            "bb80579fc546: Preparing\n",
            "075c652b44f7: Preparing\n",
            "b26121b4dfdd: Preparing\n",
            "92c41c21be74: Preparing\n",
            "ab98b1286ea9: Preparing\n",
            "c72c79cd478b: Preparing\n",
            "f7737f3c85d2: Preparing\n",
            "3da8662a6eed: Preparing\n",
            "6a6ea1335e48: Preparing\n",
            "4324e0912cc9: Preparing\n",
            "59840d625c92: Preparing\n",
            "da87e334550a: Preparing\n",
            "c5f4367d4a59: Preparing\n",
            "ceecb62b2fcc: Preparing\n",
            "193bc1d68b80: Preparing\n",
            "f0e10b20de19: Preparing\n",
            "bb80579fc546: Waiting\n",
            "075c652b44f7: Waiting\n",
            "b26121b4dfdd: Waiting\n",
            "ab98b1286ea9: Waiting\n",
            "c72c79cd478b: Waiting\n",
            "f7737f3c85d2: Waiting\n",
            "3da8662a6eed: Waiting\n",
            "6a6ea1335e48: Waiting\n",
            "4324e0912cc9: Waiting\n",
            "59840d625c92: Waiting\n",
            "da87e334550a: Waiting\n",
            "c5f4367d4a59: Waiting\n",
            "ceecb62b2fcc: Waiting\n",
            "193bc1d68b80: Waiting\n",
            "f0e10b20de19: Waiting\n",
            "92c41c21be74: Waiting\n",
            "36d8c4be150a: Pushed\n",
            "b0cbd55a00a8: Pushed\n",
            "ca908f5f2887: Pushed\n",
            "b26121b4dfdd: Pushed\n",
            "5c1526d87912: Pushed\n",
            "075c652b44f7: Pushed\n",
            "bb80579fc546: Pushed\n",
            "92c41c21be74: Pushed\n",
            "ab98b1286ea9: Pushed\n",
            "c72c79cd478b: Pushed\n",
            "f7737f3c85d2: Pushed\n",
            "6a6ea1335e48: Pushed\n",
            "59840d625c92: Layer already exists\n",
            "da87e334550a: Layer already exists\n",
            "c5f4367d4a59: Layer already exists\n",
            "193bc1d68b80: Layer already exists\n",
            "ceecb62b2fcc: Layer already exists\n",
            "f0e10b20de19: Layer already exists\n",
            "3da8662a6eed: Pushed\n",
            "4324e0912cc9: Pushed\n",
            "45ceccb827d5: Pushed\n",
            "latest: digest: sha256:62d87c911d2e151c3dd5711f404ae0de05b3186c638d46c22da1fd53f315ffbc size: 4719\n",
            "DONE\n",
            "\n",
            "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                            IMAGES                                                              STATUS\n",
            "dc288610-b9fd-45e9-8a2a-b03625dff04c  2021-07-03T09:55:41+00:00  6M1S      gs://twitch-practiceeva_cloudbuild/source/1625306139.335005-30fcf16ba0a040b99f43e23c01ddd104.tgz  gcr.io/twitch-practiceeva/troll-detection-online-service (+1 more)  SUCCESS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYVptQ0WEEd-"
      },
      "source": [
        "Desplegaremos la imagen Docker generada en el Container Registry en el servicio de Cloud Run. Después, validaremos que las inferencias funcionan en GCP usando el comando mostrado a continuación:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KC0honui-W23",
        "outputId": "8a7e643b-df91-49ae-83ad-25f43ec85d4f"
      },
      "source": [
        "! curl -X POST \"https://troll-detection-service-g2v7skqfeq-uc.a.run.app/api/model/predict\" -H  \"accept: application/json\" -H  \"Content-Type: application/json\" -d \"{\\\"text\\\":\\\"i hate you\\\"}\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"label\":\"NO_TROLL\",\"score\":0.40702152252197266,\"elapsed_time\":0.7307009696960449}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGKJtyHT6S-r",
        "outputId": "e4e7cbb9-19d5-4f74-fa87-fbf2484a9411"
      },
      "source": [
        "#con mi aplicación tampoco detecta la frase como troll..aunque le da un score un pelín mayor. \n",
        "! curl -X POST \"https://troll-detection-6ff2xxxgaq-ew.a.run.app/api/model/predict\" -H  \"accept: application/json\" -H  \"Content-Type: application/json\" -d \"{\\\"text\\\":\\\"i hate you\\\"}\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"label\":\"NOTROLL\",\"score\":0.41722559928894043,\"elapsed_time\":0.7551183700561523}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymvVL1ss8jXy",
        "outputId": "2887fb6f-62f0-4afc-982c-8cba42d3727c"
      },
      "source": [
        "! curl -X POST \"https://troll-detection-6ff2xxxgaq-ew.a.run.app/api/model/predict\" -H  \"accept: application/json\" -H  \"Content-Type: application/json\" -d \"{\\\"text\\\":\\\"shit\\\"}\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"label\":\"NOTROLL\",\"score\":0.41826510429382324,\"elapsed_time\":0.45891594886779785}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvBWPoGGKwuJ"
      },
      "source": [
        "# Detección de Trolls en Twitch en Streaming\n",
        "\n",
        "En esta última parte de la práctica se realizará un pipeline de inferencia en tiempo real de un chat de Twitch alcualmente en vivo. Para ello, usaremos mi canal de Twitch `https://www.twitch.tv/franalgaba` donde tengo un bot deplegado poniendo mensajes troll y no troll de forma aleatoria del dataset que hemos usado en la primera parte.\n",
        "\n",
        "Para acceder al chat de Twitch os proporciono el conector correspondiente que será desplegado como Cloud Function como hicimos en clase y usando mis credenciales recogerá los mensajes del chat y los enviará a un topic de Pub/Sub en GCP. Después, desarrollarás un job en streaming de Dataflow con el que leerás esos mensajes de Pub/sub, los mandarás a tu microservicio de inferencia para que haga las predicciones y enviarás los resultados a un nuevo tópico de Pub/Sub.\n",
        "\n",
        "A continuación os dejo un diagrama con la arquitectura que se va a desarrollar:\n",
        "\n",
        "![streaming_diagram](https://drive.google.com/uc?export=view&id=1TEBPPc9ZF09IM5iGq9FwGAx9PVzAYNPg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw3oY8n5GjuU"
      },
      "source": [
        "Primero, creamos el publisher que será el encargado de recoger los mensajes de Twitch y enviarlos a Pub/Sub. Esto os lo doy yo desarrollado, sólo tendréis que desplegarlo en una Cloud Function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH8WcRKrKwKy"
      },
      "source": [
        "%mkdir -p /content/streaming/publisher"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75whqxX7K54B",
        "outputId": "c8a2c5c0-59f3-4ea0-dfa5-7916eb0a7e87"
      },
      "source": [
        "# Execute after restart\n",
        "%cd /content/streaming/publisher"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/streaming/publisher\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyOecviAK8QO",
        "outputId": "a7cfae91-ce34-4143-9acb-668804c1820c"
      },
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "twitchio==1.2.3\n",
        "loguru==0.5.3\n",
        "google-cloud-pubsub==2.1.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing requirements.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0H6A2Qu2Nwyp",
        "outputId": "5f5a1c98-709e-4615-dcd2-0690e7473456"
      },
      "source": [
        "!pip install -r requirements.txt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting twitchio==1.2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/01/9e360ff53b4a9f538b258b81bf889973fedbb019a5ee6feba8d288708a7f/twitchio-1.2.3-py3-none-any.whl (48kB)\n",
            "\r\u001b[K     |██████▊                         | 10kB 12.7MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 20kB 17.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 30kB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 40kB 25.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: loguru==0.5.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (0.5.3)\n",
            "Collecting google-cloud-pubsub==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/e4/acb1b0b23d46c1b67976f218a263873d4d1aa2ccf077f73aa3af84b24339/google_cloud_pubsub-2.1.0-py2.py3-none-any.whl (177kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 30.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: async-timeout>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from twitchio==1.2.3->-r requirements.txt (line 2)) (3.0.1)\n",
            "Collecting websockets>=6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/64/78c2b3fe37730b30dca3c93d1f7f4a4286767f86e7c04cf3571b39bc2fb7/websockets-9.1-cp37-cp37m-manylinux2010_x86_64.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 43.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: aiohttp>=3.3 in /usr/local/lib/python3.7/dist-packages (from twitchio==1.2.3->-r requirements.txt (line 2)) (3.7.4.post0)\n",
            "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.22.2 in /usr/local/lib/python3.7/dist-packages (from google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (1.26.3)\n",
            "Collecting libcst>=0.3.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/47/ad8ba60c667252be40be06073b58e3234e960ec70608579518e97b368994/libcst-0.3.19-py3-none-any.whl (513kB)\n",
            "\u001b[K     |████████████████████████████████| 522kB 43.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (0.12.3)\n",
            "Collecting proto-plus>=1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/72/6f3f4cdc5bb0294f8d7f3f8aacb617b4c3cb17554ed78f7e28009162c795/proto_plus-1.19.0-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<5.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3->twitchio==1.2.3->-r requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3->twitchio==1.2.3->-r requirements.txt (line 2)) (19.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3->twitchio==1.2.3->-r requirements.txt (line 2)) (3.7.4.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3->twitchio==1.2.3->-r requirements.txt (line 2)) (5.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3->twitchio==1.2.3->-r requirements.txt (line 2)) (1.6.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (1.31.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (3.12.4)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (57.0.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (20.9)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (2.25.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (1.53.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (2018.9)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.29.0; extra == \"grpc\" in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (1.34.1)\n",
            "Collecting typing-inspect>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/fa/77565f652ce57ed56d3a63d537c885a18e4451cf2d56d944991aeb3c82bd/typing_inspect-0.7.1-py3-none-any.whl\n",
            "Collecting pyyaml>=5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 39.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp>=3.3->twitchio==1.2.3->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (4.7.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (2.4.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (2021.5.30)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (0.4.8)\n",
            "\u001b[31mERROR: tfx 0.24.1 has requirement tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15.2, but you'll have tensorflow 2.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pydrive 1.3.1 has requirement oauth2client>=4.0.0, but you'll have oauth2client 3.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: websockets, twitchio, mypy-extensions, typing-inspect, pyyaml, libcst, proto-plus, google-cloud-pubsub\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: google-cloud-pubsub 1.7.0\n",
            "    Uninstalling google-cloud-pubsub-1.7.0:\n",
            "      Successfully uninstalled google-cloud-pubsub-1.7.0\n",
            "Successfully installed google-cloud-pubsub-2.1.0 libcst-0.3.19 mypy-extensions-0.4.3 proto-plus-1.19.0 pyyaml-5.4.1 twitchio-1.2.3 typing-inspect-0.7.1 websockets-9.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "yaml"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-pnzR-jMkZ4",
        "outputId": "829efda9-d899-4899-8f31-66f0552f5d67"
      },
      "source": [
        "%%writefile main.py\n",
        "\n",
        "import os  # for importing env vars for the bot to use\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "\n",
        "from twitchio.ext import commands\n",
        "from google.cloud import pubsub_v1\n",
        "from loguru import logger\n",
        "\n",
        "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
        "TOPIC_NAME = os.getenv(\"TOPIC_NAME\")\n",
        "\n",
        "TOPIC_PATH = f\"projects/{PROJECT_ID}/topics/{TOPIC_NAME}\"\n",
        "\n",
        "publisher = pubsub_v1.PublisherClient()\n",
        "\n",
        "\n",
        "class Bot(commands.Bot):\n",
        "\n",
        "    def __init__(self, irc_token='...', client_id='...', nick='...', prefix=\"!\", initial_channels=['...'], debug=True):\n",
        "        super().__init__(irc_token=irc_token, client_id=client_id, nick=nick, prefix='!',\n",
        "                         initial_channels=initial_channels)\n",
        "        self.debug = debug\n",
        "\n",
        "    # Events don't need decorators when subclassed\n",
        "    async def event_ready(self):\n",
        "        logger.info('Ready')\n",
        "\n",
        "    async def event_message(self, message):\n",
        "        logger.info(message.content)\n",
        "        publisher.publish(TOPIC_PATH, str.encode(message.content))\n",
        "\n",
        "\n",
        "def main(request):\n",
        "\n",
        "    topic_name = f\"projects/{PROJECT_ID}/topics/{TOPIC_NAME}\"\n",
        "    # publisher.create_topic(topic_name)\n",
        "\n",
        "    request_json = request.get_json(silent=True)\n",
        "\n",
        "    logger.info(\"Starting listener...\")\n",
        "    if \"debug\" in request_json and isinstance(request_json[\"debug\"], bool):\n",
        "        logger.info(f\"Debug mode: {request_json['debug']}\")\n",
        "        bot = Bot(\n",
        "          # set up the bot\n",
        "          irc_token=\"oauth:xl5cpf8qe8tl1d03dppymchi6r04iz\",\n",
        "          client_id=\"ciliqxi534iwg4pfqj7swl1jmkt23y\",\n",
        "          nick=\"franalgaba\",\n",
        "          prefix=\"!\",\n",
        "          initial_channels=[\"franalgaba\"],\n",
        "          debug=request_json['debug'])\n",
        "    else:\n",
        "        bot = Bot(\n",
        "          # set up the bot\n",
        "          irc_token=\"oauth:xl5cpf8qe8tl1d03dppymchi6r04iz\",\n",
        "          client_id=\"ciliqxi534iwg4pfqj7swl1jmkt23y\",\n",
        "          nick=\"franalgaba\",\n",
        "          prefix=\"!\",\n",
        "          initial_channels=[\"franalgaba\"])\n",
        "\n",
        "    bot.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing main.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDrQjJbsVO-4"
      },
      "source": [
        "# In case user service error...\n",
        "! gcloud iam service-accounts add-iam-policy-binding <project_id>@appspot.gserviceaccount.com --member=user:<mail> --role=roles/iam.serviceAccountUser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x_BX3FCG3mD"
      },
      "source": [
        "Para lanzar vuestra Cloud Function, que recoja y mande mensajes solo tenéis que ejecutar el comando siguiente (haced los pasos vistos en clase para desplegar el servicio):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9qbXMiXZTIW",
        "outputId": "9e66c877-79cc-481b-8d08-9a6bfe029a42"
      },
      "source": [
        "! curl -X POST https://europe-west1-twitch-practiceeva.cloudfunctions.net/twitch-pub -H \"Content-Type:application/json\"  -d '{\"debug\": false}'\n",
        "\n",
        "#consigo los mensajes en pub/sub y en los logs de la cloudfunction, aunque al rato sale mensaje de error: could not handle the request, y paran los mensajes. supongo por alcanzar el límite? en pub sub me dice rate exceeded. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error: could not handle the request\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr3SmFrUHCvN"
      },
      "source": [
        "## Entregable 1 (3 puntos)\n",
        "\n",
        "En este entregable desarrollarás un pipeline de inferencia en streaming usando Apache Beam para ejecutar en Dataflow un job en streaming que llamará a vuestro microservicio para realizar inferencias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwlDH0KFbn1q"
      },
      "source": [
        "%mkdir /content/streaming/subscriber"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLNtrkgybtHE",
        "outputId": "8566ceca-c3b4-4b86-e78b-2612c832cf70"
      },
      "source": [
        "%cd /content/streaming/subscriber"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/streaming/subscriber\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtBrcF2zbz04",
        "outputId": "99dfa313-163f-49f9-b6b3-4a60b505a103"
      },
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "apache-beam[gcp]==2.24.0\n",
        "fsspec==0.8.4\n",
        "gcsfs==0.7.1\n",
        "loguru==0.5.3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing requirements.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6RN9KNMob1jY",
        "outputId": "c6bb9822-b2a7-40fa-db37-61234b73b609"
      },
      "source": [
        "! pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: apache-beam[gcp]==2.24.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (2.24.0)\n",
            "Requirement already satisfied: fsspec==0.8.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (0.8.4)\n",
            "Requirement already satisfied: gcsfs==0.7.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (0.7.1)\n",
            "Requirement already satisfied: loguru==0.5.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (0.5.3)\n",
            "Requirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.12.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2.25.0)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2.6.0)\n",
            "Requirement already satisfied: oauth2client<4,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.0.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2018.9)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.11.4)\n",
            "Requirement already satisfied: future<1.0.0,>=0.18.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.18.2)\n",
            "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.3.1.1)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.7)\n",
            "Requirement already satisfied: numpy<2,>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.19.5)\n",
            "Requirement already satisfied: httplib2<0.18.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.17.4)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: pyarrow<0.18.0,>=0.15.1; python_version >= \"3.0\" or platform_system != \"Windows\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.17.1)\n",
            "Requirement already satisfied: mock<3.0.0,>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: avro-python3!=1.9.2,<1.10.0,>=1.8.1; python_version >= \"3.0\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.9.2.1)\n",
            "Requirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.34.1)\n",
            "Requirement already satisfied: fastavro<0.24,>=0.21.4 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.23.6)\n",
            "Requirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.7.4.3)\n",
            "Requirement already satisfied: google-cloud-spanner<2,>=1.13.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.19.1)\n",
            "Requirement already satisfied: google-cloud-vision<2,>=0.38.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: google-cloud-language<2,>=1.3.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<2,>=1.6.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.21.0)\n",
            "Collecting google-cloud-pubsub<2,>=0.39.0; extra == \"gcp\"\n",
            "  Using cached https://files.pythonhosted.org/packages/1f/b3/dd83eca4cd1019d592e82595ea45d53f11e39db4ee99daa66ceb8a1b2d89/google_cloud_pubsub-1.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: google-apitools<0.5.32,>=0.5.31; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.5.31)\n",
            "Requirement already satisfied: google-cloud-datastore<2,>=1.7.1; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: google-cloud-dlp<2,>=0.12.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.18.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.31.0)\n",
            "Requirement already satisfied: google-cloud-videointelligence<2,>=1.8.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.16.1)\n",
            "Requirement already satisfied: google-cloud-bigtable<2,>=0.31.1; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.7.0)\n",
            "Requirement already satisfied: grpcio-gcp<1,>=0.2.2; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.2.2)\n",
            "Requirement already satisfied: cachetools<4,>=3.1.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: google-cloud-core<2,>=0.28.1; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.0.3)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 4)) (0.4.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 4)) (3.7.4.post0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 4)) (4.4.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<4,>=3.12.2->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf<4,>=3.12.2->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (57.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2021.5.30)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.6.2)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (4.7.2)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pydot<2,>=1.2.0->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2.4.7)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.7/dist-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (5.6.0)\n",
            "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-spanner<2,>=1.13.0; extra == \"gcp\"->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.12.3)\n",
            "Requirement already satisfied: google-api-core[grpc,grpcgcp]<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-spanner<2,>=1.13.0; extra == \"gcp\"->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.26.3)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<2,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: fasteners>=0.14 in /usr/local/lib/python3.7/dist-packages (from google-apitools<0.5.32,>=0.5.31; extra == \"gcp\"->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.16.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib->gcsfs==0.7.1->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (5.1.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (3.0.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (1.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (19.3.0)\n",
            "Requirement already satisfied: googleapis-common-protos[grpc]<2.0.0dev,>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpc-google-iam-v1<0.13dev,>=0.12.3->google-cloud-spanner<2,>=1.13.0; extra == \"gcp\"->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.53.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc,grpcgcp]<2.0.0dev,>=1.14.0->google-cloud-spanner<2,>=1.13.0; extra == \"gcp\"->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (20.9)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==0.7.1->-r requirements.txt (line 4)) (3.1.1)\n",
            "Installing collected packages: google-cloud-pubsub\n",
            "  Found existing installation: google-cloud-pubsub 2.1.0\n",
            "    Uninstalling google-cloud-pubsub-2.1.0:\n",
            "      Successfully uninstalled google-cloud-pubsub-2.1.0\n",
            "Successfully installed google-cloud-pubsub-1.7.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMeJPR90b3AN",
        "outputId": "d0df8ff1-9568-4109-f5bc-359350a1c63d"
      },
      "source": [
        "%%writefile predict.py\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import requests\n",
        "import json\n",
        "import sys\n",
        "\n",
        "import apache_beam as beam\n",
        "import apache_beam.transforms.window as window\n",
        "from apache_beam.options.pipeline_options import (\n",
        "    GoogleCloudOptions,\n",
        "    StandardOptions,\n",
        "    PipelineOptions,\n",
        "    SetupOptions,\n",
        ")\n",
        "from loguru import logger\n",
        "\n",
        "\n",
        "class Predict(beam.DoFn):\n",
        "    def __init__(self, predict_server) -> None:\n",
        "        self.url = predict_server\n",
        "\n",
        "    def _predict(self, text) -> str:\n",
        "        payload = {\"text\": text}\n",
        "        headers = {\"accept\": \"application/json\", \"Content-Type\": \"application/json\"}\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                self.url, data=json.dumps(payload), headers=headers\n",
        "            )\n",
        "            response = json.loads(response.text)\n",
        "        except Exception:\n",
        "            response = {\"label\": \"undefined\", \"score\": 0, \"elapsed_time\": 0}\n",
        "\n",
        "        return response\n",
        "\n",
        "    def process(self, element, window=beam.DoFn.WindowParam):\n",
        "        logger.info(f\"Text to predict: {element}\")\n",
        "        result = self._predict(element)\n",
        "        result[\"text\"] = element\n",
        "        yield json.dumps(result)\n",
        "\n",
        "\n",
        "def run(predict_server, source, sink, beam_options=None):\n",
        "    with beam.Pipeline(options=beam_options) as p:\n",
        "        _ = (\n",
        "            p\n",
        "            | \"Read data from PubSub\" >> source\n",
        "            | \"decode\" >> beam.Map(lambda x: x.decode(\"utf-8\"))\n",
        "            | \"window\" >> beam.WindowInto(window.FixedWindows(15))\n",
        "            | \"Predict\" >> beam.ParDo(Predict(predict_server))\n",
        "            | \"encode\" >> beam.Map(lambda x: x.encode(\"utf-8\")).with_output_types(bytes)\n",
        "            | \"Write predictions\" >> sink\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\"Main function\"\"\"\n",
        "    parser = argparse.ArgumentParser(\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--inputs_topic\",\n",
        "        dest=\"inputs_topic\",\n",
        "        required=True,\n",
        "        help=\"Directory for temporary files and preprocessed datasets to. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--outputs_topic\",\n",
        "        dest=\"outputs_topic\",\n",
        "        required=True,\n",
        "        help=\"Directory for temporary files and preprocessed datasets to. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--predict_server\",\n",
        "        dest=\"predict_server\",\n",
        "        required=True,\n",
        "        help=\"Directory for temporary files and preprocessed datasets to. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    args, pipeline_args = parser.parse_known_args()\n",
        "    logger.info(args)\n",
        "    beam_options = PipelineOptions(pipeline_args)\n",
        "    beam_options.view_as(SetupOptions).save_main_session = True\n",
        "    # beam_options.view_as(DirectOptions).direct_num_workers = 0\n",
        "\n",
        "    project = beam_options.view_as(GoogleCloudOptions).project\n",
        "\n",
        "    if not project:\n",
        "        parser.print_usage()\n",
        "        print(\"error: argument --project is required for streaming\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    beam_options.view_as(StandardOptions).streaming = True\n",
        "\n",
        "    source = beam.io.ReadFromPubSub(\n",
        "        topic=\"projects/{}/topics/{}\".format(project, args.inputs_topic)\n",
        "    ).with_output_types(bytes)\n",
        "\n",
        "    sink = beam.io.WriteToPubSub(\n",
        "        topic=\"projects/{}/topics/{}\".format(project, args.outputs_topic)\n",
        "    )\n",
        "\n",
        "    run(args.predict_server, source, sink, beam_options)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing predict.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT_Qmb16hgEq",
        "outputId": "31225dfd-7741-4355-eead-04ba2396b7e9"
      },
      "source": [
        "%%writefile setup.py\n",
        "\n",
        "import setuptools\n",
        "\n",
        "REQUIRED_PACKAGES = [\n",
        "    \"apache-beam[gcp]==2.24.0\",\n",
        "    \"fsspec==0.8.4\",\n",
        "    \"gcsfs==0.7.1\",\n",
        "    \"loguru==0.5.3\",\n",
        "]\n",
        "\n",
        "setuptools.setup(\n",
        "    name=\"twitchstreaming\",\n",
        "    version=\"0.0.1\",\n",
        "    install_requires=REQUIRED_PACKAGES,\n",
        "    packages=setuptools.find_packages(),\n",
        "    include_package_data=True,\n",
        "    description=\"Twitch Troll Detection\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing setup.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RdyxvdVHmnZ"
      },
      "source": [
        "### Validación inferencia en streaming\n",
        "\n",
        "Con el comando mostrado a continuación se genera un job en streaming de Dataflow. Antes de ejecutarlo, deberás crear dos topicos en Pub/Sub, `twitch-chat` donde se recibirán los mensajes de twitch, y `twitch-chat-predictions` donde se mandarán las predicciones generadas por vuestro microservicio.\n",
        "\n",
        "**Importante**: no te olvides de modificar la URL de tu microservicio de inferencia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9j4vzFn2wUPk"
      },
      "source": [
        "GCP_WORK_DIR = 'gs://twitch-practiceeva'\n",
        "GCP_REGION = 'europe-west1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5ftKoXAhiJ7",
        "outputId": "1a2eb2c2-72e9-4051-c732-295c165d3301"
      },
      "source": [
        "! python3 predict.py \\\n",
        "--project $PROJECT_ID \\\n",
        "--region $GCP_REGION \\\n",
        "--runner DataflowRunner \\\n",
        "--temp_location $BUCKET_NAME/beam-temp \\\n",
        "--setup_file ./setup.py \\\n",
        "--inputs_topic twitch-chat \\\n",
        "--outputs_topic twitch-chat-predictions \\\n",
        "--predict_server https://troll-detection-6ff2xxxgaq-ew.a.run.app/api/model/predict \\\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m2021-07-03 16:18:51.992\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mNamespace(inputs_topic='twitch-chat', outputs_topic='twitch-chat-predictions', predict_server='https://troll-detection-6ff2xxxgaq-ew.a.run.app/api/model/predict')\u001b[0m\n",
            "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
            "\n",
            "warning: check: missing required meta-data: url\n",
            "\n",
            "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 490, in run\n",
            "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 951, in communicate\n",
            "    stdout = self.stdout.read()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"predict.py\", line 111, in <module>\n",
            "    run(args.predict_server, source, sink, beam_options)\n",
            "  File \"predict.py\", line 54, in run\n",
            "    | \"Write predictions\" >> sink\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/pipeline.py\", line 555, in __exit__\n",
            "    self.result = self.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/pipeline.py\", line 534, in run\n",
            "    return self.runner.run_pipeline(self, self._options)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/runners/dataflow/dataflow_runner.py\", line 479, in run_pipeline\n",
            "    artifacts=environments.python_sdk_dependencies(options)))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/transforms/environments.py\", line 613, in python_sdk_dependencies\n",
            "    staged_name in stager.Stager.create_job_resources(options, tmp_dir))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/runners/portability/stager.py\", line 235, in create_job_resources\n",
            "    resources.extend(Stager._create_beam_sdk(sdk_remote_location, temp_dir))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/runners/portability/stager.py\", line 641, in _create_beam_sdk\n",
            "    (sys.version_info[0], sys.version_info[1], abi_suffix))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/runners/portability/stager.py\", line 731, in _download_pypi_sdk_package\n",
            "    processes.check_output(cmd_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/utils/processes.py\", line 91, in check_output\n",
            "    out = subprocess.check_output(*args, **kwargs)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 411, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 512, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 866, in __exit__\n",
            "    self._wait(timeout=self._sigint_wait_secs)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 1647, in _wait\n",
            "    time.sleep(delay)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}